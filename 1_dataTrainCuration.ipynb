{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a1587bd-ce4e-4ad0-91c2-9d5773e08eff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data curation\n",
    "Thi snotebook aims to curate all available protein-ligand binding data associated with experimental affinity values. \n",
    "Curation here implies to format experimental data and structure in a shared format before conducting standardization of structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb78a51-412c-4847-a66f-8a6feb344087",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed01d24-cae5-4e9a-b3be-55728f3182f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def dg_to_kd(delta_g_kcal, temp_k=298.15):\n",
    "    R = 1.987e-3  # kcal/molÂ·K\n",
    "    kd_molar = -math.log10(math.exp(delta_g_kcal / (R * temp_k)))\n",
    "    return kd_molar  # in Molar (M)\n",
    "\n",
    "def prepare_wang_data(base_dir):\n",
    "    data = []\n",
    "    for target in tqdm(os.listdir(base_dir)):\n",
    "        target_dir = os.path.join(base_dir, target)\n",
    "        structures_dir = os.path.join(target_dir, \"structures\")\n",
    "        dg_file = os.path.join(target_dir, \"experimental_dG.txt\")\n",
    "\n",
    "        # Load Î”G values into a dictionary\n",
    "        dg_dict = {}\n",
    "        if os.path.isfile(dg_file):\n",
    "            with open(dg_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) >= 2:\n",
    "                        ligand_name = parts[0]\n",
    "                        try:\n",
    "                            dg_value = float(parts[1])\n",
    "                            kd_value = dg_value  # RT = 0.592 kcal/mol at 298K\n",
    "                            dg_dict[ligand_name] = kd_value\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "\n",
    "        if os.path.isdir(structures_dir):\n",
    "            for ligand_folder in os.listdir(structures_dir):\n",
    "                ligand_path = os.path.join(structures_dir, ligand_folder)\n",
    "                sdf_path = os.path.join(ligand_path, \"ligand.sdf\")\n",
    "                pdb_path = os.path.join(ligand_path, \"protein.pdb\")\n",
    "\n",
    "                smiles = None\n",
    "                if os.path.isfile(sdf_path):\n",
    "                    supplier = Chem.SDMolSupplier(sdf_path)\n",
    "                    mols = [mol for mol in supplier if mol is not None]\n",
    "                    if mols:\n",
    "                        smiles = Chem.MolToSmiles(mols[0])\n",
    "\n",
    "                kd = dg_dict.get(ligand_folder, None)\n",
    "\n",
    "\n",
    "                combine_pdb_sdf(pdb_path, sdf_path, pdb_path.replace(\"protein.pdb\",\"complex.pdb\"))\n",
    "\n",
    "                data.append({\n",
    "                    \"protein_name\": target,\n",
    "                    \"ligand\": ligand_folder,\n",
    "                    \"smiles\": smiles,\n",
    "                    \"ligand_sdf_path\": sdf_path if os.path.isfile(sdf_path) else None,\n",
    "                    \"protein_pdb_path\": pdb_path if os.path.isfile(pdb_path) else None,\n",
    "                    \"complex_pdb_path\": pdb_path.replace(\"protein.pdb\",\"complex.pdb\"),\n",
    "                    \"pKd (Wang, FEP)\":  dg_to_kd(kd, temp_k=298.15)\n",
    "                })\n",
    "                \n",
    "    return(pd.DataFrame(data))\n",
    "        \n",
    "\n",
    "from rdkit import Chem\n",
    "import os\n",
    "\n",
    "from rdkit import Chem\n",
    "def combine_pdb_sdf(protein_pdb_path, ligand_sdf_path, output_pdb_path):\n",
    "    # Load ligand from SDF\n",
    "    mol = Chem.MolFromMolFile(ligand_sdf_path, removeHs=False)\n",
    "    if mol is None:\n",
    "        raise ValueError(\"Invalid SDF file or molecule failed to load\")\n",
    "\n",
    "    # Set residue and chain info properly\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        info = Chem.AtomPDBResidueInfo()\n",
    "        info.SetName(\" {:<2}\".format(atom.GetSymbol()))  # Proper spacing\n",
    "        info.SetResidueName(\"LIG\")\n",
    "        info.SetResidueNumber(1)\n",
    "        info.SetChainId(\"A\")  # <-- Important for dpocket\n",
    "        info.SetIsHeteroAtom(True)\n",
    "        atom.SetMonomerInfo(info)\n",
    "\n",
    "    # Convert ligand to PDB block\n",
    "    ligand_pdb_block = Chem.MolToPDBBlock(mol)\n",
    "\n",
    "    # Combine protein and ligand\n",
    "    with open(output_pdb_path, \"w\") as out:\n",
    "        with open(protein_pdb_path, \"r\") as prot_file:\n",
    "            for line in prot_file:\n",
    "                if not line.startswith(\"END\"):  # avoid duplicating END\n",
    "                    out.write(line)\n",
    "        out.write(\"\\n\")\n",
    "        out.write(ligand_pdb_block)\n",
    "        out.write(\"END\\n\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def build_hqbind_pose_dataset(metadata_path=\"HQBind/hiqbind_metadata.csv\", root=\"HQBind/raw_data_hiq_sm\"):\n",
    "    df = pd.read_csv(metadata_path)\n",
    "\n",
    "    pose_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        pdbid = row[\"PDBID\"]\n",
    "        ligand_name = row[\"Ligand Name\"]\n",
    "        chain = row[\"Ligand Chain\"]\n",
    "        resnum = row[\"Ligand Residue Number\"]\n",
    "\n",
    "        ligand_dir = f\"{pdbid}_{ligand_name}_{chain}_{resnum}\"\n",
    "        ligand_path = Path(root) / pdbid / ligand_dir / f\"{ligand_dir}_ligand.pdb\"\n",
    "        protein_path = Path(root) / pdbid / ligand_dir / f\"{ligand_dir}_protein.pdb\"\n",
    "\n",
    "        if not ligand_path.exists() or not protein_path.exists():\n",
    "            continue\n",
    "\n",
    "        pose_data.append({\n",
    "            \"PDBID\": pdbid,\n",
    "            \"Resolution\": row[\"Resolution\"],\n",
    "            \"Ligand Name\": ligand_name,\n",
    "            \"Ligand Chain\": chain,\n",
    "            \"Ligand Residue Number\": resnum,\n",
    "            \"Ligand SMILES\": row[\"Ligand SMILES\"],\n",
    "            \"Binding Affinity Measurement\": row[\"Binding Affinity Measurement\"],\n",
    "            \"Binding Affinity Sign\": row[\"Binding Affinity Sign\"],\n",
    "            \"Binding Affinity Value\": row[\"Binding Affinity Value\"],\n",
    "            \"Binding Affinity Unit\": row[\"Binding Affinity Unit\"],\n",
    "            \"Binding Affinity Source\": row[\"Binding Affinity Source\"],\n",
    "            \"Ligand PDB Path\": str(ligand_path),\n",
    "            \"Protein PDB Path\": str(protein_path)\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(pose_data)\n",
    "\n",
    "def format_hqbind(df_pose):\n",
    "\n",
    "    unit_to_M = {\n",
    "    \"M\": 1,\n",
    "    \"mM\": 1e-3,\n",
    "    \"uM\": 1e-6,\n",
    "    \"nM\": 1e-9,\n",
    "    \"pM\": 1e-12\n",
    "    }\n",
    "    \n",
    "    df_pose = df_pose[[\"Ligand PDB Path\", \"Protein PDB Path\", \"Ligand SMILES\", \"Resolution\", \"Binding Affinity Measurement\", \"Binding Affinity Sign\", \"Binding Affinity Value\", \"Binding Affinity Unit\"]]\n",
    "    df_pose[\"Binding Affinity (M)\"] = df_pose.apply(\n",
    "        lambda row: row[\"Binding Affinity Value\"] * unit_to_M.get(row[\"Binding Affinity Unit\"], None)\n",
    "        if pd.notnull(row[\"Binding Affinity Value\"]) and row[\"Binding Affinity Unit\"] in unit_to_M\n",
    "        else None,\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "\n",
    "    df_pose[\"neg log10(M)\"] = df_pose[\"Binding Affinity (M)\"].apply(\n",
    "        lambda x: -np.log10(x) if pd.notnull(x) and x > 0 else None\n",
    "    )\n",
    "    return(df_pose)\n",
    "\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_binding_data(df):\n",
    "    tqdm.pandas()  \n",
    "\n",
    "    # Keep only standard_relation == \"=\"\n",
    "    df = df[df[\"standard_relation\"] == \"=\"].copy()\n",
    "\n",
    "    unit_to_multiplier = {\n",
    "        \"M\": 1,\n",
    "        \"mM\": 1e-3,\n",
    "        \"uM\": 1e-6,\n",
    "        \"ÂµM\": 1e-6,\n",
    "        \"nM\": 1e-9,\n",
    "        \"pM\": 1e-12,\n",
    "        \"fM\": 1e-15,\n",
    "        \"ug.mL-1\": None,\n",
    "        \"mg.mL-1\": None,\n",
    "    }\n",
    "\n",
    "    def to_molar(row):\n",
    "        unit = row[\"standard_units\"]\n",
    "        val = row[\"standard_value\"]\n",
    "        mw = row[\"Molecule MW\"]\n",
    "        if pd.isna(val) or pd.isna(unit) or pd.isna(mw):\n",
    "            return np.nan\n",
    "        if unit in unit_to_multiplier and unit_to_multiplier[unit] is not None:\n",
    "            return float(val) * unit_to_multiplier[unit]\n",
    "        elif unit == \"ug.mL-1\":\n",
    "            return (float(val) * 1e-6) / mw\n",
    "        elif unit == \"mg.mL-1\":\n",
    "            return (float(val) * 1e-3) / mw\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"Molar\"] = df.apply(to_molar, axis=1)\n",
    "    df[\"-log10(M)\"] = -np.log10(df[\"Molar\"])\n",
    "\n",
    "    def path(row, ext):\n",
    "        return f\"../data/raw/BindingNetv1/BindingNetv1/{row['Protein Source']}/target_{row['Target ChEMBLID']}/{row['Molecule ChEMBLID']}/{row['Protein Source']}_{row['Target ChEMBLID']}_{row['Molecule ChEMBLID']}.{ext}\"\n",
    "\n",
    "    df[\"SDF_path\"] = df.progress_apply(lambda r: path(r, \"sdf\"), axis=1)\n",
    "    df[\"PDB_path\"] = df.progress_apply(lambda r: path(r, \"pdb\"), axis=1)\n",
    "\n",
    "    # Group by standard_type\n",
    "    grouped = {std_type: group.reset_index(drop=True) for std_type, group in df.groupby(\"standard_type\")}\n",
    "    return grouped\n",
    "\n",
    "\n",
    "def process_binding_data_bindingdnetv2(df):\n",
    "    tqdm.pandas()  \n",
    "    # Keep only standard_relation == \"=\"\n",
    "    df = df[df[\"standard_relation\"] == \"=\"].copy()\n",
    "\n",
    "    unit_to_multiplier = {\n",
    "        \"M\": 1,\n",
    "        \"mM\": 1e-3,\n",
    "        \"uM\": 1e-6,\n",
    "        \"ÂµM\": 1e-6,\n",
    "        \"nM\": 1e-9,\n",
    "        \"pM\": 1e-12,\n",
    "        \"fM\": 1e-15,\n",
    "        \"ug.mL-1\": None,\n",
    "        \"mg.mL-1\": None,\n",
    "    }\n",
    "\n",
    "    def to_molar(row):\n",
    "        unit = row[\"standard_units\"]\n",
    "        val = row[\"standard_value\"]\n",
    "        mw = row[\"Molecule MW\"]\n",
    "        if pd.isna(val) or pd.isna(unit) or pd.isna(mw):\n",
    "            return np.nan\n",
    "        if unit in unit_to_multiplier and unit_to_multiplier[unit] is not None:\n",
    "            return float(val) * unit_to_multiplier[unit]\n",
    "        elif unit == \"ug.mL-1\":\n",
    "            return (float(val) * 1e-6) / mw\n",
    "        elif unit == \"mg.mL-1\":\n",
    "            return (float(val) * 1e-3) / mw\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"Molar\"] = df.apply(to_molar, axis=1)\n",
    "    df[\"-log10(M)\"] = -np.log10(df[\"Molar\"])\n",
    "\n",
    "    def file_exists_for(row, level):\n",
    "        sdf_path = f\"../data/raw/BindingNetv2/{level}/{row['Protein Source']}/target_{row['Target ChEMBLID']}/{row['Molecule ChEMBLID']}/{row['Protein Source']}_{row['Target ChEMBLID']}_{row['Molecule ChEMBLID']}.sdf\"\n",
    "        pdb_path = f\"../data/raw/BindingNetv2/{level}/{row['Protein Source']}/target_{row['Target ChEMBLID']}/{row['Molecule ChEMBLID']}/{row['Protein Source']}_{row['Target ChEMBLID']}_{row['Molecule ChEMBLID']}.pdb\"\n",
    "        return os.path.exists(sdf_path) and os.path.exists(pdb_path)\n",
    "\n",
    "    def resolve_high_low(row):\n",
    "        return \"high\" if file_exists_for(row, \"high\") else \"moderate\" if file_exists_for(row, \"moderate\") else None\n",
    "\n",
    "    df[\"high_low\"] = df.progress_apply(resolve_high_low, axis=1)\n",
    "    df = df[df[\"high_low\"].notna()].copy()\n",
    "\n",
    "    \n",
    "    def path(row, ext):\n",
    "        return f\"../data/raw/BindingNetv2/{row['high_low']}/{row['Protein Source']}/target_{row['Target ChEMBLID']}/{row['Molecule ChEMBLID']}/{row['Protein Source']}_{row['Target ChEMBLID']}_{row['Molecule ChEMBLID']}.{ext}\"\n",
    "    import os\n",
    "\n",
    "\n",
    "    df[\"SDF_path\"] = df.progress_apply(lambda r: path(r, \"sdf\"), axis=1)\n",
    "    df[\"PDB_path\"] = df.progress_apply(lambda r: path(r, \"pdb\"), axis=1)\n",
    "\n",
    "    # Group by standard_type\n",
    "    grouped = {std_type: group.reset_index(drop=True) for std_type, group in df.groupby(\"standard_type\")}\n",
    "    return grouped\n",
    "\n",
    "import os\n",
    "\n",
    "def process_binding_data_bindingdnetv2(df):\n",
    "    tqdm.pandas()\n",
    "    # Keep only standard_relation == \"=\"\n",
    "    df = df[df[\"standard_relation\"] == \"=\"].copy()\n",
    "\n",
    "    unit_to_multiplier = {\n",
    "        \"M\": 1,\n",
    "        \"mM\": 1e-3,\n",
    "        \"uM\": 1e-6,\n",
    "        \"ÂµM\": 1e-6,\n",
    "        \"nM\": 1e-9,\n",
    "        \"pM\": 1e-12,\n",
    "        \"fM\": 1e-15,\n",
    "        \"ug.mL-1\": None,\n",
    "        \"mg.mL-1\": None,\n",
    "    }\n",
    "\n",
    "    def to_molar(row):\n",
    "        unit = row[\"standard_units\"]\n",
    "        val = row[\"standard_value\"]\n",
    "        mw = row[\"Molecule MW\"]\n",
    "        if pd.isna(val) or pd.isna(unit) or pd.isna(mw):\n",
    "            return np.nan\n",
    "        if unit in unit_to_multiplier and unit_to_multiplier[unit] is not None:\n",
    "            return float(val) * unit_to_multiplier[unit]\n",
    "        elif unit == \"ug.mL-1\":\n",
    "            return (float(val) * 1e-6) / mw\n",
    "        elif unit == \"mg.mL-1\":\n",
    "            return (float(val) * 1e-3) / mw\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "    df[\"Molar\"] = df.apply(to_molar, axis=1)\n",
    "    df[\"-log10(M)\"] = -np.log10(df[\"Molar\"])\n",
    "\n",
    "    def file_exists_for(row, level):\n",
    "        target_id = row['Target ChEMBLID']\n",
    "        mol_id = row['Molecule ChEMBLID']\n",
    "        base = f\"../data/raw/BindingNetv2/{level}/target_{target_id}/{mol_id}\"\n",
    "        return os.path.exists(f\"{base}/ligand.sdf\") and os.path.exists(f\"{base}/protein.pdb\")\n",
    "\n",
    "    def path(row, ext):\n",
    "        return f\"../data/raw/BindingNetv2/{row['high_low']}/target_{row['Target ChEMBLID']}/{row['Molecule ChEMBLID']}/{ext}\"\n",
    "\n",
    "    def resolve_high_low(row):\n",
    "        return \"high\" if file_exists_for(row, \"high\") else \"moderate\" if file_exists_for(row, \"moderate\") else None\n",
    "\n",
    "    df[\"high_low\"] = df.progress_apply(resolve_high_low, axis=1)\n",
    "    df = df[df[\"high_low\"].notna()].copy()\n",
    "\n",
    "\n",
    "    df[\"SDF_path\"] = df.progress_apply(lambda r: path(r, \"ligand.sdf\"), axis=1)\n",
    "    df[\"PDB_path\"] = df.progress_apply(lambda r: path(r, \"protein.pdb\"), axis=1)\n",
    "\n",
    "    # Group by standard_type\n",
    "    grouped = {std_type: group.reset_index(drop=True) for std_type, group in df.groupby(\"standard_type\")}\n",
    "    return grouped, df\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "\n",
    "def perpare_refined_PDBbind2020():\n",
    "    base_dir = '../data/raw/PDBbind2020/PDBbind2020/main'\n",
    "\n",
    "    index_file = os.path.join(base_dir, 'index', 'INDEX_refined_data.2020')\n",
    "    refined_dir = os.path.join(base_dir, 'refined-set')\n",
    "\n",
    "    records = []\n",
    "\n",
    "    with open(index_file, 'r') as f:\n",
    "        for line in tqdm(f):\n",
    "            if line.startswith('#') or not line.strip():\n",
    "                continue\n",
    "\n",
    "            # Split by multiple spaces\n",
    "            parts = re.split(r'\\s{2,}', line.strip())\n",
    "            if len(parts) < 5:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                pdbid = parts[0]\n",
    "                resolution = float(parts[1])\n",
    "                year = int(parts[2])\n",
    "                log_affinity = float(parts[3])\n",
    "                binding_str = parts[4]\n",
    "            except:\n",
    "                continue  # Skip malformed rows\n",
    "\n",
    "            # Parse binding string like \"Ki=0.1uM\", \"Ki<=100nM\"\n",
    "            if '=' in binding_str:\n",
    "                activity_part, value_unit_part = binding_str.split('=')\n",
    "                activity_type = activity_part.strip().replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                value_unit_part = value_unit_part.strip().replace(\"<\", \"\").replace(\">\", \"\")\n",
    "                value_str = ''.join(filter(lambda x: x in '0123456789.-', value_unit_part))\n",
    "                unit = ''.join(filter(lambda x: x.isalpha(), value_unit_part))\n",
    "\n",
    "                try:\n",
    "                    value = float(value_str)\n",
    "                except ValueError:\n",
    "                    continue  # skip invalid value\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # File paths\n",
    "            protein_path = os.path.join(refined_dir, pdbid, f\"{pdbid}_protein.pdb\")\n",
    "            ligand_path = os.path.join(refined_dir, pdbid, f\"{pdbid}_ligand.sdf\")\n",
    "\n",
    "            if not os.path.exists(protein_path) or not os.path.exists(ligand_path):\n",
    "                continue\n",
    "\n",
    "            records.append({\n",
    "                'pdbid': pdbid,\n",
    "                'resolution': resolution,\n",
    "                'year': year,\n",
    "                '-log_affinity': log_affinity,\n",
    "                'activity_type': activity_type,\n",
    "                'value': value,\n",
    "                'unit': unit,\n",
    "                'protein_path': protein_path,\n",
    "                'ligand_path': ligand_path\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    import numpy as np\n",
    "\n",
    "    unit_multipliers = {\n",
    "        'M': 1,\n",
    "        'mM': 1e-3,\n",
    "        'uM': 1e-6,\n",
    "        'nM': 1e-9,\n",
    "        'pM': 1e-12,\n",
    "        'fM': 1e-15\n",
    "    }\n",
    "\n",
    "    def convert_to_molar(row):\n",
    "        factor = unit_multipliers.get(row['unit'], None)\n",
    "        if factor is None or row['value'] <= 0:\n",
    "            return None\n",
    "        return -np.log10(row['value'] * factor)\n",
    "\n",
    "    df['-log10_affinity_M'] = df.apply(convert_to_molar, axis=1)\n",
    "    df\n",
    "    return(df)\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def parse_affinity_column(entry):\n",
    "    if pd.isna(entry):\n",
    "        return {}\n",
    "\n",
    "    pattern = r\"(Ki|Kd|IC50|EC50)=([0-9eE\\.\\+\\-]+)([a-zA-ZÎ¼Âµ]*)\"\n",
    "    result = {}\n",
    "    for match in re.finditer(pattern, entry):\n",
    "        aff_type, val_str, unit = match.groups()\n",
    "        try:\n",
    "            val = float(val_str)\n",
    "            result[f\"{aff_type}_value\"] = val\n",
    "            result[f\"{aff_type}_unit\"] = unit.strip()\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return result\n",
    "\n",
    "def convert_to_log_molar(value, unit):\n",
    "    unit_map = {\n",
    "        'M': 1,\n",
    "        'mM': 1e-3,\n",
    "        'uM': 1e-6,\n",
    "        'Î¼M': 1e-6,\n",
    "        'ÂµM': 1e-6,\n",
    "        'nM': 1e-9,\n",
    "        'pM': 1e-12,\n",
    "        'fM': 1e-15\n",
    "    }\n",
    "    if unit not in unit_map or value <= 0:\n",
    "        return None\n",
    "    return -np.log10(value * unit_map[unit])\n",
    "\n",
    "def extract_smiles_from_pdb(pdb_file):\n",
    "    try:\n",
    "        mol = Chem.MolFromPDBFile(pdb_file, removeHs=False)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        Chem.SanitizeMol(mol)\n",
    "        return Chem.MolToSmiles(mol)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_biolip_dataset(biolip_path, ligand_dir, receptor_dir):\n",
    "    col_names = [\n",
    "        \"pdb_id\", \"receptor_chain\", \"resolution\", \"binding_site_id\", \"ligand_id\",\n",
    "        \"ligand_chain\", \"ligand_serial\", \"bs_residues_pdb\", \"bs_residues_renum\",\n",
    "        \"catalytic_residues_pdb\", \"catalytic_residues_renum\", \"ec_number\",\n",
    "        \"go_terms\", \"affinity_lit\", \"affinity_moad\", \"affinity_pdbbind\",\n",
    "        \"affinity_bindingdb\", \"uniprot_id\", \"pubmed_id\", \"ligand_seq_number\",\n",
    "        \"receptor_sequence\"\n",
    "    ]\n",
    "\n",
    "    df = pd.read_csv(biolip_path, sep=\"\\t\", header=None, names=col_names)\n",
    "\n",
    "    # Extract affinities\n",
    "    sources = [\"affinity_lit\", \"affinity_moad\", \"affinity_pdbbind\", \"affinity_bindingdb\"]\n",
    "    for src in sources:\n",
    "        parsed = df[src].apply(parse_affinity_column)\n",
    "        parsed_df = parsed.apply(pd.Series).add_prefix(f\"{src}_\")\n",
    "        df = pd.concat([df, parsed_df], axis=1)\n",
    "\n",
    "    # Output dict\n",
    "    result_dfs = {\n",
    "        'Ki': [],\n",
    "        'Kd': [],\n",
    "        'IC50': [],\n",
    "        'EC50': []\n",
    "    }\n",
    "\n",
    "    for _, row in tqdm(df.iterrows()):\n",
    "        pdb_id = row[\"pdb_id\"]\n",
    "        ligand_id = row[\"ligand_id\"]\n",
    "        receptor_chain = row[\"receptor_chain\"]\n",
    "\n",
    "        ligand_fname = f\"{pdb_id}_{ligand_id}_{row['ligand_chain']}_{row['ligand_serial']}.pdb\"\n",
    "        receptor_fname = f\"{pdb_id}{receptor_chain}.pdb\"\n",
    "\n",
    "        ligand_path = os.path.join(ligand_dir, ligand_fname)\n",
    "        receptor_path = os.path.join(receptor_dir, receptor_fname)\n",
    "\n",
    "        if not os.path.exists(ligand_path) or not os.path.exists(receptor_path):\n",
    "            continue\n",
    "\n",
    "        smiles = extract_ligands_from_pdb(ligand_path)\n",
    "        if smiles is None:\n",
    "            continue\n",
    "\n",
    "        for aff_type in result_dfs.keys():\n",
    "            # Try from all sources\n",
    "            for source in sources:\n",
    "                val_key = f\"{source}_{aff_type}_value\"\n",
    "                unit_key = f\"{source}_{aff_type}_unit\"\n",
    "                value = row.get(val_key)\n",
    "                unit = row.get(unit_key)\n",
    "                if pd.notna(value) and pd.notna(unit):\n",
    "                    log_aff = convert_to_log_molar(value, unit)\n",
    "                    if log_aff is not None:\n",
    "                        result_dfs[aff_type].append({\n",
    "                            \"protein_path\": receptor_path,\n",
    "                            \"ligand_path\": ligand_path,\n",
    "                            \"smiles\": smiles,\n",
    "                            \"-log10_affinity_M\": log_aff\n",
    "                        })\n",
    "                        break  # stop at first valid source\n",
    "\n",
    "    # Create dataframes\n",
    "    return {k: pd.DataFrame(v) for k, v in result_dfs.items()}\n",
    "\n",
    "\n",
    "def extract_ligands_from_pdb(file_path):\n",
    "    from rdkit import Chem\n",
    "    import os\n",
    "\n",
    "    smiles_list = []\n",
    "\n",
    "    if not os.path.isfile(file_path):\n",
    "        return [None]\n",
    "\n",
    "    ext = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "    try:\n",
    "        if file_path.endswith('.sdf'):\n",
    "            suppl = Chem.SDMolSupplier(file_path, sanitize=False, removeHs=True)\n",
    "            if isinstance(suppl, list) or hasattr(suppl, '__iter__'):\n",
    "                for mol in suppl:\n",
    "                    if mol:\n",
    "                        smiles_list.append(Chem.MolToSmiles(mol))\n",
    "                        break\n",
    "            else:\n",
    "                smiles_list.append(Chem.MolToSmiles(suppl))\n",
    "\n",
    "        elif file_path.endswith('.pdb'):\n",
    "            suppl = Chem.MolFromPDBFile(file_path, sanitize=False, removeHs=True)\n",
    "            if isinstance(suppl, list) or hasattr(suppl, '__iter__'):\n",
    "                for mol in suppl:\n",
    "                    if mol:\n",
    "                        smiles_list.append(Chem.MolToSmiles(mol))\n",
    "                        break\n",
    "            else:\n",
    "                smiles_list.append(Chem.MolToSmiles(suppl))\n",
    "        else:\n",
    "            smiles_list.append(None)\n",
    "\n",
    "    except Exception:\n",
    "        smiles_list.append(None)\n",
    "\n",
    "    return smiles_list[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf318b-9d40-4fc4-a2ad-93ec1f5ca251",
   "metadata": {},
   "source": [
    "## 1. Wang 2015 FEP - Ross structures (from dG to Kd using Gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0bc4ee-f641-47eb-8291-834bc9d471a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = \"../data/raw/Wang_2015_Ross_structures\"\n",
    "df = prepare_wang_data(base_dir)\n",
    "df = df[[\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd (Wang, FEP)\"]]\n",
    "print(df.columns.tolist())\n",
    "df[\"resolution\"] = None\n",
    "df.to_parquet(\"../data/curated/exp/pKd_FEP_Wang_2015.parquet\", index = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6c3251-9739-4420-aab2-27dc74322645",
   "metadata": {},
   "source": [
    "## 2. Zariquiey structures extended from Wang 2015 (from dG to Kd using Gibbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e643794a-1d2d-413c-8f03-c1941238e4e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = \"../data/raw/Wang_2015_Zariquiey_structures_extended\"\n",
    "df = prepare_wang_data(base_dir)\n",
    "df = df[[\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd (Wang, FEP)\"]]\n",
    "print(df.columns.tolist())\n",
    "df[\"resolution\"] = None\n",
    "df.to_parquet(\"../data/curated/exp/pKd_FEP_Zariquiey_extended_Wang_2015.parquet\", index = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a0047c-8c71-488a-a9eb-751d2e1731fe",
   "metadata": {},
   "source": [
    "## 3. PDBbind2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b21e6-d1cc-4d85-9823-284f14ba3ac5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = perpare_refined_PDBbind2020()\n",
    "df['smiles'] = [extract_ligands_from_pdb(i) for i in tqdm(df[\"ligand_path\"].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7719b5-0dd8-41b0-8557-d1b685d72f81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee48733-a9c7-4c07-90cf-a1848aede5c8",
   "metadata": {},
   "source": [
    "a. pKi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd481c2f-cb05-40e6-a18a-b1cadf73e4e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Ki = df[df[\"activity_type\"]==\"Ki\"]\n",
    "df_Ki = df_Ki[[\"protein_path\",\"ligand_path\", \"smiles\", \"-log10_affinity_M\", 'resolution']]\n",
    "df_Ki.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKi\", 'resolution']\n",
    "df_Ki.to_parquet(\"../data/curated/exp/pKi_PDBbind2020.parquet\", index = False)\n",
    "df_Ki.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0495b84-9e51-4e51-84df-ee13474b6ce8",
   "metadata": {},
   "source": [
    "b. pKd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde3461-9df6-41d1-8aab-71d8ee8bee07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Kd = df[df[\"activity_type\"]==\"Kd\"]\n",
    "df_Kd = df_Kd[[\"protein_path\",\"ligand_path\", \"smiles\", \"-log10_affinity_M\", 'resolution']]\n",
    "df_Kd.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd\", 'resolution']\n",
    "df_Kd.to_parquet(\"../data/curated/exp/pKd_PDBbind2020.parquet\", index = False)\n",
    "df_Kd.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775dff2-b477-4cc6-aa1d-39d5ce854037",
   "metadata": {},
   "source": [
    "## 4. HQBind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6edad-387f-4a03-a4f2-b1661f62a4b3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "metadata_path=\"../data/raw/HiQBind//hiqbind_metadata.csv\"\n",
    "root_path=\"../data/raw/HiQBind//raw_data_hiq_sm\"\n",
    "\n",
    "df = build_hqbind_pose_dataset(metadata_path, root_path)\n",
    "df = format_hqbind(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c3ac3-b255-4e4e-a52a-12aacb916015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad7261-4243-4310-a45c-63a4b3b6c548",
   "metadata": {},
   "source": [
    "a. pKi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13a5eea-736a-49e6-bc93-ab4f2c6f3b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Ki = df[df[\"Binding Affinity Measurement\"]==\"ki\"]\n",
    "df_Ki = df_Ki[[\"Protein PDB Path\",\"Ligand PDB Path\", \"Ligand SMILES\", \"neg log10(M)\", \"Resolution\"]]\n",
    "df_Ki.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKi\", \"resolution\"]\n",
    "df_Ki.to_parquet(\"../data/curated/exp/pKi_HiQBind.parquet\", index = False)\n",
    "df_Ki.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925bee91-9c0b-40cb-9620-92679e7cb7ad",
   "metadata": {},
   "source": [
    "b. pKd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941be66b-9d38-4d19-9c99-eda37563c011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Kd = df[df[\"Binding Affinity Measurement\"]==\"kd\"]\n",
    "df_Kd = df_Kd[[\"Protein PDB Path\",\"Ligand PDB Path\", \"Ligand SMILES\", \"neg log10(M)\", \"Resolution\"]]\n",
    "df_Kd.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd\", \"resolution\"]\n",
    "df_Kd.to_parquet(\"../data/curated/exp/pKd_HiQBind.parquet\", index = False)\n",
    "df_Kd.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6e1195-4556-458c-b6eb-aec043777ecb",
   "metadata": {},
   "source": [
    "c. pIC50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f1347-0c8e-4c63-a4f6-f3cdd2184dd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_IC50 = df[df[\"Binding Affinity Measurement\"]==\"ic50\"]\n",
    "df_IC50 = df_IC50[[\"Protein PDB Path\",\"Ligand PDB Path\", \"Ligand SMILES\", \"neg log10(M)\", \"Resolution\"]]\n",
    "df_IC50.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pIC50\", \"resolution\"]\n",
    "df_IC50.to_parquet(\"../data/curated/exp/pIC50_HiQBind.parquet\", index = False)\n",
    "df_IC50.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6959d68f-b10e-4065-9e7e-263e83b13863",
   "metadata": {},
   "source": [
    "a. pEC50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb512e-2fef-45c2-b924-3bb6e48b8872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_EC50 = df[df[\"Binding Affinity Measurement\"]==\"ec50\"]\n",
    "df_EC50 = df_EC50[[\"Protein PDB Path\",\"Ligand PDB Path\", \"Ligand SMILES\", \"neg log10(M)\", \"Resolution\"]]\n",
    "df_EC50.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pEC50\", \"resolution\"]\n",
    "df_EC50.to_parquet(\"../data/curated/exp/pEC50_HiQBind.parquet\", index = False)\n",
    "df_EC50.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d161b425-7f32-4f3a-ba78-d487173b3a57",
   "metadata": {},
   "source": [
    "# 5. BioLip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb0309-4be1-497a-94e1-1cde1357c85b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "biolip_path = \"../data/raw/BioLip2/BioLigInfo.txt\"\n",
    "ligand_dir = \"../data/raw/BioLip2/biolip_downloads/biolip_redundant_all/ligand\"\n",
    "receptor_dir = \"../data/raw/BioLip2/biolip_downloads/biolip_redundant_all/receptor\"\n",
    "datasets = process_biolip_dataset(biolip_path, ligand_dir, receptor_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ceda9-a772-48ce-befa-f6388a5f7c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = datasets[\"Ki\"]\n",
    "df.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKi\"]\n",
    "df[\"resolution\"] = None\n",
    "df.to_parquet(\"../data/curated/exp/pKi_BioLip2.parquet\", index = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36298fd1-ef3c-430c-af52-1a7c768a1b33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = datasets[\"Kd\"]\n",
    "df.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd\"]\n",
    "df[\"resolution\"] = None\n",
    "df.to_parquet(\"../data/curated/exp/pKd_BioLip2.parquet\", index = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb02393-3a3d-49d3-8901-c1ffb3b50f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = datasets[\"IC50\"]\n",
    "df.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pIC50\"]\n",
    "df[\"resolution\"] = None\n",
    "df.to_parquet(\"../data/curated/exp/pIC50_BioLip2.parquet\", index = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934582ab-6fe3-48dc-afda-a7a8311b077c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = datasets[\"EC50\"]\n",
    "df.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pEC50\"]\n",
    "df[\"resolution\"] = None\n",
    "df.to_parquet(\"../data/curated/exp/pEC50_BioLip2.parquet\", index = False)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414f6c1-15b1-4690-90ed-d1775232d648",
   "metadata": {},
   "source": [
    "# 6. BindingNet v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560efe86-0ac3-45fa-a8eb-d590b1b073fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/BindingNetv2/Index_for_BindingNetv1_and_BindingNetv2.csv\")\n",
    "df = df[df[\"Dataset\"]==\"BindingNet v1\"]\n",
    "print(df.columns.tolist())\n",
    "data_by_type = process_binding_data(df)\n",
    "# Example: access Ki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a6c0e-67af-4f93-b3f8-77c05e24fd0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Ki = data_by_type.get(\"Ki\")\n",
    "df_Ki = df_Ki[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_Ki.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKi\"]\n",
    "df[\"resolution\"] = None\n",
    "df_Ki.to_parquet(\"../data/curated/exp/pKi_BindingNetv1.parquet\", index = False)\n",
    "df_Ki.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a45873f-05bd-4009-8c16-71e3aae0fa71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Kd = data_by_type.get(\"Kd\")\n",
    "df_Kd = df_Kd[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_Kd.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd\"]\n",
    "df[\"resolution\"] = None\n",
    "df_Kd.to_parquet(\"../data/curated/exp/pKd_BindingNetv1.parquet\", index = False)\n",
    "df_Kd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cd8ee-98bc-4ee2-9795-943da6b0e215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_IC50 = data_by_type.get(\"IC50\")\n",
    "df_IC50 = df_IC50[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_IC50.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pEC50\"]\n",
    "df[\"resolution\"] = None\n",
    "df_IC50.to_parquet(\"../data/curated/exp/pIC50_BindingNetv1.parquet\", index = False)\n",
    "df_IC50.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91404ae1-ff22-429e-9d53-8389d71f0b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_EC50 = data_by_type.get(\"EC50\")\n",
    "df_EC50 = df_EC50[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_EC50.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pEC50\"]\n",
    "df[\"resolution\"] = None\n",
    "df_EC50.to_parquet(\"../data/curated/exp/pEC50_BindingNetv1.parquet\", index = False)\n",
    "df_EC50.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d282c2cc-258c-478e-abfc-ca451d2954ce",
   "metadata": {},
   "source": [
    "# 7. BindingNet v2\n",
    "> I only downloaded the mid and high quality, not the low, for storage reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fa4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/BindingNetv2/Index_for_BindingNetv1_and_BindingNetv2.csv\")\n",
    "df = df[df[\"Dataset\"]==\"BindingNet v2\"]\n",
    "print(df.columns.tolist())\n",
    "data_by_type, df = process_binding_data_bindingdnetv2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df09b0-0b43-4cc9-bdf6-24c0555fd11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/BindingNetv2/Index_for_BindingNetv1_and_BindingNetv2.csv\")\n",
    "df = df[df[\"Dataset\"]==\"BindingNet v2\"]\n",
    "df.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ad068-b594-49fe-abdd-9c723d8e0f80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Ki = data_by_type.get(\"Ki\")\n",
    "df_Ki = df_Ki[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_Ki.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKi\"]\n",
    "df[\"resolution\"] = None\n",
    "df_Ki.to_parquet(\"../data/curated/exp/pKi_BindingNetv2.parquet\", index = False)\n",
    "df_Ki.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a661b133-8e83-4669-8286-5e0a0ab5aeb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_Kd = data_by_type.get(\"Kd\")\n",
    "df_Kd = df_Kd[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_Kd.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pKd\"]\n",
    "df[\"resolution\"] = None\n",
    "df_Kd.to_parquet(\"../data/curated/exp/pKd_BindingNetv2.parquet\", index = False)\n",
    "df_Kd.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485255fb-4e85-4e9e-85b2-adcbea7c722c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_IC50 = data_by_type.get(\"IC50\")\n",
    "df_IC50 = df_IC50[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_IC50.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pIC50\"]\n",
    "df[\"resolution\"] = None\n",
    "df_IC50.to_parquet(\"../data/curated/exp/pIC50_BindingNetv2.parquet\", index = False)\n",
    "df_IC50.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da540e12-c45c-4a7b-8feb-ddda2a9ec164",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_EC50 = data_by_type.get(\"EC50\")\n",
    "df_EC50 = df_EC50[[\"PDB_path\",\"SDF_path\",\"Molecule SMILES\", \"-log10(M)\"]]\n",
    "df_EC50.columns = [\"protein_pdb_path\",\"ligand_sdf_path\", \"smiles\", \"pEC50\"]\n",
    "df[\"resolution\"] = None\n",
    "df_EC50.to_parquet(\"../data/curated/exp/pEC50_BindingNetv2.parquet\", index = False)\n",
    "df_EC50.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590bf4bd-136f-4af4-88fa-97aadff575d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 8. SAIR Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344b9c7-02b0-49c7-9238-9508d21d3f61",
   "metadata": {},
   "source": [
    "- unzip with \n",
    "> 7z x file.zip.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd594da-03b0-4264-a7f9-418d8ca729f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import time\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import required libraries\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdmolfiles\n",
    "from Bio.PDB import MMCIFParser, PDBIO, Select\n",
    "\n",
    "class ProteinSelect(Select):\n",
    "    def accept_residue(self, residue):\n",
    "        return residue.get_id()[0] == ' '\n",
    "\n",
    "class LigandSelect(Select):\n",
    "    def accept_residue(self, residue):\n",
    "        return residue.get_id()[0] != ' '\n",
    "\n",
    "def check_files_exist_and_valid(protein_path, ligand_path, min_size_bytes=50):\n",
    "    \"\"\"Check if both protein and ligand files exist and are valid\"\"\"\n",
    "    try:\n",
    "        if not (os.path.exists(protein_path) and os.path.exists(ligand_path)):\n",
    "            return False\n",
    "        \n",
    "        protein_size = os.path.getsize(protein_path)\n",
    "        ligand_size = os.path.getsize(ligand_path)\n",
    "        \n",
    "        if protein_size < min_size_bytes or ligand_size < min_size_bytes:\n",
    "            return False\n",
    "            \n",
    "        # Quick content validation\n",
    "        try:\n",
    "            with open(protein_path, 'r') as f:\n",
    "                first_line = f.readline().strip()\n",
    "                if not (first_line.startswith(('ATOM', 'HETATM', 'MODEL', 'HEADER'))):\n",
    "                    return False\n",
    "            \n",
    "            with open(ligand_path, 'r') as f:\n",
    "                content = f.read(100)\n",
    "                if len(content.strip()) < 10:\n",
    "                    return False\n",
    "                    \n",
    "        except Exception:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def process_single_cif(cif_path, protein_dir, ligand_dir):\n",
    "    \"\"\"Process a single CIF file\"\"\"\n",
    "    \n",
    "    cif_filename = os.path.basename(cif_path)\n",
    "    pdb_filename = cif_filename.replace(\".cif\", \".pdb\")\n",
    "    sdf_filename = cif_filename.replace(\".cif\", \".sdf\")\n",
    "    \n",
    "    pdb_path = os.path.join(protein_dir, pdb_filename)\n",
    "    sdf_path = os.path.join(ligand_dir, sdf_filename)\n",
    "    \n",
    "    # Skip if files already exist and are valid\n",
    "    if check_files_exist_and_valid(pdb_path, sdf_path):\n",
    "        return {\n",
    "            'protein_path': pdb_path,\n",
    "            'ligand_path': sdf_path,\n",
    "            'status': 'already_exists',\n",
    "            'success': True\n",
    "        }\n",
    "    \n",
    "    # Clean up any partially created files\n",
    "    for path in [pdb_path, sdf_path]:\n",
    "        if os.path.exists(path) and os.path.getsize(path) < 50:\n",
    "            try:\n",
    "                os.remove(path)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    try:\n",
    "        # Parse CIF file\n",
    "        parser = MMCIFParser(QUIET=True)\n",
    "        structure = parser.get_structure(\"complex\", cif_path)\n",
    "        \n",
    "        # Write protein PDB\n",
    "        io = PDBIO()\n",
    "        io.set_structure(structure)\n",
    "        io.save(pdb_path, select=ProteinSelect())\n",
    "        \n",
    "        # Write ligand to temporary PDB first\n",
    "        ligand_temp_pdb = sdf_path.replace(\".sdf\", \"_temp.pdb\")\n",
    "        io.save(ligand_temp_pdb, select=LigandSelect())\n",
    "        \n",
    "        # Convert ligand PDB to SDF using RDKit\n",
    "        mol = rdmolfiles.MolFromPDBFile(ligand_temp_pdb, removeHs=False)\n",
    "        \n",
    "        if mol is not None:\n",
    "            writer = Chem.SDWriter(sdf_path)\n",
    "            writer.write(mol)\n",
    "            writer.close()\n",
    "            \n",
    "            # Clean up temp file\n",
    "            if os.path.exists(ligand_temp_pdb):\n",
    "                os.remove(ligand_temp_pdb)\n",
    "            \n",
    "            # Final validation\n",
    "            if check_files_exist_and_valid(pdb_path, sdf_path):\n",
    "                return {\n",
    "                    'protein_path': pdb_path,\n",
    "                    'ligand_path': sdf_path,\n",
    "                    'status': 'converted_successfully',\n",
    "                    'success': True\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'protein_path': pdb_path,\n",
    "                    'ligand_path': sdf_path,\n",
    "                    'status': 'validation_failed',\n",
    "                    'success': False\n",
    "                }\n",
    "        else:\n",
    "            # RDKit conversion failed\n",
    "            if os.path.exists(ligand_temp_pdb):\n",
    "                os.remove(ligand_temp_pdb)\n",
    "            return {\n",
    "                'protein_path': pdb_path,\n",
    "                'ligand_path': sdf_path,\n",
    "                'status': 'rdkit_failed',\n",
    "                'success': False\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Clean up any partial files\n",
    "        for path in [pdb_path, sdf_path]:\n",
    "            if os.path.exists(path):\n",
    "                try:\n",
    "                    os.remove(path)\n",
    "                except:\n",
    "                    pass\n",
    "                    \n",
    "        return {\n",
    "            'protein_path': pdb_path,\n",
    "            'ligand_path': sdf_path,\n",
    "            'status': f'error: {str(e)[:30]}',\n",
    "            'success': False\n",
    "        }\n",
    "\n",
    "class SimpleSAIRProcessor:\n",
    "    \"\"\"Simple row-by-row SAIR processor\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, structure_dir, output_dir):\n",
    "        self.csv_path = csv_path\n",
    "        self.structure_dir = structure_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        # Set up directories\n",
    "        self.protein_dir = os.path.join(output_dir, \"protein\")\n",
    "        self.ligand_dir = os.path.join(output_dir, \"ligand\")\n",
    "        \n",
    "        Path(self.protein_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.ligand_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.checkpoint_file = f\"{output_dir}/simple_checkpoint.pkl\"\n",
    "        \n",
    "        print(f\"ðŸ”¬ Simple SAIR Processor initialized\")\n",
    "        print(f\"ðŸ“ Output: {output_dir}\")\n",
    "    \n",
    "    def get_available_cif_files(self):\n",
    "        \"\"\"Get all CIF files actually available in the structure directory\"\"\"\n",
    "        print(\"ðŸ“‚ Scanning structure directory for CIF files...\")\n",
    "        \n",
    "        cif_files = []\n",
    "        if os.path.exists(self.structure_dir):\n",
    "            for file in os.listdir(self.structure_dir):\n",
    "                if file.endswith('.cif'):\n",
    "                    cif_files.append(file)\n",
    "        \n",
    "        print(f\"Found {len(cif_files):,} CIF files in structure directory\")\n",
    "        return set(cif_files)  # Return as set for fast lookup\n",
    "    \n",
    "    def filter_dataframe_by_available_files(self):\n",
    "        \"\"\"Filter experimental data to only include rows with existing CIF files\"\"\"\n",
    "        print(\"ðŸ“Š Loading experimental data...\")\n",
    "        df_exp = pd.read_csv(self.csv_path)\n",
    "        print(f\"Original dataset: {len(df_exp):,} entries\")\n",
    "        \n",
    "        # Get available CIF files\n",
    "        available_cifs = self.get_available_cif_files()\n",
    "        \n",
    "        # Filter dataframe\n",
    "        print(\"ðŸ” Filtering data by available CIF files...\")\n",
    "        \n",
    "        # Extract CIF filename from path and check if it exists\n",
    "        def cif_exists(path):\n",
    "            cif_filename = os.path.basename(path)\n",
    "            return cif_filename in available_cifs\n",
    "        \n",
    "        df_filtered = df_exp[df_exp['path'].apply(cif_exists)].copy()\n",
    "        \n",
    "        print(f\"Filtered dataset: {len(df_filtered):,} entries\")\n",
    "        print(f\"Removed {len(df_exp) - len(df_filtered):,} entries without CIF files\")\n",
    "        \n",
    "        return df_filtered\n",
    "    \n",
    "    def check_existing_processed(self, df):\n",
    "        \"\"\"Check how many files are already processed\"\"\"\n",
    "        print(\"ðŸ” Checking existing processed files...\")\n",
    "        \n",
    "        existing_count = 0\n",
    "        for idx, row in df.iterrows():\n",
    "            cif_filename = os.path.basename(row[\"path\"])\n",
    "            pdb_filename = cif_filename.replace(\".cif\", \".pdb\")\n",
    "            sdf_filename = cif_filename.replace(\".cif\", \".sdf\")\n",
    "            \n",
    "            pdb_path = os.path.join(self.protein_dir, pdb_filename)\n",
    "            sdf_path = os.path.join(self.ligand_dir, sdf_filename)\n",
    "            \n",
    "            if check_files_exist_and_valid(pdb_path, sdf_path):\n",
    "                existing_count += 1\n",
    "        \n",
    "        print(f\"Already processed: {existing_count:,}/{len(df):,} ({existing_count/len(df)*100:.1f}%)\")\n",
    "        return existing_count\n",
    "    \n",
    "    def process_all_simple(self, save_every=1000):\n",
    "        \"\"\"Simple row-by-row processing with minimal output\"\"\"\n",
    "        \n",
    "        print(\"ðŸš€ Starting simple row-by-row processing...\")\n",
    "        \n",
    "        # Filter data by available CIF files\n",
    "        df_filtered = self.filter_dataframe_by_available_files()\n",
    "        \n",
    "        if len(df_filtered) == 0:\n",
    "            print(\"âŒ No matching CIF files found!\")\n",
    "            return None, None\n",
    "        \n",
    "        # Check existing processed files\n",
    "        existing_count = self.check_existing_processed(df_filtered)\n",
    "        \n",
    "        # Initialize tracking\n",
    "        results = {}\n",
    "        processed_count = 0\n",
    "        success_count = existing_count\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load checkpoint if exists\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            try:\n",
    "                with open(self.checkpoint_file, 'rb') as f:\n",
    "                    checkpoint_data = pickle.load(f)\n",
    "                    results = checkpoint_data.get('results', {})\n",
    "                    processed_count = len(results)\n",
    "                print(f\"ðŸ“‹ Loaded checkpoint: {processed_count:,} files in progress\")\n",
    "            except:\n",
    "                print(\"âš ï¸ Could not load checkpoint, starting fresh\")\n",
    "        \n",
    "        print(f\"âš¡ Processing {len(df_filtered):,} entries...\")\n",
    "        print(\"ðŸ”„ Progress will be shown every 1000 files\")\n",
    "        \n",
    "        # Process row by row\n",
    "        with tqdm(total=len(df_filtered), desc=\"Processing\", \n",
    "                  initial=processed_count, unit=\"files\") as pbar:\n",
    "            \n",
    "            for idx, row in df_filtered.iterrows():\n",
    "                # Skip if already processed\n",
    "                if idx in results:\n",
    "                    continue\n",
    "                \n",
    "                cif_path = os.path.join(self.structure_dir, os.path.basename(row[\"path\"]))\n",
    "                \n",
    "                # Process the file\n",
    "                result = process_single_cif(cif_path, self.protein_dir, self.ligand_dir)\n",
    "                results[idx] = result\n",
    "                \n",
    "                if result['success']:\n",
    "                    success_count += 1\n",
    "                \n",
    "                processed_count += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # Update progress info occasionally\n",
    "                if processed_count % 100 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'Success': f\"{success_count:,}\",\n",
    "                        'Rate': f\"{success_count/processed_count*100:.1f}%\"\n",
    "                    })\n",
    "                \n",
    "                # Save checkpoint periodically\n",
    "                if processed_count % save_every == 0:\n",
    "                    self.save_checkpoint(results, processed_count)\n",
    "        \n",
    "        # Final save\n",
    "        self.save_checkpoint(results, processed_count)\n",
    "        \n",
    "        # Print summary\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nðŸŽ‰ Processing complete!\")\n",
    "        print(f\"  Total entries processed: {processed_count:,}\")\n",
    "        print(f\"  Successfully converted: {success_count:,}\")\n",
    "        print(f\"  Success rate: {success_count/len(df_filtered)*100:.1f}%\")\n",
    "        print(f\"  Processing time: {elapsed_time/60:.1f} minutes\")\n",
    "        print(f\"  Rate: {processed_count/elapsed_time:.1f} files/second\")\n",
    "        \n",
    "        return self.create_final_dataframe(df_filtered, results)\n",
    "    \n",
    "    def save_checkpoint(self, results, processed_count):\n",
    "        \"\"\"Save checkpoint\"\"\"\n",
    "        checkpoint_data = {\n",
    "            'results': results,\n",
    "            'processed_count': processed_count,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        \n",
    "        with open(self.checkpoint_file, 'wb') as f:\n",
    "            pickle.dump(checkpoint_data, f)\n",
    "    \n",
    "    def create_final_dataframe(self, df_filtered, results):\n",
    "        \"\"\"Create final processed dataframe\"\"\"\n",
    "        print(\"\\nðŸ“‹ Creating final datasets...\")\n",
    "        \n",
    "        # Add result paths to dataframe\n",
    "        protein_paths = []\n",
    "        ligand_paths = []\n",
    "        success_flags = []\n",
    "        \n",
    "        for idx, row in df_filtered.iterrows():\n",
    "            if idx in results and results[idx]['success']:\n",
    "                protein_paths.append(results[idx]['protein_path'])\n",
    "                ligand_paths.append(results[idx]['ligand_path'])\n",
    "                success_flags.append(True)\n",
    "            else:\n",
    "                protein_paths.append('')\n",
    "                ligand_paths.append('')\n",
    "                success_flags.append(False)\n",
    "        \n",
    "        df_filtered['protein_pdb_path'] = protein_paths\n",
    "        df_filtered['ligand_sdf_path'] = ligand_paths\n",
    "        df_filtered['conversion_success'] = success_flags\n",
    "        \n",
    "        # Keep only successful conversions\n",
    "        df_final = df_filtered[df_filtered['conversion_success']].copy()\n",
    "        df_final = df_final[[\"protein_pdb_path\", \"ligand_sdf_path\", \"SMILES\", \"potency\", \"assay\"]]\n",
    "        \n",
    "        print(f\"âœ… Final dataset: {len(df_final):,} valid entries\")\n",
    "        \n",
    "        # Split by assay type and save\n",
    "        assay_types = [\"biochem\", \"na\", \"cell\", \"homogenate\"]\n",
    "        split_dfs = {}\n",
    "        \n",
    "        for assay in assay_types:\n",
    "            df_assay = df_final[df_final[\"assay\"] == assay].copy()\n",
    "            if len(df_assay) > 0:\n",
    "                df_assay = df_assay.rename(columns={\"potency\": f\"potency ({assay})\"})\n",
    "                split_dfs[assay] = df_assay\n",
    "                \n",
    "                # Save to CSV\n",
    "                output_csv = f\"{self.output_dir}/{assay}_data.csv\"\n",
    "                df_assay.to_csv(output_csv, index=False)\n",
    "                print(f\"ðŸ’¾ Saved {len(df_assay):,} {assay} entries\")\n",
    "        \n",
    "        # Save complete dataset\n",
    "        complete_csv = f\"{self.output_dir}/complete_processed_data.csv\"\n",
    "        df_final.to_csv(complete_csv, index=False)\n",
    "        print(f\"ðŸ’¾ Saved complete dataset\")\n",
    "        \n",
    "        return df_final, split_dfs\n",
    "\n",
    "# Convenience function for Jupyter\n",
    "def run_simple_processing(csv_path, structure_dir, output_dir):\n",
    "    \"\"\"Run simple processing\"\"\"\n",
    "    processor = SimpleSAIRProcessor(csv_path, structure_dir, output_dir)\n",
    "    return processor.process_all_simple()\n",
    "\n",
    "def quick_check_available_files(csv_path, structure_dir):\n",
    "    \"\"\"Quick check of available files\"\"\"\n",
    "    processor = SimpleSAIRProcessor(csv_path, structure_dir, \"/tmp\")\n",
    "    df_filtered = processor.filter_dataframe_by_available_files()\n",
    "    return len(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2256cd7-0814-4ce7-b0ef-cf14c613cbb1",
   "metadata": {},
   "source": [
    "# SAIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25efabaf-25c3-4b22-b88c-cb06b78f9441",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Simple SAIR CIF Processor - Jupyter Usage\n",
    "# =============================================================================\n",
    "\n",
    "# Cell 1: Setup and check available files\n",
    "\n",
    "# Configuration\n",
    "CSV_PATH = \"../data/raw/SAIR/best_models.csv\"\n",
    "STRUCTURE_DIR = \"../data/raw/SAIR/structures/\"\n",
    "OUTPUT_DIR = \"../data/raw/SAIR_split/\"\n",
    "\n",
    "print(\"ðŸ”¬ Simple SAIR CIF Processor\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Quick check of how many files we can actually process\n",
    "available_count = quick_check_available_files(CSV_PATH, STRUCTURE_DIR)\n",
    "print(f\"ðŸ“Š Files available for processing: {available_count:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c4d40-59bf-41fc-99a6-9fed7c5e690f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 2: Run the simple processing\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸš€ Starting simple processing...\")\n",
    "\n",
    "# Run processing - this will handle everything automatically\n",
    "df_final, split_dfs = run_simple_processing(\n",
    "    csv_path=CSV_PATH,\n",
    "    structure_dir=STRUCTURE_DIR,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(\"âœ… Processing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2854b8-48d2-4154-b7c0-0a6442da8bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 3: Check results\n",
    "# =============================================================================\n",
    "\n",
    "if df_final is not None:\n",
    "    print(\"ðŸ“Š FINAL RESULTS\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Total processed entries: {len(df_final):,}\")\n",
    "    \n",
    "    if split_dfs:\n",
    "        for assay_type, df in split_dfs.items():\n",
    "            print(f\"  {assay_type}: {len(df):,} entries\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nðŸ“‹ Sample of processed data:\")\n",
    "    print(df_final.head())\n",
    "else:\n",
    "    print(\"âŒ No files were processed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fca6d7-b584-40f4-9eaa-043d5627c8f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 4: Monitor progress (run during processing if needed)\n",
    "# =============================================================================\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "checkpoint_file = f\"{OUTPUT_DIR}/simple_checkpoint.pkl\"\n",
    "\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    results = data.get('results', {})\n",
    "    processed_count = data.get('processed_count', 0)\n",
    "    timestamp = data.get('timestamp', 0)\n",
    "    \n",
    "    successful = sum(1 for r in results.values() if r['success'])\n",
    "    \n",
    "    print(f\"ðŸ“Š Current Progress ({datetime.fromtimestamp(timestamp)})\")\n",
    "    print(f\"  Processed: {processed_count:,}\")\n",
    "    print(f\"  Successful: {successful:,}\")\n",
    "    if processed_count > 0:\n",
    "        print(f\"  Success rate: {successful/processed_count*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No checkpoint file found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adf214e-1d6f-426f-8a95-7335e6a51669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 5: Validate some output files\n",
    "# =============================================================================\n",
    "\n",
    "if df_final is not None and len(df_final) > 0:\n",
    "    \n",
    "    # Check a few files\n",
    "    sample_size = min(5, len(df_final))\n",
    "    sample_files = df_final.sample(sample_size)\n",
    "    \n",
    "    print(f\"ðŸ” Validating {sample_size} random files:\")\n",
    "    \n",
    "    for _, row in sample_files.iterrows():\n",
    "        protein_path = row['protein_pdb_path']\n",
    "        ligand_path = row['ligand_sdf_path']\n",
    "        \n",
    "        is_valid = check_files_exist_and_valid(protein_path, ligand_path)\n",
    "        status = \"âœ…\" if is_valid else \"âŒ\"\n",
    "        \n",
    "        protein_size = os.path.getsize(protein_path) if os.path.exists(protein_path) else 0\n",
    "        ligand_size = os.path.getsize(ligand_path) if os.path.exists(ligand_path) else 0\n",
    "        \n",
    "        print(f\"{status} Protein: {protein_size:,}B, Ligand: {ligand_size:,}B\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1cb25-b30c-4938-a652-799d07aadeed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Cell 6: Check final output structure\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ðŸ“ Output directory contents:\")\n",
    "\n",
    "if os.path.exists(OUTPUT_DIR):\n",
    "    # Count files\n",
    "    protein_files = len([f for f in os.listdir(f\"{OUTPUT_DIR}/protein\") if f.endswith('.pdb')])\n",
    "    ligand_files = len([f for f in os.listdir(f\"{OUTPUT_DIR}/ligand\") if f.endswith('.sdf')])\n",
    "    csv_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.csv')]\n",
    "    \n",
    "    print(f\"  ðŸ§¬ Protein PDB files: {protein_files:,}\")\n",
    "    print(f\"  ðŸ’Š Ligand SDF files: {ligand_files:,}\")\n",
    "    print(f\"  ðŸ“Š CSV datasets: {len(csv_files)}\")\n",
    "    \n",
    "    for csv_file in tqdm(csv_files):\n",
    "        if \"complete_processed_data.csv\" in csv_file:\n",
    "            df_check = pd.read_csv(f\"{OUTPUT_DIR}/{csv_file}\")\n",
    "            parquet_file = csv_file.replace(\".csv\",\".parquet\")\n",
    "            df_check.to_parquet(f'../data/curated/exp/{parquet_file}')\n",
    "            print(f\"    - {csv_file}: {len(df_check):,} entries\")\n",
    "else:\n",
    "    print(\"  Output directory not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6a5af-a507-4572-8cea-08c3aefeb532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e06b747-8536-47ba-b9b5-7cdd07211fdd",
   "metadata": {},
   "source": [
    "# 10. SIU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c2d15ef-dd00-4fe1-9bf5-396509575de3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# data have to be recovered from hugging face\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open(\"../data/raw/SIU/final_dic.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "    \n",
    "for lig in data[\"O88703\"]:\n",
    "    lig[\"source_data\"]\n",
    "    lig[\"smi\"]\n",
    "    lig[\"label\"]\n",
    "    \n",
    "!ls ../data/raw/SIU/result_structures/O88703\n",
    "\n",
    "PDB_1q3e_PCG_A_401  PDB_3bpz_CMP_C_641\tPDB_5khi_6SX_A_701\n",
    "PDB_3bpz_CMP_A_641  PDB_5khg_CC7_A_701\tPDB_5khj_6SY_A_701\n",
    "PDB_3bpz_CMP_B_641  PDB_5khh_6SW_A_701\tPDB_5khk_6SZ_A_701"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7066f1-1266-4f6e-91a7-5563e613c774",
   "metadata": {},
   "source": [
    "# B. Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec5ff4-dc15-4bbf-935f-f71c84277ca7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_dir = \"../data/curated/exp/\"\n",
    "df_list = []\n",
    "\n",
    "for fname in os.listdir(data_dir):\n",
    "    full_path = os.path.join(data_dir, fname)\n",
    "    print(full_path)\n",
    "    df = pd.read_parquet(full_path)\n",
    "    df[\"source_file\"] = fname\n",
    "    df[\"is_experimental\"] = any(x in fname for x in [\"BioLip\", \"PDBbind\", \"HiQBind\"])\n",
    "    df_list.append(df)\n",
    "\n",
    "df_combined = pd.concat(df_list, ignore_index=True)\n",
    "df_combined[\"source_file\"] = df_combined[\"source_file\"].str.replace(r\"^[^_]*_\", \"\", regex=True).str.replace(\".parquet\", \"\", regex=False)\n",
    "\n",
    "df_combined[\"resolution\"] = pd.to_numeric(df_combined[\"resolution\"], errors=\"coerce\")\n",
    "print(len(df_combined))\n",
    "df_combined.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "print(len(df_combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64d057-722b-417d-8b00-b7c0efc44632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count plot for source_file with counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.countplot(\n",
    "    data=df_combined,\n",
    "    y=\"source_file\",\n",
    "    order=df_combined[\"source_file\"].value_counts().index\n",
    ")\n",
    "plt.title(\"Entry count per source file\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Source File\")\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add count labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type=\"edge\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count plot for is_experimental with counts\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.countplot(data=df_combined, x=\"is_experimental\")\n",
    "plt.title(\"Experimental vs Non-experimental\")\n",
    "plt.xlabel(\"Is Experimental\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add count labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type=\"edge\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca0beb4-ad83-48dc-aa7b-d7d79b52ddf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e75d4-dbc7-463a-8658-24573d378d01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exp_cols = ['resolution', 'pKi', 'pEC50',\n",
    " 'pKd (Wang, FEP)',\n",
    " 'pKd',\n",
    " 'pIC50', \"potency\"]\n",
    "\n",
    "for col in exp_cols:\n",
    "    if col in df_combined.columns:\n",
    "        data = df_combined[col]\n",
    "        data = data[np.isfinite(data)]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        stats = {\n",
    "            'N': len(data),\n",
    "            'min': data.min(),\n",
    "            'mean': data.mean(),\n",
    "            'median': data.median(),\n",
    "            'max': data.max()\n",
    "        }\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(data, kde=True, bins=50)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        for label in ['min', 'mean', 'median', 'max']:\n",
    "            plt.axvline(stats[label], linestyle='--', label=f\"{label}: {stats[label]:.2f}\")\n",
    "\n",
    "        plt.legend(title=f\"N = {stats['N']}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de840136-a9d7-4023-a366-960458c4176a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exp_cols = [ 'pKi', 'pEC50',\n",
    " 'pKd (Wang, FEP)',\n",
    " 'pKd',\n",
    " 'pIC50', \"potency\"]  # exclude 'resolution'\n",
    "\n",
    "for col in exp_cols:\n",
    "    if col not in df_combined.columns:\n",
    "        continue\n",
    "\n",
    "    df_plot = df_combined[[col, 'is_experimental']].copy()\n",
    "    df_plot = df_plot[np.isfinite(df_plot[col])]\n",
    "\n",
    "    if df_plot.empty:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(\n",
    "        data=df_plot,\n",
    "        x=col,\n",
    "        hue='is_experimental',\n",
    "        kde=True,\n",
    "        bins=50,\n",
    "        palette={True: \"tab:blue\", False: \"tab:orange\"},\n",
    "        element=\"step\",\n",
    "        stat=\"density\",  # <- normalizes histograms\n",
    "        common_norm=False\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{col} - Normalized Distribution (Experimental vs Non-Experimental)\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Experimental\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7c3d83-1d35-4d3c-8421-459df09ee1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined = df_combined[df_combined[\"pKi\"].isna() | (df_combined[\"pKi\"] > 3)]\n",
    "print(len(df_combined))\n",
    "df_combined = df_combined[df_combined[\"pEC50\"].isna() | (df_combined[\"pEC50\"] > 3)]\n",
    "print(len(df_combined))\n",
    "df_combined = df_combined[df_combined[\"pKd (Wang, FEP)\"].isna() | (df_combined[\"pKd (Wang, FEP)\"] > 3)]\n",
    "print(len(df_combined))\n",
    "df_combined = df_combined[df_combined[\"pKd\"].isna() | (df_combined[\"pKd\"] > 3)]\n",
    "print(len(df_combined))\n",
    "df_combined = df_combined[df_combined[\"pIC50\"].isna() | (df_combined[\"pIC50\"] > 3)]\n",
    "print(len(df_combined))\n",
    "df_combined = df_combined[df_combined[\"resolution\"].isna() | (df_combined[\"resolution\"] > 0)]\n",
    "print(len(df_combined))\n",
    "df_combined = df_combined[df_combined[\"resolution\"].isna() | (df_combined[\"resolution\"] < 3)]\n",
    "print(len(df_combined))\n",
    "\n",
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aeb27f-19ea-4757-9371-172b534e2be3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count plot for source_file with counts\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = sns.countplot(\n",
    "    data=df_combined,\n",
    "    y=\"source_file\",\n",
    "    order=df_combined[\"source_file\"].value_counts().index\n",
    ")\n",
    "plt.title(\"Entry count per source file\")\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Source File\")\n",
    "plt.xscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add count labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type=\"edge\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count plot for is_experimental with counts\n",
    "plt.figure(figsize=(5, 5))\n",
    "ax = sns.countplot(data=df_combined, x=\"is_experimental\")\n",
    "plt.title(\"Experimental vs Non-experimental\")\n",
    "plt.xlabel(\"Is Experimental\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "\n",
    "# Add count labels\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, label_type=\"edge\", padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7223f2-c9b3-4db6-b07a-37361094041b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exp_cols = ['resolution',  'pKi', 'pEC50',\n",
    " 'pKd (Wang, FEP)',\n",
    " 'pKd',\n",
    " 'pIC50', \"potency\"]\n",
    "\n",
    "for col in exp_cols:\n",
    "    if col in df_combined.columns:\n",
    "        data = df_combined[col]\n",
    "        data = data[np.isfinite(data)]\n",
    "\n",
    "        if len(data) == 0:\n",
    "            continue\n",
    "\n",
    "        stats = {\n",
    "            'N': len(data),\n",
    "            'min': data.min(),\n",
    "            'mean': data.mean(),\n",
    "            'median': data.median(),\n",
    "            'max': data.max()\n",
    "        }\n",
    "\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(data, kde=True, bins=50)\n",
    "        plt.title(f\"Distribution of {col}\")\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        for label in ['min', 'mean', 'median', 'max']:\n",
    "            plt.axvline(stats[label], linestyle='--', label=f\"{label}: {stats[label]:.2f}\")\n",
    "\n",
    "        plt.legend(title=f\"N = {stats['N']}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9dbd72-25b7-454e-933f-c036b05e9fa9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "exp_cols = ['pKi' 'pKi', 'pEC50',\n",
    " 'pKd (Wang, FEP)',\n",
    " 'pKd',\n",
    " 'pIC50', \"potency\"]  # exclude 'resolution'\n",
    "\n",
    "for col in exp_cols:\n",
    "    if col not in df_combined.columns:\n",
    "        continue\n",
    "\n",
    "    df_plot = df_combined[[col, 'is_experimental']].copy()\n",
    "    df_plot = df_plot[np.isfinite(df_plot[col])]\n",
    "\n",
    "    if df_plot.empty:\n",
    "        continue\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.histplot(\n",
    "        data=df_plot,\n",
    "        x=col,\n",
    "        hue='is_experimental',\n",
    "        kde=True,\n",
    "        bins=50,\n",
    "        palette={True: \"tab:blue\", False: \"tab:orange\"},\n",
    "        element=\"step\",\n",
    "        stat=\"density\",  # <- normalizes histograms\n",
    "        common_norm=False\n",
    "    )\n",
    "\n",
    "    plt.title(f\"{col} - Normalized Distribution (Experimental vs Non-Experimental)\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(True)\n",
    "    plt.legend(title=\"Experimental\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b502d2-14a0-49bf-91ff-76c4ef6eab13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined.to_parquet(\"../data/curated/combined/df_combined.parquet\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa3dde-dcd8-44ab-84b9-c422e2fda3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_combined = pd.read_parquet(\"../data/curated/combined/df_combined.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a81618d-6a1f-4b5a-b8ff-601207ded76f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6010b821-b44b-48b6-869b-339ba9581a8b",
   "metadata": {},
   "source": [
    "standardize smiles then group by to remove duplicates\n",
    "remove too small compounds\n",
    "add the cell or biochemical origin \n",
    "the Ligand Efficiency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b31719d9-b1a0-41c4-b293-831481b0a6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d260b7ae-19e9-4f85-ae94-37733fb9c62d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
