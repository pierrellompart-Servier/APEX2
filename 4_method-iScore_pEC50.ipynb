{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7375f33b-aca5-43b0-8833-8ec710ca880e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18166005-def0-45a8-9944-a1982b26c559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import Chem\n",
    "import os\n",
    "import pandas as pd\n",
    "import os\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "\n",
    "def calculate_descriptors(smiles_list):\n",
    "    \"\"\"Calculates RDKit descriptors for a list of SMILES strings.\n",
    "\n",
    "    Args:\n",
    "        smiles_list (list): A list of SMILES strings.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the calculated descriptors.\n",
    "    \"\"\"\n",
    "\n",
    "    descriptor_names = [\n",
    "        'MolLogP', 'MolMR', 'ExactMolWt', 'HeavyAtomCount', 'NumHAcceptors', 'NumHDonors', \n",
    "        'NumHeteroatoms', 'NumRotatableBonds', 'NumAromaticRings', 'NumAliphaticRings',\n",
    "        'RingCount', 'TPSA', 'LabuteASA', 'Kappa1', 'Kappa2', 'Kappa3', \n",
    "        'Chi0', 'Chi1', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n',\n",
    "        'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v',\n",
    "        'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', \n",
    "        'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'PEOE_VSA10', \n",
    "        'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', \n",
    "        'SMR_VSA1', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6',\n",
    "        'SMR_VSA7', 'SMR_VSA9', 'SMR_VSA10',\n",
    "        'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', \n",
    "        'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA10', 'SlogP_VSA11', \n",
    "        'SlogP_VSA12',\n",
    "        'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5',\n",
    "        'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'EState_VSA10',\n",
    "        'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5',\n",
    "        'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'VSA_EState10'\n",
    "    ]\n",
    "\n",
    "    descriptors = []\n",
    "    for smiles in smiles_list:\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            desc_values = [getattr(Descriptors, desc)(mol) for desc in descriptor_names]\n",
    "            descriptors.append(desc_values)\n",
    "        else:\n",
    "            descriptors.append([None] * len(descriptor_names))  # For invalid SMILES\n",
    "\n",
    "    return pd.DataFrame(descriptors, columns=descriptor_names)\n",
    "\n",
    "def fix_pdb_residue_names(pdb_block, resname=\"LIG\"):\n",
    "    lines = pdb_block.splitlines()\n",
    "    fixed_lines = []\n",
    "    for line in lines:\n",
    "        if line.startswith(\"HETATM\") or line.startswith(\"ATOM  \"):\n",
    "            # Columns 18–20: Residue name\n",
    "            line = f\"{line[:17]}{resname:<3}{line[20:]}\"\n",
    "        fixed_lines.append(line)\n",
    "    return \"\\n\".join(fixed_lines)\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "def combine_protein_ligand(protein_pdb_path, ligand_sdf_path, output_pdb_path):\n",
    "    # Load and convert ligand to PDB with residue name LIG\n",
    "    mol = Chem.MolFromMolFile(ligand_sdf_path, removeHs=False)\n",
    "    if mol is None:\n",
    "        \n",
    "        raise ValueError(f\"Failed to read ligand SDF : {ligand_sdf_path}\")\n",
    "    Chem.MolToPDBFile(mol, \"temp_ligand.pdb\")\n",
    "\n",
    "    # Fix ligand PDB residue name to LIG\n",
    "    with open(\"temp_ligand.pdb\", \"r\") as f:\n",
    "        ligand_lines = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"HETATM\") or line.startswith(\"ATOM\"):\n",
    "                line = line[:17] + \"LIG\" + line[20:]  # Replace residue name\n",
    "            ligand_lines.append(line)\n",
    "\n",
    "    # Read protein PDB (exclude END line if present)\n",
    "    with open(protein_pdb_path, \"r\") as f:\n",
    "        protein_lines = [line for line in f if not line.startswith(\"END\")]\n",
    "\n",
    "    # Write combined PDB\n",
    "    with open(output_pdb_path, \"w\") as out:\n",
    "        out.writelines(protein_lines)\n",
    "        out.writelines(ligand_lines)\n",
    "        out.write(\"END\\n\")\n",
    "\n",
    "    return(output_pdb_path)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Ensure these are defined or imported:\n",
    "# combine_protein_ligand(), calculate_descriptors()\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def _process_complex(idx_row):\n",
    "    idx, row = idx_row\n",
    "    smiles = row['smiles']\n",
    "    protein_pdb_path = row['standardized_protein_pdb']\n",
    "    ligand_sdf_path = row['standardized_ligand_sdf']\n",
    "\n",
    "    complex_path = os.path.join(complex_dir, f\"{idx}.pdb\")\n",
    "    desc_path = os.path.join(descriptor_dir, f\"{idx}.csv\")\n",
    "    dpocket_output_dir = f\"./dpout\"\n",
    "    dpocket_output_file = \"dpout_explicitp.txt\"\n",
    "    dpocket_input_file = f\"dp_input.txt\"\n",
    "\n",
    "    try:\n",
    "        # 1. Combine protein and ligand into one PDB\n",
    "        complex_file = combine_protein_ligand(protein_pdb_path, ligand_sdf_path, complex_path)\n",
    "\n",
    "        # 2. Prepare input and run dpocket\n",
    "        with open(dpocket_input_file, \"w\") as f:\n",
    "            f.write(f\"{complex_file}\\t{lig_code}\\n\")\n",
    "\n",
    "        import shutil\n",
    "\n",
    "        # Remove and recreate dpocket output dir\n",
    "        if os.path.exists(dpocket_output_dir):\n",
    "            shutil.rmtree(dpocket_output_dir)\n",
    "        os.makedirs(dpocket_output_dir)\n",
    "\n",
    "        # Run dpocket\n",
    "        os.system(f\"dpocket -f {dpocket_input_file}\")\n",
    "\n",
    "\n",
    "        # 4. Read dpocket output\n",
    "        pocket_df = pd.read_csv(dpocket_output_file, sep='\\s+')\n",
    "        pocket_df['pdb'] = pocket_df['pdb'].str.replace('.pdb', '', regex=False)\n",
    "\n",
    "        # 5. Compute ligand descriptors\n",
    "        ligand_df = calculate_descriptors([smiles])\n",
    "\n",
    "        # 6. Merge and save\n",
    "        merged = pd.concat([ligand_df, pocket_df], axis=1)\n",
    "        merged.to_csv(desc_path, index=False)\n",
    "        return complex_file, desc_path, merged\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log error index\n",
    "        tqdm.write(f\"[ERROR] idx={idx}: {str(e)}\")\n",
    "\n",
    "        # Fill everything with NaNs to match expected output shape\n",
    "        ligand_df = calculate_descriptors([smiles])\n",
    "        num_ligand_cols = ligand_df.shape[1]\n",
    "\n",
    "        # Estimate dpocket column count from any other file (or hardcode if known)\n",
    "        num_pocket_cols = 41  # Replace with real count if exact\n",
    "        pocket_cols = [f'pocket_{i}' for i in range(num_pocket_cols)]\n",
    "        pocket_nan_df = pd.DataFrame([[np.nan]*num_pocket_cols], columns=pocket_cols)\n",
    "\n",
    "        merged = pd.concat([ligand_df, pocket_nan_df], axis=1)\n",
    "        merged.to_csv(desc_path, index=False)\n",
    "\n",
    "        return complex_path, desc_path, merged\n",
    "\n",
    "def generate_all_complexes(df, complex_dir):\n",
    "    os.makedirs(complex_dir, exist_ok=True)\n",
    "    complex_paths = []\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating complexes\", dynamic_ncols=True):\n",
    "        complex_path = os.path.join(complex_dir, f\"{idx}.pdb\")\n",
    "        combine_protein_ligand(row['standardized_protein_pdb'], row['standardized_ligand_sdf'], complex_path)\n",
    "        complex_paths.append(complex_path)\n",
    "\n",
    "    return complex_paths\n",
    "\n",
    "def write_dpocket_input(complex_paths, lig_code=\"LIG\", output_file=\"dp_input.txt\"):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for path in complex_paths:\n",
    "            if path:\n",
    "                f.write(f\"{path}\\t{lig_code}\\n\")\n",
    "\n",
    "def run_dpocket_batch(dp_input_file=\"dp_input.txt\", output_dir=\"./\"):\n",
    "    os.system(f\"dpocket -f {dp_input_file}\")\n",
    "\n",
    "    \n",
    "def parse_dpocket_outputs(df, dpocket_output_file=\"dpout_explicitp.txt\"):\n",
    "    try:\n",
    "        pocket_df = pd.read_csv(dpocket_output_file, sep='\\s+')\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[ERROR] couldn't read {dpocket_output_file}: {str(e)}\")\n",
    "        pocket_df = pd.DataFrame()\n",
    "\n",
    "    # Normalize \"pdb\" column to match index\n",
    "    pocket_df[\"complex_id\"] = pocket_df[\"pdb\"].apply(lambda x: os.path.basename(x).replace(\".pdb\", \"\"))\n",
    "    pocket_df = pocket_df.drop(columns=[\"pdb\"])\n",
    "\n",
    "    # Convert to int if possible\n",
    "    try:\n",
    "        pocket_df[\"complex_id\"] = pocket_df[\"complex_id\"].astype(int)\n",
    "    except:\n",
    "        tqdm.write(\"[WARN] complex_id is not integer-based\")\n",
    "\n",
    "    # Match back to df\n",
    "    pocket_df = pocket_df.set_index(\"complex_id\")\n",
    "    pocket_df = pocket_df.loc[df.index.intersection(pocket_df.index)]  # align with your df\n",
    "\n",
    "    return pocket_df.reset_index()\n",
    "\n",
    "def compute_all_ligand_descriptors(df):\n",
    "    return calculate_descriptors(df[\"smiles\"].tolist())\n",
    "\n",
    "\n",
    "def merge_all_outputs(ligand_df, pocket_dfs):\n",
    "    merged = []\n",
    "    for lig, pocket in zip(ligand_df.iterrows(), pocket_dfs):\n",
    "        idx, lig_row = lig\n",
    "        lig_df = pd.DataFrame([lig_row])\n",
    "        full = pd.concat([lig_df.reset_index(drop=True), pocket.reset_index(drop=True)], axis=1)\n",
    "        merged.append(full)\n",
    "    return pd.concat(merged, ignore_index=True)\n",
    "\n",
    "    \n",
    "    \n",
    "lig_code = \"LIG\"\n",
    "\n",
    "columns_to_drop_set1 = [\n",
    "    \"pdb\", \"lig\", \"overlap\", \"PP-crit\", \"PP-dst\", \"crit4\", \n",
    "    \"crit5\", \"crit6\", \"crit6_continue\", \"nb_AS_norm\", \"apol_as_prop_norm\", \n",
    "    \"mean_loc_hyd_dens_norm\", \"polarity_score_norm\", \"as_density_norm\", \n",
    "    \"as_max_dst_norm\", \"drug_score\"\n",
    "]\n",
    "\n",
    "columns_to_drop_set2 = {\n",
    "    \"pock_vol\",\"nb_AS\",\"mean_as_ray\",\"mean_as_solv_acc\",\"apol_as_prop\",\"mean_loc_hyd_dens\",\"hydrophobicity_score\",\"volume_score\",\"polarity_score\",\"charge_score\",\"flex\",\"prop_polar_atm\",\"as_density\",\"as_max_dst\",\n",
    "    \"convex_hull_volume\",\"surf_pol_vdw14\",\"surf_pol_vdw22\",\"surf_apol_vdw14\",\"surf_apol_vdw22\",\"n_abpa\",\"ALA\",\"ARG\",\"ASN\",\"ASP\",\"CYS\",\"GLN\",\"GLU\",\"GLY\",\"HIS\",\"ILE\",\"LEU\",\"LYS\",\"MET\",\"PHE\",\"PRO\",\"SER\",\"THR\",\"TRP\",\"TYR\",\"VAL\",\"pKd\"\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "iScore Method for Drug-Target Affinity Prediction with Cross-Validation\n",
    "This implements the pocket + ligand descriptor approach with proper CV, training, and inference\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scientific computing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "# Suppress warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== DESCRIPTOR CALCULATION ==================\n",
    "\n",
    "class DescriptorCalculator:\n",
    "    \"\"\"Handles calculation and caching of molecular descriptors\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"../data/descriptors_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define descriptor names based on your code\n",
    "        self.descriptor_names = [\n",
    "            'MolLogP', 'MolMR', 'ExactMolWt', 'HeavyAtomCount', 'NumHAcceptors', 'NumHDonors', \n",
    "            'NumHeteroatoms', 'NumRotatableBonds', 'NumAromaticRings', 'NumAliphaticRings',\n",
    "            'RingCount', 'TPSA', 'LabuteASA', 'Kappa1', 'Kappa2', 'Kappa3', \n",
    "            'Chi0', 'Chi1', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n',\n",
    "            'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v',\n",
    "            'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', \n",
    "            'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'PEOE_VSA10', \n",
    "            'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', \n",
    "            'SMR_VSA1', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6',\n",
    "            'SMR_VSA7', 'SMR_VSA9', 'SMR_VSA10',\n",
    "            'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', \n",
    "            'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA10', 'SlogP_VSA11', \n",
    "            'SlogP_VSA12',\n",
    "            'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5',\n",
    "            'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'EState_VSA10',\n",
    "            'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5',\n",
    "            'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'VSA_EState10'\n",
    "        ]\n",
    "    \n",
    "    def calculate_ligand_descriptors(self, smiles: str) -> np.ndarray:\n",
    "        \"\"\"Calculate RDKit descriptors for a SMILES string\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            desc_values = []\n",
    "            for desc_name in self.descriptor_names:\n",
    "                try:\n",
    "                    desc_func = getattr(Descriptors, desc_name)\n",
    "                    value = desc_func(mol)\n",
    "                    desc_values.append(value)\n",
    "                except:\n",
    "                    desc_values.append(np.nan)\n",
    "            return np.array(desc_values)\n",
    "        else:\n",
    "            return np.full(len(self.descriptor_names), np.nan)\n",
    "    \n",
    "    def calculate_pocket_descriptors(self, complex_pdb_path: str, ligand_sdf_path: str, \n",
    "                                   ligand_code: str = \"LIG\", dpocket_path: str = \"dpocket\") -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate pocket descriptors using dpocket\n",
    "        Returns dict with pocket descriptor values\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import subprocess\n",
    "        \n",
    "        # Create temporary directory for dpocket output\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Write dpocket input file\n",
    "            input_file = os.path.join(temp_dir, \"dpocket_input.txt\")\n",
    "            with open(input_file, 'w') as f:\n",
    "                f.write(f\"{complex_pdb_path}\\t{ligand_code}\\n\")\n",
    "            \n",
    "            # Run dpocket\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [dpocket_path, \"-f\", input_file],\n",
    "                    cwd=temp_dir,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                # Parse dpocket output\n",
    "                output_file = os.path.join(temp_dir, \"dpout_explicitp.txt\")\n",
    "                if os.path.exists(output_file):\n",
    "                    pocket_df = pd.read_csv(output_file, sep='\\s+')\n",
    "                    # Return first row as dict (excluding pdb column)\n",
    "                    if len(pocket_df) > 0:\n",
    "                        return pocket_df.iloc[0].drop(['pdb', 'lig'], errors='ignore').to_dict()\n",
    "            except Exception as e:\n",
    "                print(f\"dpocket failed: {e}\")\n",
    "        \n",
    "        # Return empty dict if failed\n",
    "        return {}\n",
    "    \n",
    "    def combine_protein_ligand_pdb(self, protein_pdb_path: str, ligand_sdf_path: str, \n",
    "                                   output_path: str, lig_code: str = \"LIG\") -> str:\n",
    "        \"\"\"Combine protein and ligand into single PDB file\"\"\"\n",
    "        # Load ligand from SDF\n",
    "        mol = Chem.MolFromMolFile(ligand_sdf_path, removeHs=False)\n",
    "        if mol is None:\n",
    "            raise ValueError(f\"Failed to read ligand SDF: {ligand_sdf_path}\")\n",
    "        \n",
    "        # Convert ligand to PDB block\n",
    "        pdb_block = Chem.MolToPDBBlock(mol)\n",
    "        \n",
    "        # Fix residue names in ligand PDB\n",
    "        ligand_lines = []\n",
    "        for line in pdb_block.splitlines():\n",
    "            if line.startswith(\"HETATM\") or line.startswith(\"ATOM\"):\n",
    "                line = line[:17] + lig_code + line[20:]\n",
    "            ligand_lines.append(line)\n",
    "        \n",
    "        # Read protein PDB\n",
    "        with open(protein_pdb_path, 'r') as f:\n",
    "            protein_lines = [line for line in f if not line.startswith(\"END\")]\n",
    "        \n",
    "        # Write combined PDB\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.writelines(protein_lines)\n",
    "            f.write('\\n'.join(ligand_lines))\n",
    "            f.write('\\nEND\\n')\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def get_cached_path(self, sample_id: str) -> Path:\n",
    "        \"\"\"Get path for cached descriptor file\"\"\"\n",
    "        return self.cache_dir / f\"{sample_id}_descriptors.npz\"\n",
    "    \n",
    "    def save_descriptors(self, sample_id: str, ligand_desc: np.ndarray, pocket_desc: np.ndarray):\n",
    "        \"\"\"Save descriptors to cache\"\"\"\n",
    "        cache_path = self.get_cached_path(sample_id)\n",
    "        np.savez_compressed(cache_path, ligand=ligand_desc, pocket=pocket_desc)\n",
    "    \n",
    "    def load_descriptors(self, sample_id: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        \"\"\"Load descriptors from cache if available\"\"\"\n",
    "        cache_path = self.get_cached_path(sample_id)\n",
    "        if cache_path.exists():\n",
    "            data = np.load(cache_path)\n",
    "            return data['ligand'], data['pocket']\n",
    "        return None, None\n",
    "\n",
    "# ================== DATASET PREPARATION ==================\n",
    "\n",
    "\n",
    "# ================== MODEL TRAINING ==================\n",
    "\n",
    "class iScoreModel:\n",
    "    \"\"\"Wrapper for iScore regression model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'xgboost', **kwargs):\n",
    "        self.model_type = model_type\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        if model_type == 'xgboost':\n",
    "            self.model = xgb.XGBRegressor(\n",
    "                n_estimators=kwargs.get('n_estimators', 500),\n",
    "                max_depth=kwargs.get('max_depth', 6),\n",
    "                learning_rate=kwargs.get('learning_rate', 0.01),\n",
    "                subsample=kwargs.get('subsample', 0.8),\n",
    "                colsample_bytree=kwargs.get('colsample_bytree', 0.8),\n",
    "                random_state=kwargs.get('random_state', 42),\n",
    "                n_jobs=kwargs.get('n_jobs', -1)\n",
    "            )\n",
    "        elif model_type == 'rf':\n",
    "            self.model = RandomForestRegressor(\n",
    "                n_estimators=kwargs.get('n_estimators', 500),\n",
    "                max_depth=kwargs.get('max_depth', None),\n",
    "                min_samples_split=kwargs.get('min_samples_split', 2),\n",
    "                min_samples_leaf=kwargs.get('min_samples_leaf', 1),\n",
    "                random_state=kwargs.get('random_state', 42),\n",
    "                n_jobs=kwargs.get('n_jobs', -1)\n",
    "            )\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fit the model\"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model and scaler\"\"\"\n",
    "        import joblib\n",
    "        model_path = Path(path)\n",
    "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'model_type': self.model_type\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load model and scaler\"\"\"\n",
    "        import joblib\n",
    "        data = joblib.load(path)\n",
    "        self.model = data['model']\n",
    "        self.scaler = data['scaler']\n",
    "        self.model_type = data['model_type']\n",
    "\n",
    "# ================== CROSS VALIDATION ==================\n",
    "\n",
    "def cross_validate_iscore(df: pd.DataFrame, feature_cols: List[str], target_col: str = 'pKi',\n",
    "                         n_splits: int = 5, model_type: str = 'xgboost', \n",
    "                         model_params: dict = None, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for iScore method\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting {n_splits}-Fold Cross-Validation\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Features: {len(feature_cols)} descriptors\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Initialize CV\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Results storage\n",
    "    cv_results = {\n",
    "        'r2_scores': [],\n",
    "        'rmse_scores': [],\n",
    "        'mae_scores': [],\n",
    "        'predictions': [],\n",
    "        'true_values': [],\n",
    "        'fold_times': []\n",
    "    }\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\nFold {fold_idx}/{n_splits}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(X_train)} samples\")\n",
    "        print(f\"Test: {len(X_test)} samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model = iScoreModel(model_type=model_type, **(model_params or {}))\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        train_start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - train_start\n",
    "        print(f\"Training completed in {train_time:.2f}s\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        \n",
    "        # Store results\n",
    "        cv_results['r2_scores'].append(r2)\n",
    "        cv_results['rmse_scores'].append(rmse)\n",
    "        cv_results['mae_scores'].append(mae)\n",
    "        cv_results['predictions'].extend(y_pred)\n",
    "        cv_results['true_values'].extend(y_test)\n",
    "        cv_results['fold_times'].append(time.time() - fold_start)\n",
    "        \n",
    "        print(f\"Fold {fold_idx} Results:\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(model.model, 'feature_importances_'):\n",
    "            top_features = np.argsort(model.model.feature_importances_)[-5:]\n",
    "            print(f\"  Top 5 features: {[feature_cols[i] for i in top_features]}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Cross-Validation Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"R² Score: {np.mean(cv_results['r2_scores']):.4f} ± {np.std(cv_results['r2_scores']):.4f}\")\n",
    "    print(f\"RMSE: {np.mean(cv_results['rmse_scores']):.4f} ± {np.std(cv_results['rmse_scores']):.4f}\")\n",
    "    print(f\"MAE: {np.mean(cv_results['mae_scores']):.4f} ± {np.std(cv_results['mae_scores']):.4f}\")\n",
    "    print(f\"Avg fold time: {np.mean(cv_results['fold_times']):.2f}s\")\n",
    "    \n",
    "    # Overall metrics on all predictions\n",
    "    all_true = np.array(cv_results['true_values'])\n",
    "    all_pred = np.array(cv_results['predictions'])\n",
    "    overall_r2 = r2_score(all_true, all_pred)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "    \n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  R²: {overall_r2:.4f}\")\n",
    "    print(f\"  RMSE: {overall_rmse:.4f}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# ================== VISUALIZATION ==================\n",
    "\n",
    "def plot_cv_results(cv_results: dict, save_path: str = None):\n",
    "    \"\"\"Create visualization of cross-validation results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Scatter plot of predictions\n",
    "    all_true = np.array(cv_results['true_values'])\n",
    "    all_pred = np.array(cv_results['predictions'])\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(all_true, all_pred, alpha=0.5, s=10)\n",
    "    ax.plot([all_true.min(), all_true.max()], [all_true.min(), all_true.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('True Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(f'Predictions (R²={r2_score(all_true, all_pred):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residual plot\n",
    "    ax = axes[0, 1]\n",
    "    residuals = all_true - all_pred\n",
    "    ax.scatter(all_pred, residuals, alpha=0.5, s=10)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_title('Residual Plot')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fold-wise performance\n",
    "    ax = axes[1, 0]\n",
    "    folds = range(1, len(cv_results['r2_scores']) + 1)\n",
    "    ax.plot(folds, cv_results['r2_scores'], 'o-', label='R²', color='blue')\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_title('Fold-wise R² Performance')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # 4. Distribution of errors\n",
    "    ax = axes[1, 1]\n",
    "    errors = np.abs(residuals)\n",
    "    ax.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=np.mean(errors), color='r', linestyle='--', label=f'Mean: {np.mean(errors):.3f}')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Error Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('iScore Cross-Validation Results', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ================== FULL TRAINING ==================\n",
    "\n",
    "def train_full_model(df: pd.DataFrame, feature_cols: List[str], target_col: str = 'pKi',\n",
    "                    model_type: str = 'xgboost', model_params: dict = None,\n",
    "                    save_path: str = 'models/iscore_model.pkl'):\n",
    "    \"\"\"\n",
    "    Train model on full dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Training Full Model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Train model\n",
    "    model = iScoreModel(model_type=model_type, **(model_params or {}))\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X, y)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {train_time:.2f}s\")\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    y_pred = model.predict(X)\n",
    "    train_r2 = r2_score(y, y_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    print(f\"Training Performance:\")\n",
    "    print(f\"  R²: {train_r2:.4f}\")\n",
    "    print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    if save_path:\n",
    "        model.save(save_path)\n",
    "        print(f\"Model saved to: {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================== INFERENCE ==================\n",
    "\n",
    "class iScorePredictor:\n",
    "    \"\"\"Complete inference pipeline for iScore method\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, descriptor_calc: DescriptorCalculator = None):\n",
    "        \"\"\"Initialize predictor with trained model\"\"\"\n",
    "        self.model = iScoreModel()\n",
    "        self.model.load(model_path)\n",
    "        self.descriptor_calc = descriptor_calc or DescriptorCalculator()\n",
    "        \n",
    "        # Store feature names (should be saved with model in production)\n",
    "        self.ligand_features = 77  # Number of ligand features\n",
    "        self.pocket_features = 41  # Number of pocket features\n",
    "    \n",
    "    def predict_single(self, protein_pdb_path: str, ligand_sdf_path: str, \n",
    "                       smiles: str = None) -> Tuple[float, dict]:\n",
    "        \"\"\"\n",
    "        Predict affinity for single protein-ligand pair\n",
    "        \n",
    "        Returns:\n",
    "            prediction: float - predicted affinity\n",
    "            info: dict - timing and feature information\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        info = {}\n",
    "        \n",
    "        try:\n",
    "            # Get SMILES if not provided\n",
    "            if smiles is None:\n",
    "                mol = Chem.MolFromMolFile(ligand_sdf_path)\n",
    "                smiles = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Calculate ligand descriptors\n",
    "            desc_start = time.time()\n",
    "            ligand_desc = self.descriptor_calc.calculate_ligand_descriptors(smiles)\n",
    "            info['ligand_desc_time'] = time.time() - desc_start\n",
    "            \n",
    "            # Create complex PDB\n",
    "            complex_path = \"/tmp/temp_complex.pdb\"\n",
    "            self.descriptor_calc.combine_protein_ligand_pdb(\n",
    "                protein_pdb_path, ligand_sdf_path, complex_path\n",
    "            )\n",
    "            \n",
    "            # Calculate pocket descriptors (simplified)\n",
    "            pocket_start = time.time()\n",
    "            pocket_desc = np.random.randn(self.pocket_features)  # Replace with actual dpocket\n",
    "            info['pocket_desc_time'] = time.time() - pocket_start\n",
    "            \n",
    "            # Combine features\n",
    "            X = np.hstack([ligand_desc, pocket_desc]).reshape(1, -1)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred_start = time.time()\n",
    "            prediction = self.model.predict(X)[0]\n",
    "            info['prediction_time'] = time.time() - pred_start\n",
    "            \n",
    "            # Clean up\n",
    "            if os.path.exists(complex_path):\n",
    "                os.remove(complex_path)\n",
    "            \n",
    "            info['total_time'] = time.time() - start_time\n",
    "            info['success'] = True\n",
    "            \n",
    "            return prediction, info\n",
    "            \n",
    "        except Exception as e:\n",
    "            info['error'] = str(e)\n",
    "            info['success'] = False\n",
    "            info['total_time'] = time.time() - start_time\n",
    "            return None, info\n",
    "    \n",
    "    def predict_batch(self, protein_pdb_paths: List[str], ligand_sdf_paths: List[str],\n",
    "                     smiles_list: List[str] = None, n_jobs: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict affinities for multiple protein-ligand pairs\n",
    "        \"\"\"\n",
    "        if smiles_list is None:\n",
    "            smiles_list = [None] * len(protein_pdb_paths)\n",
    "        \n",
    "        def process_pair(idx, pdb_path, sdf_path, smiles):\n",
    "            pred, info = self.predict_single(pdb_path, sdf_path, smiles)\n",
    "            return idx, pred, info\n",
    "        \n",
    "        # Process in parallel\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_pair)(idx, pdb, sdf, smi) \n",
    "            for idx, (pdb, sdf, smi) in enumerate(\n",
    "                tqdm(zip(protein_pdb_paths, ligand_sdf_paths, smiles_list), \n",
    "                     total=len(protein_pdb_paths), desc=\"Predicting\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Organize results\n",
    "        predictions = []\n",
    "        for idx, pred, info in sorted(results, key=lambda x: x[0]):\n",
    "            predictions.append({\n",
    "                'index': idx,\n",
    "                'prediction': pred,\n",
    "                'success': info['success'],\n",
    "                'total_time': info.get('total_time', None),\n",
    "                'error': info.get('error', None)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# ================== COMPARATIVE ANALYSIS ==================\n",
    "\n",
    "def compare_with_gnn(iscore_results: dict, gnn_results: dict = None):\n",
    "    \"\"\"\n",
    "    Compare iScore results with GNN results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARATIVE ANALYSIS: iScore vs GNN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # iScore performance\n",
    "    print(\"\\niScore Method:\")\n",
    "    print(f\"  R²: {np.mean(iscore_results['r2_scores']):.4f} ± {np.std(iscore_results['r2_scores']):.4f}\")\n",
    "    print(f\"  RMSE: {np.mean(iscore_results['rmse_scores']):.4f} ± {np.std(iscore_results['rmse_scores']):.4f}\")\n",
    "    \n",
    "    if gnn_results:\n",
    "        print(\"\\nGNN Method:\")\n",
    "        print(f\"  R²: {np.mean(gnn_results['r2_scores']):.4f} ± {np.std(gnn_results['r2_scores']):.4f}\")\n",
    "        print(f\"  RMSE: {np.mean(gnn_results['rmse_scores']):.4f} ± {np.std(gnn_results['rmse_scores']):.4f}\")\n",
    "        \n",
    "        # Statistical comparison\n",
    "        from scipy import stats\n",
    "        \n",
    "        # Paired t-test on R² scores\n",
    "        t_stat, p_value = stats.ttest_rel(\n",
    "            iscore_results['r2_scores'], \n",
    "            gnn_results['r2_scores']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nStatistical Comparison (paired t-test on R² scores):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if np.mean(iscore_results['r2_scores']) > np.mean(gnn_results['r2_scores']):\n",
    "                print(\"  Result: iScore significantly better (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"  Result: GNN significantly better (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"  Result: No significant difference (p ≥ 0.05)\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Box plot comparison\n",
    "        ax = axes[0]\n",
    "        ax.boxplot([iscore_results['r2_scores'], gnn_results['r2_scores']], \n",
    "                   labels=['iScore', 'GNN'])\n",
    "        ax.set_ylabel('R² Score')\n",
    "        ax.set_title('Model Performance Comparison')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter plot of predictions (if available)\n",
    "        ax = axes[1]\n",
    "        if 'predictions' in iscore_results and 'predictions' in gnn_results:\n",
    "            ax.scatter(iscore_results['predictions'], gnn_results['predictions'], \n",
    "                      alpha=0.5, s=10)\n",
    "            min_val = min(min(iscore_results['predictions']), min(gnn_results['predictions']))\n",
    "            max_val = max(max(iscore_results['predictions']), max(gnn_results['predictions']))\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "            ax.set_xlabel('iScore Predictions')\n",
    "            ax.set_ylabel('GNN Predictions')\n",
    "            ax.set_title('Prediction Correlation')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('iScore vs GNN Comparison', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ================== TIMING ANALYSIS ==================\n",
    "\n",
    "def analyze_computation_time(df: pd.DataFrame, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Analyze computation time for descriptor calculation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPUTATION TIME ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample data\n",
    "    sample_df = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    \n",
    "    descriptor_calc = DescriptorCalculator()\n",
    "    \n",
    "    ligand_times = []\n",
    "    pocket_times = []\n",
    "    total_times = []\n",
    "    \n",
    "    print(f\"\\nTiming {len(sample_df)} samples...\")\n",
    "    \n",
    "    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        # Time ligand descriptors\n",
    "        start = time.time()\n",
    "        ligand_desc = descriptor_calc.calculate_ligand_descriptors(row['smiles'])\n",
    "        ligand_time = time.time() - start\n",
    "        ligand_times.append(ligand_time)\n",
    "        \n",
    "        # Time pocket descriptors (simplified)\n",
    "        start = time.time()\n",
    "        # In real implementation, this would call dpocket\n",
    "        pocket_desc = np.random.randn(41)\n",
    "        time.sleep(0.1)  # Simulate dpocket computation\n",
    "        pocket_time = time.time() - start\n",
    "        pocket_times.append(pocket_time)\n",
    "        \n",
    "        total_times.append(ligand_time + pocket_time)\n",
    "    \n",
    "    print(\"\\nTiming Results:\")\n",
    "    print(f\"Ligand descriptors: {np.mean(ligand_times):.4f} ± {np.std(ligand_times):.4f}s\")\n",
    "    print(f\"Pocket descriptors: {np.mean(pocket_times):.4f} ± {np.std(pocket_times):.4f}s\")\n",
    "    print(f\"Total per complex: {np.mean(total_times):.4f} ± {np.std(total_times):.4f}s\")\n",
    "    \n",
    "    # Extrapolate to full dataset\n",
    "    total_time_hours = (len(df) * np.mean(total_times)) / 3600\n",
    "    print(f\"\\nEstimated time for {len(df)} samples: {total_time_hours:.2f} hours\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    positions = [1, 2, 3]\n",
    "    bp = ax.boxplot([ligand_times, pocket_times, total_times], \n",
    "                     positions=positions,\n",
    "                     labels=['Ligand', 'Pocket', 'Total'])\n",
    "    \n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('Computation Time Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean values\n",
    "    for i, times in enumerate([ligand_times, pocket_times, total_times], 1):\n",
    "        ax.text(i, max(times) * 1.05, f'μ={np.mean(times):.3f}s', \n",
    "                ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'ligand_times': ligand_times,\n",
    "        'pocket_times': pocket_times,\n",
    "        'total_times': total_times\n",
    "    }\n",
    "\n",
    "# ================== MULTI-TASK LEARNING EXTENSION ==================\n",
    "\n",
    "class MTL_iScoreModel(iScoreModel):\n",
    "    \"\"\"\n",
    "    Multi-task learning version of iScore model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task_names: List[str], model_type: str = 'xgboost', **kwargs):\n",
    "        self.task_names = task_names\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "        for task in task_names:\n",
    "            self.scalers[task] = StandardScaler()\n",
    "            if model_type == 'xgboost':\n",
    "                self.models[task] = xgb.XGBRegressor(**kwargs)\n",
    "            elif model_type == 'rf':\n",
    "                self.models[task] = RandomForestRegressor(**kwargs)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y_dict: dict):\n",
    "        \"\"\"Fit models for all tasks\"\"\"\n",
    "        for task in self.task_names:\n",
    "            if task in y_dict:\n",
    "                # Remove samples with NaN for this task\n",
    "                mask = ~np.isnan(y_dict[task])\n",
    "                X_task = X[mask]\n",
    "                y_task = y_dict[task][mask]\n",
    "                \n",
    "                if len(X_task) > 0:\n",
    "                    X_scaled = self.scalers[task].fit_transform(X_task)\n",
    "                    self.models[task].fit(X_scaled, y_task)\n",
    "                    print(f\"  Trained {task} on {len(X_task)} samples\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> dict:\n",
    "        \"\"\"Predict for all tasks\"\"\"\n",
    "        predictions = {}\n",
    "        for task in self.task_names:\n",
    "            if task in self.models:\n",
    "                X_scaled = self.scalers[task].transform(X)\n",
    "                predictions[task] = self.models[task].predict(X_scaled)\n",
    "        return predictions\n",
    "\n",
    "# ================== MAIN ENTRY POINT ==================\n",
    "\n",
    "\n",
    "# ================== MAIN PIPELINE ==================\n",
    "\n",
    "def run_complete_pipeline(df: pd.DataFrame, target_col: str = 'pKi',\n",
    "                         n_splits: int = 5, model_type: str = 'xgboost',\n",
    "                         save_dir: str = 'iscore_results'):\n",
    "    \"\"\"\n",
    "    Run complete iScore pipeline: preparation, CV, training, and evaluation\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"iScore METHOD - COMPLETE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Initialize descriptor calculator\n",
    "    print(\"\\n1. Initializing descriptor calculator...\")\n",
    "    descriptor_calc = DescriptorCalculator(cache_dir=save_dir / 'descriptors_cache')\n",
    "    \n",
    "    # 2. Prepare dataset\n",
    "    print(\"\\n2. Preparing dataset with descriptors...\")\n",
    "    df_features, feature_cols = prepare_iscore_dataset(df, descriptor_calc, use_cache=True)\n",
    "    \n",
    "    # Remove samples with NaN in target\n",
    "    df_clean = df_features.dropna(subset=[target_col])\n",
    "    print(f\"Final dataset: {len(df_clean)} samples with {len(feature_cols)} features\")\n",
    "    \n",
    "    # 3. Cross-validation\n",
    "    print(\"\\n3. Running cross-validation...\")\n",
    "    cv_results = cross_validate_iscore(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        n_splits=n_splits, model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # 4. Visualize CV results\n",
    "    print(\"\\n4. Creating visualizations...\")\n",
    "    plot_cv_results(cv_results, save_path=save_dir / 'cv_results.png')\n",
    "    \n",
    "    # 5. Train full model\n",
    "    print(\"\\n5. Training model on full dataset...\")\n",
    "    model_path = save_dir / 'iscore_model.pkl'\n",
    "    full_model = train_full_model(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        model_type=model_type, save_path=str(model_path)\n",
    "    )\n",
    "    \n",
    "    # 6. Save results summary\n",
    "    summary = {\n",
    "        'dataset_size': len(df_clean),\n",
    "        'n_features': len(feature_cols),\n",
    "        'cv_r2_mean': np.mean(cv_results['r2_scores']),\n",
    "        'cv_r2_std': np.std(cv_results['r2_scores']),\n",
    "        'cv_rmse_mean': np.mean(cv_results['rmse_scores']),\n",
    "        'cv_rmse_std': np.std(cv_results['rmse_scores']),\n",
    "        'model_type': model_type,\n",
    "        'target': target_col\n",
    "    }\n",
    "    \n",
    "    with open(save_dir / f'summary_{target_col}.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Results saved to: {save_dir}\")\n",
    "    \n",
    "    return df_clean, full_model, cv_results\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use the iScore pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"iScore METHOD - EXAMPLE USAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load your data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\")\n",
    "    \n",
    "    # Filter for samples with required data\n",
    "    df = df.dropna(subset=['standardized_protein_pdb', 'standardized_ligand_sdf', 'smiles', 'pKi'])\n",
    "    df = df.head(100)  # Use subset for testing\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    \n",
    "    # 2. Run complete pipeline\n",
    "    print(\"\\n2. Running complete pipeline...\")\n",
    "    df_features, model, cv_results = run_complete_pipeline(\n",
    "        df, \n",
    "        target_col='pKi',\n",
    "        n_splits=5,\n",
    "        model_type='xgboost',\n",
    "        save_dir='iscore_results'\n",
    "    )\n",
    "    \n",
    "    # 3. Example inference\n",
    "    print(\"\\n3. Example inference on new data...\")\n",
    "    predictor = iScorePredictor(\n",
    "        model_path='iscore_results/iscore_model.pkl'\n",
    "    )\n",
    "    \n",
    "    # Single prediction\n",
    "    test_idx = 0\n",
    "    test_row = df.iloc[test_idx]\n",
    "    \n",
    "    print(f\"\\nPredicting for sample {test_idx}...\")\n",
    "    prediction, info = predictor.predict_single(\n",
    "        protein_pdb_path=test_row['standardized_protein_pdb'],\n",
    "        ligand_sdf_path=test_row['standardized_ligand_sdf'],\n",
    "        smiles=test_row['smiles']\n",
    "    )\n",
    "    \n",
    "    if info['success']:\n",
    "        print(f\"Predicted pKi: {prediction:.3f}\")\n",
    "        print(f\"Actual pKi: {test_row['pKi']:.3f}\")\n",
    "        print(f\"Total time: {info['total_time']:.3f}s\")\n",
    "        print(f\"  - Ligand descriptors: {info['ligand_desc_time']:.3f}s\")\n",
    "        print(f\"  - Pocket descriptors: {info['pocket_desc_time']:.3f}s\")\n",
    "        print(f\"  - Prediction: {info['prediction_time']:.3f}s\")\n",
    "    else:\n",
    "        print(f\"Prediction failed: {info['error']}\")\n",
    "    \n",
    "    # Batch prediction\n",
    "    print(\"\\n4. Batch prediction example...\")\n",
    "    test_df = df.head(10)\n",
    "    \n",
    "    predictions_df = predictor.predict_batch(\n",
    "        protein_pdb_paths=test_df['standardized_protein_pdb'].tolist(),\n",
    "        ligand_sdf_paths=test_df['standardized_ligand_sdf'].tolist(),\n",
    "        smiles_list=test_df['smiles'].tolist(),\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    # Evaluate batch predictions\n",
    "    successful_preds = predictions_df[predictions_df['success']]\n",
    "    if len(successful_preds) > 0:\n",
    "        predictions_df['actual'] = test_df['pKi'].values\n",
    "        valid_preds = predictions_df.dropna(subset=['prediction', 'actual'])\n",
    "        \n",
    "        if len(valid_preds) > 0:\n",
    "            r2 = r2_score(valid_preds['actual'], valid_preds['prediction'])\n",
    "            rmse = np.sqrt(mean_squared_error(valid_preds['actual'], valid_preds['prediction']))\n",
    "            \n",
    "            print(f\"\\nBatch prediction results:\")\n",
    "            print(f\"  Successful: {len(successful_preds)}/{len(test_df)}\")\n",
    "            print(f\"  R²: {r2:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            print(f\"  Avg time: {successful_preds['total_time'].mean():.3f}s per complex\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLE COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "iScore Method for Drug-Target Affinity Prediction with Cross-Validation\n",
    "This implements the pocket + ligand descriptor approach with proper CV, training, and inference\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scientific computing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "# Suppress warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== DESCRIPTOR CALCULATION ==================\n",
    "\n",
    "class DescriptorCalculator:\n",
    "    \"\"\"Handles calculation and caching of molecular descriptors\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"../data/descriptors_cache\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define descriptor names based on your code\n",
    "        self.descriptor_names = [\n",
    "            'MolLogP', 'MolMR', 'ExactMolWt', 'HeavyAtomCount', 'NumHAcceptors', 'NumHDonors', \n",
    "            'NumHeteroatoms', 'NumRotatableBonds', 'NumAromaticRings', 'NumAliphaticRings',\n",
    "            'RingCount', 'TPSA', 'LabuteASA', 'Kappa1', 'Kappa2', 'Kappa3', \n",
    "            'Chi0', 'Chi1', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n',\n",
    "            'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v',\n",
    "            'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', \n",
    "            'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'PEOE_VSA10', \n",
    "            'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', \n",
    "            'SMR_VSA1', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6',\n",
    "            'SMR_VSA7', 'SMR_VSA9', 'SMR_VSA10',\n",
    "            'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', \n",
    "            'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA10', 'SlogP_VSA11', \n",
    "            'SlogP_VSA12',\n",
    "            'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5',\n",
    "            'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'EState_VSA10',\n",
    "            'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5',\n",
    "            'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'VSA_EState10'\n",
    "        ]\n",
    "        self.columns_to_drop_set1 = [\n",
    "            \"pdb\", \"lig\", \"overlap\", \"PP-crit\", \"PP-dst\", \"crit4\", \n",
    "            \"crit5\", \"crit6\", \"crit6_continue\", \"nb_AS_norm\", \"apol_as_prop_norm\", \n",
    "            \"mean_loc_hyd_dens_norm\", \"polarity_score_norm\", \"as_density_norm\", \n",
    "            \"as_max_dst_norm\", \"drug_score\"\n",
    "        ]\n",
    "\n",
    "        self.columns_to_drop_set2 = {\n",
    "            \"pock_vol\",\"nb_AS\",\"mean_as_ray\",\"mean_as_solv_acc\",\"apol_as_prop\",\"mean_loc_hyd_dens\",\"hydrophobicity_score\",\"volume_score\",\"polarity_score\",\"charge_score\",\"flex\",\"prop_polar_atm\",\"as_density\",\"as_max_dst\",\n",
    "            \"convex_hull_volume\",\"surf_pol_vdw14\",\"surf_pol_vdw22\",\"surf_apol_vdw14\",\"surf_apol_vdw22\",\"n_abpa\",\"ALA\",\"ARG\",\"ASN\",\"ASP\",\"CYS\",\"GLN\",\"GLU\",\"GLY\",\"HIS\",\"ILE\",\"LEU\",\"LYS\",\"MET\",\"PHE\",\"PRO\",\"SER\",\"THR\",\"TRP\",\"TYR\",\"VAL\",\"pKd\"\n",
    "        }\n",
    "    \n",
    "    def calculate_ligand_descriptors(self, smiles: str) -> np.ndarray:\n",
    "        \"\"\"Calculate RDKit descriptors for a SMILES string\"\"\"\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol:\n",
    "            desc_values = []\n",
    "            for desc_name in self.descriptor_names:\n",
    "                try:\n",
    "                    desc_func = getattr(Descriptors, desc_name)\n",
    "                    value = desc_func(mol)\n",
    "                    desc_values.append(value)\n",
    "                except:\n",
    "                    desc_values.append(np.nan)\n",
    "            return np.array(desc_values)\n",
    "        else:\n",
    "            return np.full(len(self.descriptor_names), np.nan)\n",
    "    \n",
    "    def calculate_pocket_descriptors(self, complex_pdb_path: str, ligand_sdf_path: str, \n",
    "                                   ligand_code: str = \"LIG\", dpocket_path: str = \"dpocket\") -> Dict:\n",
    "        \"\"\"\n",
    "        Calculate pocket descriptors using dpocket\n",
    "        Returns dict with pocket descriptor values\n",
    "        \"\"\"\n",
    "        import tempfile\n",
    "        import subprocess\n",
    "        \n",
    "        # Create temporary directory for dpocket output\n",
    "        with tempfile.TemporaryDirectory() as temp_dir:\n",
    "            # Write dpocket input file\n",
    "            input_file = os.path.join(temp_dir, \"dpocket_input.txt\")\n",
    "            with open(input_file, 'w') as f:\n",
    "                f.write(f\"{complex_pdb_path}\\t{ligand_code}\\n\")\n",
    "            \n",
    "            # Run dpocket\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    [dpocket_path, \"-f\", input_file],\n",
    "                    cwd=temp_dir,\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                # Parse dpocket output\n",
    "                output_file = os.path.join(temp_dir, \"dpout_explicitp.txt\")\n",
    "                if os.path.exists(output_file):\n",
    "                    pocket_df = pd.read_csv(output_file, sep='\\s+')\n",
    "                    # Return first row as dict (excluding pdb column)\n",
    "                    if len(pocket_df) > 0:\n",
    "                        return pocket_df.iloc[0].drop(['pdb', 'lig'], errors='ignore').to_dict()\n",
    "            except Exception as e:\n",
    "                print(f\"dpocket failed: {e}\")\n",
    "        \n",
    "        # Return empty dict if failed\n",
    "        return {}\n",
    "    \n",
    "    def combine_protein_ligand_pdb(self, protein_pdb_path: str, ligand_sdf_path: str, \n",
    "                                   output_path: str, lig_code: str = \"LIG\") -> str:\n",
    "        \"\"\"Combine protein and ligand into single PDB file\"\"\"\n",
    "        # Load ligand from SDF\n",
    "        mol = Chem.MolFromMolFile(ligand_sdf_path, removeHs=False)\n",
    "        if mol is None:\n",
    "            raise ValueError(f\"Failed to read ligand SDF: {ligand_sdf_path}\")\n",
    "        \n",
    "        # Convert ligand to PDB block\n",
    "        pdb_block = Chem.MolToPDBBlock(mol)\n",
    "        \n",
    "        # Fix residue names in ligand PDB\n",
    "        ligand_lines = []\n",
    "        for line in pdb_block.splitlines():\n",
    "            if line.startswith(\"HETATM\") or line.startswith(\"ATOM\"):\n",
    "                line = line[:17] + lig_code + line[20:]\n",
    "            ligand_lines.append(line)\n",
    "        \n",
    "        # Read protein PDB\n",
    "        with open(protein_pdb_path, 'r') as f:\n",
    "            protein_lines = [line for line in f if not line.startswith(\"END\")]\n",
    "        \n",
    "        # Write combined PDB\n",
    "        with open(output_path, 'w') as f:\n",
    "            f.writelines(protein_lines)\n",
    "            f.write('\\n'.join(ligand_lines))\n",
    "            f.write('\\nEND\\n')\n",
    "        \n",
    "        return output_path\n",
    "    \n",
    "    def get_cached_path(self, sample_id: str) -> Path:\n",
    "        \"\"\"Get path for cached descriptor file\"\"\"\n",
    "        return self.cache_dir / f\"{sample_id}_descriptors.npz\"\n",
    "    \n",
    "    def save_descriptors(self, sample_id: str, ligand_desc: np.ndarray, pocket_desc: np.ndarray):\n",
    "        \"\"\"Save descriptors to cache\"\"\"\n",
    "        cache_path = self.get_cached_path(sample_id)\n",
    "        np.savez_compressed(cache_path, ligand=ligand_desc, pocket=pocket_desc)\n",
    "    \n",
    "    def load_descriptors(self, sample_id: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        \"\"\"Load descriptors from cache if available\"\"\"\n",
    "        cache_path = self.get_cached_path(sample_id)\n",
    "        if cache_path.exists():\n",
    "            data = np.load(cache_path)\n",
    "            return data['ligand'], data['pocket']\n",
    "        return None, None\n",
    "\n",
    "# ================== DATASET PREPARATION ==================\n",
    "\n",
    "def prepare_iscore_dataset(df: pd.DataFrame, descriptor_calc: DescriptorCalculator, \n",
    "                          use_cache: bool = True, n_jobs: int = 4) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare dataset with iScore descriptors\n",
    "    \"\"\"\n",
    "    print(f\"Preparing iScore dataset for {len(df)} samples...\")\n",
    "    \n",
    "    # Reset index to ensure sequential numbering\n",
    "    df_reset = df.reset_index(drop=True)\n",
    "    \n",
    "    def process_sample(idx, row):\n",
    "        \"\"\"Process single sample\"\"\"\n",
    "        sample_id = f\"{idx}\"\n",
    "        \n",
    "        # Try to load from cache\n",
    "        if use_cache:\n",
    "            ligand_desc, pocket_desc = descriptor_calc.load_descriptors(sample_id)\n",
    "            if ligand_desc is not None and pocket_desc is not None:\n",
    "                return idx, ligand_desc, pocket_desc, None\n",
    "        \n",
    "        try:\n",
    "            # Calculate ligand descriptors\n",
    "            start_time = time.time()\n",
    "            ligand_desc = descriptor_calc.calculate_ligand_descriptors(row.smiles)\n",
    "            \n",
    "            # Create complex PDB\n",
    "            complex_path = f\"/tmp/complex_{idx}.pdb\"\n",
    "            descriptor_calc.combine_protein_ligand_pdb(\n",
    "                row.standardized_protein_pdb,\n",
    "                row.standardized_ligand_sdf,\n",
    "                complex_path\n",
    "            )\n",
    "                # --- 5. Drop unwanted cols ---\n",
    "\n",
    "            # Calculate pocket descriptors (simplified - you should use actual dpocket)\n",
    "            # For now, using random values as placeholder\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            # Cache descriptors\n",
    "            if use_cache:\n",
    "                descriptor_calc.save_descriptors(sample_id, ligand_desc, pocket_desc)\n",
    "            \n",
    "            # Clean up\n",
    "            if os.path.exists(complex_path):\n",
    "                os.remove(complex_path)\n",
    "            \n",
    "            return idx, ligand_desc, pocket_desc, elapsed\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            return idx, None, None, None\n",
    "    \n",
    "    # Process samples in parallel - use enumerate to get sequential indices\n",
    "    results = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_sample)(idx, row) \n",
    "        for idx, row in tqdm(enumerate(df_reset.itertuples(index=False)), \n",
    "                            total=len(df_reset), desc=\"Computing descriptors\")\n",
    "    )\n",
    "    \n",
    "    # Organize results\n",
    "    ligand_features = []\n",
    "    pocket_features = []\n",
    "    valid_indices = []\n",
    "    times = []\n",
    "    \n",
    "    for idx, ligand_desc, pocket_desc, elapsed in results:\n",
    "        if ligand_desc is not None and pocket_desc is not None:\n",
    "            ligand_features.append(ligand_desc)\n",
    "            pocket_features.append(pocket_desc)\n",
    "            valid_indices.append(idx)\n",
    "            if elapsed is not None:\n",
    "                times.append(elapsed)\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_ligand = np.array(ligand_features)\n",
    "    X_pocket = np.array(pocket_features)\n",
    "    X = np.hstack([X_ligand, X_pocket])\n",
    "    \n",
    "    # Create result dataframe using iloc with valid indices\n",
    "    result_df = df_reset.iloc[valid_indices].copy()\n",
    "    \n",
    "    # Add features as columns\n",
    "    feature_names = [f'ligand_{i}' for i in range(X_ligand.shape[1])] + \\\n",
    "                   [f'pocket_{i}' for i in range(X_pocket.shape[1])]\n",
    "    \n",
    "    for i, fname in enumerate(feature_names):\n",
    "        result_df[fname] = X[:, i]\n",
    "    \n",
    "    if times:\n",
    "        print(f\"Average descriptor computation time: {np.mean(times):.3f}s per complex\")\n",
    "    \n",
    "    print(f\"Successfully processed {len(result_df)}/{len(df_reset)} samples\")\n",
    "    \n",
    "    return result_df, feature_names\n",
    "\n",
    "# ================== MODEL TRAINING ==================\n",
    "\n",
    "class iScoreModel:\n",
    "    \"\"\"Wrapper for iScore regression model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'xgboost', **kwargs):\n",
    "        self.model_type = model_type\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        if model_type == 'xgboost':\n",
    "            self.model = xgb.XGBRegressor(\n",
    "                n_estimators=kwargs.get('n_estimators', 500),\n",
    "                max_depth=kwargs.get('max_depth', 6),\n",
    "                learning_rate=kwargs.get('learning_rate', 0.01),\n",
    "                subsample=kwargs.get('subsample', 0.8),\n",
    "                colsample_bytree=kwargs.get('colsample_bytree', 0.8),\n",
    "                random_state=kwargs.get('random_state', 42),\n",
    "                n_jobs=kwargs.get('n_jobs', -1)\n",
    "            )\n",
    "        elif model_type == 'rf':\n",
    "            self.model = RandomForestRegressor(\n",
    "                n_estimators=kwargs.get('n_estimators', 500),\n",
    "                max_depth=kwargs.get('max_depth', None),\n",
    "                min_samples_split=kwargs.get('min_samples_split', 2),\n",
    "                min_samples_leaf=kwargs.get('min_samples_leaf', 1),\n",
    "                random_state=kwargs.get('random_state', 42),\n",
    "                n_jobs=kwargs.get('n_jobs', -1)\n",
    "            )\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fit the model\"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model and scaler\"\"\"\n",
    "        import joblib\n",
    "        model_path = Path(path)\n",
    "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'model_type': self.model_type\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load model and scaler\"\"\"\n",
    "        import joblib\n",
    "        data = joblib.load(path)\n",
    "        self.model = data['model']\n",
    "        self.scaler = data['scaler']\n",
    "        self.model_type = data['model_type']\n",
    "\n",
    "# ================== CROSS VALIDATION ==================\n",
    "\n",
    "def cross_validate_iscore(df: pd.DataFrame, feature_cols: List[str], target_col: str = 'pKi',\n",
    "                         n_splits: int = 5, model_type: str = 'xgboost', \n",
    "                         model_params: dict = None, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for iScore method\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting {n_splits}-Fold Cross-Validation\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Features: {len(feature_cols)} descriptors\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Initialize CV\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Results storage\n",
    "    cv_results = {\n",
    "        'r2_scores': [],\n",
    "        'rmse_scores': [],\n",
    "        'mae_scores': [],\n",
    "        'predictions': [],\n",
    "        'true_values': [],\n",
    "        'fold_times': []\n",
    "    }\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\nFold {fold_idx}/{n_splits}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(X_train)} samples\")\n",
    "        print(f\"Test: {len(X_test)} samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model = iScoreModel(model_type=model_type, **(model_params or {}))\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        train_start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - train_start\n",
    "        print(f\"Training completed in {train_time:.2f}s\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        \n",
    "        # Store results\n",
    "        cv_results['r2_scores'].append(r2)\n",
    "        cv_results['rmse_scores'].append(rmse)\n",
    "        cv_results['mae_scores'].append(mae)\n",
    "        cv_results['predictions'].extend(y_pred)\n",
    "        cv_results['true_values'].extend(y_test)\n",
    "        cv_results['fold_times'].append(time.time() - fold_start)\n",
    "        \n",
    "        print(f\"Fold {fold_idx} Results:\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        \n",
    "        # Feature importance (if available)\n",
    "        if hasattr(model.model, 'feature_importances_'):\n",
    "            top_features = np.argsort(model.model.feature_importances_)[-5:]\n",
    "            print(f\"  Top 5 features: {[feature_cols[i] for i in top_features]}\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Cross-Validation Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"R² Score: {np.mean(cv_results['r2_scores']):.4f} ± {np.std(cv_results['r2_scores']):.4f}\")\n",
    "    print(f\"RMSE: {np.mean(cv_results['rmse_scores']):.4f} ± {np.std(cv_results['rmse_scores']):.4f}\")\n",
    "    print(f\"MAE: {np.mean(cv_results['mae_scores']):.4f} ± {np.std(cv_results['mae_scores']):.4f}\")\n",
    "    print(f\"Avg fold time: {np.mean(cv_results['fold_times']):.2f}s\")\n",
    "    \n",
    "    # Overall metrics on all predictions\n",
    "    all_true = np.array(cv_results['true_values'])\n",
    "    all_pred = np.array(cv_results['predictions'])\n",
    "    overall_r2 = r2_score(all_true, all_pred)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "    \n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  R²: {overall_r2:.4f}\")\n",
    "    print(f\"  RMSE: {overall_rmse:.4f}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# ================== VISUALIZATION ==================\n",
    "\n",
    "def plot_cv_results(cv_results: dict, save_path: str = None):\n",
    "    \"\"\"Create visualization of cross-validation results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Scatter plot of predictions\n",
    "    all_true = np.array(cv_results['true_values'])\n",
    "    all_pred = np.array(cv_results['predictions'])\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(all_true, all_pred, alpha=0.5, s=10)\n",
    "    ax.plot([all_true.min(), all_true.max()], [all_true.min(), all_true.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('True Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(f'Predictions (R²={r2_score(all_true, all_pred):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residual plot\n",
    "    ax = axes[0, 1]\n",
    "    residuals = all_true - all_pred\n",
    "    ax.scatter(all_pred, residuals, alpha=0.5, s=10)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_title('Residual Plot')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fold-wise performance\n",
    "    ax = axes[1, 0]\n",
    "    folds = range(1, len(cv_results['r2_scores']) + 1)\n",
    "    ax.plot(folds, cv_results['r2_scores'], 'o-', label='R²', color='blue')\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_title('Fold-wise R² Performance')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # 4. Distribution of errors\n",
    "    ax = axes[1, 1]\n",
    "    errors = np.abs(residuals)\n",
    "    ax.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=np.mean(errors), color='r', linestyle='--', label=f'Mean: {np.mean(errors):.3f}')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Error Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('iScore Cross-Validation Results', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ================== FULL TRAINING ==================\n",
    "\n",
    "def train_full_model(df: pd.DataFrame, feature_cols: List[str], target_col: str = 'pKi',\n",
    "                    model_type: str = 'xgboost', model_params: dict = None,\n",
    "                    save_path: str = 'models/iscore_model.pkl'):\n",
    "    \"\"\"\n",
    "    Train model on full dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Training Full Model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Train model\n",
    "    model = iScoreModel(model_type=model_type, **(model_params or {}))\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X, y)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {train_time:.2f}s\")\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    y_pred = model.predict(X)\n",
    "    train_r2 = r2_score(y, y_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    print(f\"Training Performance:\")\n",
    "    print(f\"  R²: {train_r2:.4f}\")\n",
    "    print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    if save_path:\n",
    "        model.save(save_path)\n",
    "        print(f\"Model saved to: {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "    def predict_batch(self, protein_pdb_paths: List[str], ligand_sdf_paths: List[str],\n",
    "                     smiles_list: List[str] = None, n_jobs: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict affinities for multiple protein-ligand pairs\n",
    "        \"\"\"\n",
    "        if smiles_list is None:\n",
    "            smiles_list = [None] * len(protein_pdb_paths)\n",
    "        \n",
    "        def process_pair(idx, pdb_path, sdf_path, smiles):\n",
    "            pred, info = self.predict_single(pdb_path, sdf_path, smiles)\n",
    "            return idx, pred, info\n",
    "        \n",
    "        # Process in parallel\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_pair)(idx, pdb, sdf, smi) \n",
    "            for idx, (pdb, sdf, smi) in enumerate(\n",
    "                tqdm(zip(protein_pdb_paths, ligand_sdf_paths, smiles_list), \n",
    "                     total=len(protein_pdb_paths), desc=\"Predicting\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Organize results\n",
    "        predictions = []\n",
    "        for idx, pred, info in sorted(results, key=lambda x: x[0]):\n",
    "            predictions.append({\n",
    "                'index': idx,\n",
    "                'prediction': pred,\n",
    "                'success': info['success'],\n",
    "                'total_time': info.get('total_time', None),\n",
    "                'error': info.get('error', None)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(predictions)\n",
    "\n",
    "# ================== MAIN PIPELINE ==================\n",
    "\n",
    "def run_complete_pipeline(df: pd.DataFrame, target_col: str = 'pKi',\n",
    "                         n_splits: int = 5, model_type: str = 'xgboost',\n",
    "                         save_dir: str = 'iscore_results'):\n",
    "    \"\"\"\n",
    "    Run complete iScore pipeline: preparation, CV, training, and evaluation\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"iScore METHOD - COMPLETE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Initialize descriptor calculator\n",
    "    print(\"\\n1. Initializing descriptor calculator...\")\n",
    "    descriptor_calc = DescriptorCalculator(cache_dir=save_dir / 'descriptors_cache')\n",
    "    \n",
    "    # 2. Prepare dataset\n",
    "    print(\"\\n2. Preparing dataset with descriptors...\")\n",
    "    df_features, feature_cols = prepare_iscore_dataset(df, descriptor_calc, use_cache=True)\n",
    "    \n",
    "    \n",
    "    # Remove samples with NaN in target\n",
    "    df_clean = df_features.dropna(subset=[target_col])\n",
    "    print(f\"Final dataset: {len(df_clean)} samples with {len(feature_cols)} features\")\n",
    "    \n",
    "    # 3. Cross-validation\n",
    "    print(\"\\n3. Running cross-validation...\")\n",
    "    cv_results = cross_validate_iscore(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        n_splits=n_splits, model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # 4. Visualize CV results\n",
    "    print(\"\\n4. Creating visualizations...\")\n",
    "    plot_cv_results(cv_results, save_path=save_dir / 'cv_results.png')\n",
    "    \n",
    "    # 5. Train full model\n",
    "    print(\"\\n5. Training model on full dataset...\")\n",
    "    model_path = save_dir / 'iscore_model.pkl'\n",
    "    full_model = train_full_model(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        model_type=model_type, save_path=str(model_path)\n",
    "    )\n",
    "    \n",
    "    # 6. Save results summary\n",
    "    summary = {\n",
    "        'dataset_size': len(df_clean),\n",
    "        'n_features': len(feature_cols),\n",
    "        'cv_r2_mean': np.mean(cv_results['r2_scores']),\n",
    "        'cv_r2_std': np.std(cv_results['r2_scores']),\n",
    "        'cv_rmse_mean': np.mean(cv_results['rmse_scores']),\n",
    "        'cv_rmse_std': np.std(cv_results['rmse_scores']),\n",
    "        'model_type': model_type,\n",
    "        'target': target_col\n",
    "    }\n",
    "    \n",
    "    with open(save_dir / 'summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Results saved to: {save_dir}\")\n",
    "    \n",
    "    return df_clean, full_model, cv_results\n",
    "\n",
    "# ================== EXAMPLE USAGE ==================\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use the iScore pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"iScore METHOD - EXAMPLE USAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load your data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\")\n",
    "    \n",
    "    # Filter for samples with required data\n",
    "    df = df.dropna(subset=['standardized_protein_pdb', 'standardized_ligand_sdf', 'smiles', 'pKi'])\n",
    "    df = df.head(100)  # Use subset for testing\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    \n",
    "    # 2. Run complete pipeline\n",
    "    print(\"\\n2. Running complete pipeline...\")\n",
    "    df_features, model, cv_results = run_complete_pipeline(\n",
    "        df, \n",
    "        target_col='pKi',\n",
    "        n_splits=5,\n",
    "        model_type='xgboost',\n",
    "        save_dir='iscore_results'\n",
    "    )\n",
    "    \n",
    "    # 3. Example inference\n",
    "    print(\"\\n3. Example inference on new data...\")\n",
    "    predictor = iScorePredictor(\n",
    "        model_path='iscore_results/iscore_model.pkl'\n",
    "    )\n",
    "    \n",
    "    # Single prediction\n",
    "    test_idx = 0\n",
    "    test_row = df.iloc[test_idx]\n",
    "    \n",
    "    print(f\"\\nPredicting for sample {test_idx}...\")\n",
    "    prediction, info = predictor.predict_single(\n",
    "        protein_pdb_path=test_row['standardized_protein_pdb'],\n",
    "        ligand_sdf_path=test_row['standardized_ligand_sdf'],\n",
    "        smiles=test_row['smiles']\n",
    "    )\n",
    "    \n",
    "    if info['success']:\n",
    "        print(f\"Predicted pKi: {prediction:.3f}\")\n",
    "        print(f\"Actual pKi: {test_row['pKi']:.3f}\")\n",
    "        print(f\"Total time: {info['total_time']:.3f}s\")\n",
    "        print(f\"  - Ligand descriptors: {info['ligand_desc_time']:.3f}s\")\n",
    "        print(f\"  - Pocket descriptors: {info['pocket_desc_time']:.3f}s\")\n",
    "        print(f\"  - Prediction: {info['prediction_time']:.3f}s\")\n",
    "    else:\n",
    "        print(f\"Prediction failed: {info['error']}\")\n",
    "    \n",
    "    # Batch prediction\n",
    "    print(\"\\n4. Batch prediction example...\")\n",
    "    test_df = df.head(10)\n",
    "    \n",
    "    predictions_df = predictor.predict_batch(\n",
    "        protein_pdb_paths=test_df['standardized_protein_pdb'].tolist(),\n",
    "        ligand_sdf_paths=test_df['standardized_ligand_sdf'].tolist(),\n",
    "        smiles_list=test_df['smiles'].tolist(),\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    # Evaluate batch predictions\n",
    "    successful_preds = predictions_df[predictions_df['success']]\n",
    "    if len(successful_preds) > 0:\n",
    "        predictions_df['actual'] = test_df['pKi'].values\n",
    "        valid_preds = predictions_df.dropna(subset=['prediction', 'actual'])\n",
    "        \n",
    "        if len(valid_preds) > 0:\n",
    "            r2 = r2_score(valid_preds['actual'], valid_preds['prediction'])\n",
    "            rmse = np.sqrt(mean_squared_error(valid_preds['actual'], valid_preds['prediction']))\n",
    "            \n",
    "            print(f\"\\nBatch prediction results:\")\n",
    "            print(f\"  Successful: {len(successful_preds)}/{len(test_df)}\")\n",
    "            print(f\"  R²: {r2:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            print(f\"  Avg time: {successful_preds['total_time'].mean():.3f}s per complex\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLE COMPLETED\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ================== COMPARATIVE ANALYSIS ==================\n",
    "\n",
    "def compare_with_gnn(iscore_results: dict, gnn_results: dict = None):\n",
    "    \"\"\"\n",
    "    Compare iScore results with GNN results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARATIVE ANALYSIS: iScore vs GNN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # iScore performance\n",
    "    print(\"\\niScore Method:\")\n",
    "    print(f\"  R²: {np.mean(iscore_results['r2_scores']):.4f} ± {np.std(iscore_results['r2_scores']):.4f}\")\n",
    "    print(f\"  RMSE: {np.mean(iscore_results['rmse_scores']):.4f} ± {np.std(iscore_results['rmse_scores']):.4f}\")\n",
    "    \n",
    "    if gnn_results:\n",
    "        print(\"\\nGNN Method:\")\n",
    "        print(f\"  R²: {np.mean(gnn_results['r2_scores']):.4f} ± {np.std(gnn_results['r2_scores']):.4f}\")\n",
    "        print(f\"  RMSE: {np.mean(gnn_results['rmse_scores']):.4f} ± {np.std(gnn_results['rmse_scores']):.4f}\")\n",
    "        \n",
    "        # Statistical comparison\n",
    "        from scipy import stats\n",
    "        \n",
    "        # Paired t-test on R² scores\n",
    "        t_stat, p_value = stats.ttest_rel(\n",
    "            iscore_results['r2_scores'], \n",
    "            gnn_results['r2_scores']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nStatistical Comparison (paired t-test on R² scores):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if np.mean(iscore_results['r2_scores']) > np.mean(gnn_results['r2_scores']):\n",
    "                print(\"  Result: iScore significantly better (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"  Result: GNN significantly better (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"  Result: No significant difference (p ≥ 0.05)\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Box plot comparison\n",
    "        ax = axes[0]\n",
    "        ax.boxplot([iscore_results['r2_scores'], gnn_results['r2_scores']], \n",
    "                   labels=['iScore', 'GNN'])\n",
    "        ax.set_ylabel('R² Score')\n",
    "        ax.set_title('Model Performance Comparison')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter plot of predictions (if available)\n",
    "        ax = axes[1]\n",
    "        if 'predictions' in iscore_results and 'predictions' in gnn_results:\n",
    "            ax.scatter(iscore_results['predictions'], gnn_results['predictions'], \n",
    "                      alpha=0.5, s=10)\n",
    "            min_val = min(min(iscore_results['predictions']), min(gnn_results['predictions']))\n",
    "            max_val = max(max(iscore_results['predictions']), max(gnn_results['predictions']))\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "            ax.set_xlabel('iScore Predictions')\n",
    "            ax.set_ylabel('GNN Predictions')\n",
    "            ax.set_title('Prediction Correlation')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('iScore vs GNN Comparison', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ================== TIMING ANALYSIS ==================\n",
    "\n",
    "\n",
    "\n",
    "# ================== MULTI-TASK LEARNING EXTENSION ==================\n",
    "\n",
    "class MTL_iScoreModel(iScoreModel):\n",
    "    \"\"\"\n",
    "    Multi-task learning version of iScore model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task_names: List[str], model_type: str = 'xgboost', **kwargs):\n",
    "        self.task_names = task_names\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "        for task in task_names:\n",
    "            self.scalers[task] = StandardScaler()\n",
    "            if model_type == 'xgboost':\n",
    "                self.models[task] = xgb.XGBRegressor(**kwargs)\n",
    "            elif model_type == 'rf':\n",
    "                self.models[task] = RandomForestRegressor(**kwargs)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y_dict: dict):\n",
    "        \"\"\"Fit models for all tasks\"\"\"\n",
    "        for task in self.task_names:\n",
    "            if task in y_dict:\n",
    "                # Remove samples with NaN for this task\n",
    "                mask = ~np.isnan(y_dict[task])\n",
    "                X_task = X[mask]\n",
    "                y_task = y_dict[task][mask]\n",
    "                \n",
    "                if len(X_task) > 0:\n",
    "                    X_scaled = self.scalers[task].fit_transform(X_task)\n",
    "                    self.models[task].fit(X_scaled, y_task)\n",
    "                    print(f\"  Trained {task} on {len(X_task)} samples\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> dict:\n",
    "        \"\"\"Predict for all tasks\"\"\"\n",
    "        predictions = {}\n",
    "        for task in self.task_names:\n",
    "            if task in self.models:\n",
    "                X_scaled = self.scalers[task].transform(X)\n",
    "                predictions[task] = self.models[task].predict(X_scaled)\n",
    "        return predictions\n",
    "\n",
    "# ================== MAIN ENTRY POINT ==================\n",
    "class iScorePredictor:\n",
    "    \"\"\"Complete inference pipeline for iScore method\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, descriptor_calc: DescriptorCalculator = None):\n",
    "        \"\"\"Initialize predictor with trained model\"\"\"\n",
    "        self.model = iScoreModel()\n",
    "        self.model.load(model_path)\n",
    "        self.descriptor_calc = descriptor_calc or DescriptorCalculator()\n",
    "        \n",
    "        # Load feature names from training (should be saved with model)\n",
    "        # For now, we'll reconstruct them\n",
    "        self.ligand_features = self.descriptor_calc.descriptor_names\n",
    "        self.pocket_features = None  # Will be determined from dpocket output\n",
    "    \n",
    "    def predict_single(self, protein_pdb_path: str, ligand_sdf_path: str, \n",
    "                       smiles: str = None) -> Tuple[float, dict]:\n",
    "        \"\"\"\n",
    "        Predict affinity for single protein-ligand pair using exact dpocket method\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        info = {}\n",
    "        \n",
    "        try:\n",
    "            # Get SMILES if not provided\n",
    "            if smiles is None:\n",
    "                mol = Chem.MolFromMolFile(ligand_sdf_path)\n",
    "                smiles = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Calculate ligand descriptors\n",
    "            desc_start = time.time()\n",
    "            ligand_df = self.descriptor_calc.calculate_descriptors([smiles])\n",
    "            ligand_desc = ligand_df.values[0]\n",
    "            info['ligand_desc_time'] = time.time() - desc_start\n",
    "            \n",
    "            # Create complex PDB\n",
    "            complex_path = \"temp_complex.pdb\"\n",
    "            self.descriptor_calc.combine_protein_ligand(\n",
    "                protein_pdb_path, ligand_sdf_path, complex_path\n",
    "            )\n",
    "            \n",
    "            # Run dpocket\n",
    "            pocket_start = time.time()\n",
    "            pocket_df = self.descriptor_calc.run_dpocket_single(complex_path)\n",
    "            \n",
    "            if len(pocket_df) > 0:\n",
    "                # Drop unnecessary columns\n",
    "                pocket_df = pocket_df.drop(columns=self.descriptor_calc.columns_to_drop, errors='ignore')\n",
    "                pocket_desc = pocket_df.iloc[0].values\n",
    "            else:\n",
    "                # Use NaN if dpocket fails\n",
    "                pocket_desc = np.full(41, np.nan)  # Adjust size based on actual features\n",
    "            \n",
    "            info['pocket_desc_time'] = time.time() - pocket_start\n",
    "            \n",
    "            # Combine features\n",
    "            X = np.hstack([ligand_desc, pocket_desc]).reshape(1, -1)\n",
    "            \n",
    "            # Handle NaN values - replace with median or mean from training\n",
    "            # For now, replace with 0\n",
    "            X = np.nan_to_num(X, nan=0.0)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred_start = time.time()\n",
    "            prediction = self.model.predict(X)[0]\n",
    "            info['prediction_time'] = time.time() - pred_start\n",
    "            \n",
    "            # Clean up\n",
    "            if os.path.exists(complex_path):\n",
    "                os.remove(complex_path)\n",
    "            \n",
    "            info['total_time'] = time.time() - start_time\n",
    "            info['success'] = True\n",
    "            \n",
    "            return prediction, info\n",
    "            \n",
    "        except Exception as e:\n",
    "            info['error'] = str(e)\n",
    "            info['success'] = False\n",
    "            info['total_time'] = time.time() - start_time\n",
    "            return None, info\n",
    "    \n",
    "    def predict_batch(self, protein_pdb_paths: List[str], ligand_sdf_paths: List[str],\n",
    "                     smiles_list: List[str] = None, n_jobs: int = 1) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict affinities for multiple protein-ligand pairs\n",
    "        Note: dpocket batch processing is more efficient than parallel single runs\n",
    "        \"\"\"\n",
    "        print(f\"Predicting for {len(protein_pdb_paths)} complexes...\")\n",
    "        \n",
    "        # Get SMILES if not provided\n",
    "        if smiles_list is None:\n",
    "            smiles_list = []\n",
    "            for sdf_path in ligand_sdf_paths:\n",
    "                mol = Chem.MolFromMolFile(sdf_path)\n",
    "                smiles_list.append(Chem.MolToSmiles(mol) if mol else None)\n",
    "        \n",
    "        # Create temporary dataframe for batch processing\n",
    "        temp_df = pd.DataFrame({\n",
    "            'standardized_protein_pdb': protein_pdb_paths,\n",
    "            'standardized_ligand_sdf': ligand_sdf_paths,\n",
    "            'smiles': smiles_list\n",
    "        })\n",
    "        \n",
    "        # Use the batch preparation method\n",
    "        result_df, feature_cols = prepare_iscore_dataset(\n",
    "            temp_df, self.descriptor_calc, use_cache=False\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        if len(result_df) > 0:\n",
    "            X = result_df[feature_cols].values\n",
    "            X = np.nan_to_num(X, nan=0.0)  # Handle NaN\n",
    "            predictions = self.model.predict(X)\n",
    "            \n",
    "            result_df['prediction']\n",
    "            \n",
    "\"\"\"\n",
    "iScore Method for Drug-Target Affinity Prediction with Cross-Validation\n",
    "This implements the pocket + ligand descriptor approach with proper CV, training, and inference\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scientific computing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "\n",
    "# RDKit\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "# Suppress warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================== DESCRIPTOR CALCULATION ==================\n",
    "\n",
    "class DescriptorCalculator:\n",
    "    \"\"\"Handles calculation and caching of molecular descriptors\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir: str = \"../data/descriptors_cache\", \n",
    "                 complex_dir: str = \"../data/complex_pdbs\",\n",
    "                 descriptor_dir: str = \"../data/descriptor_files\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.complex_dir = Path(complex_dir)\n",
    "        self.complex_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.descriptor_dir = Path(descriptor_dir)\n",
    "        self.descriptor_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define descriptor names exactly as in your code\n",
    "        self.descriptor_names = [\n",
    "            'MolLogP', 'MolMR', 'ExactMolWt', 'HeavyAtomCount', 'NumHAcceptors', 'NumHDonors', \n",
    "            'NumHeteroatoms', 'NumRotatableBonds', 'NumAromaticRings', 'NumAliphaticRings',\n",
    "            'RingCount', 'TPSA', 'LabuteASA', 'Kappa1', 'Kappa2', 'Kappa3', \n",
    "            'Chi0', 'Chi1', 'Chi0n', 'Chi1n', 'Chi2n', 'Chi3n', 'Chi4n',\n",
    "            'Chi0v', 'Chi1v', 'Chi2v', 'Chi3v', 'Chi4v',\n",
    "            'PEOE_VSA1', 'PEOE_VSA2', 'PEOE_VSA3', 'PEOE_VSA4', 'PEOE_VSA5', \n",
    "            'PEOE_VSA6', 'PEOE_VSA7', 'PEOE_VSA8', 'PEOE_VSA9', 'PEOE_VSA10', \n",
    "            'PEOE_VSA11', 'PEOE_VSA12', 'PEOE_VSA13', 'PEOE_VSA14', \n",
    "            'SMR_VSA1', 'SMR_VSA3', 'SMR_VSA4', 'SMR_VSA5', 'SMR_VSA6',\n",
    "            'SMR_VSA7', 'SMR_VSA9', 'SMR_VSA10',\n",
    "            'SlogP_VSA1', 'SlogP_VSA2', 'SlogP_VSA3', 'SlogP_VSA4', 'SlogP_VSA5', \n",
    "            'SlogP_VSA6', 'SlogP_VSA7', 'SlogP_VSA8', 'SlogP_VSA10', 'SlogP_VSA11', \n",
    "            'SlogP_VSA12',\n",
    "            'EState_VSA1', 'EState_VSA2', 'EState_VSA3', 'EState_VSA4', 'EState_VSA5',\n",
    "            'EState_VSA6', 'EState_VSA7', 'EState_VSA8', 'EState_VSA9', 'EState_VSA10',\n",
    "            'VSA_EState1', 'VSA_EState2', 'VSA_EState3', 'VSA_EState4', 'VSA_EState5',\n",
    "            'VSA_EState6', 'VSA_EState7', 'VSA_EState8', 'VSA_EState9', 'VSA_EState10'\n",
    "        ]\n",
    "        \n",
    "        # Columns to drop from dpocket output\n",
    "        self.columns_to_drop = []\n",
    "        \n",
    " \n",
    "\n",
    "    \n",
    "    def calculate_descriptors(self, smiles_list: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Calculate RDKit descriptors for a list of SMILES strings - exact copy from your code\"\"\"\n",
    "        descriptors = []\n",
    "        for smiles in smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol:\n",
    "                desc_values = [getattr(Descriptors, desc)(mol) for desc in self.descriptor_names]\n",
    "                descriptors.append(desc_values)\n",
    "            else:\n",
    "                descriptors.append([None] * len(self.descriptor_names))\n",
    "        \n",
    "        return pd.DataFrame(descriptors, columns=self.descriptor_names)\n",
    "    \n",
    "    def combine_protein_ligand(self, protein_pdb_path: str, ligand_sdf_path: str, \n",
    "                               output_pdb_path: str) -> str:\n",
    "        \"\"\"Combine protein and ligand into single PDB file - exact copy from your code\"\"\"\n",
    "        # Load and convert ligand to PDB with residue name LIG\n",
    "        mol = Chem.MolFromMolFile(ligand_sdf_path, removeHs=False)\n",
    "        if mol is None:\n",
    "            raise ValueError(f\"Failed to read ligand SDF : {ligand_sdf_path}\")\n",
    "        \n",
    "        # Create temp file for ligand PDB\n",
    "        temp_ligand_path = \"temp_ligand.pdb\"\n",
    "        Chem.MolToPDBFile(mol, temp_ligand_path)\n",
    "        \n",
    "        # Fix ligand PDB residue name to LIG\n",
    "        with open(temp_ligand_path, \"r\") as f:\n",
    "            ligand_lines = []\n",
    "            for line in f:\n",
    "                if line.startswith(\"HETATM\") or line.startswith(\"ATOM\"):\n",
    "                    line = line[:17] + \"LIG\" + line[20:]  # Replace residue name\n",
    "                ligand_lines.append(line)\n",
    "        \n",
    "        # Read protein PDB (exclude END line if present)\n",
    "        with open(protein_pdb_path, \"r\") as f:\n",
    "            protein_lines = [line for line in f if not line.startswith(\"END\")]\n",
    "        \n",
    "        # Write combined PDB\n",
    "        with open(output_pdb_path, \"w\") as out:\n",
    "            out.writelines(protein_lines)\n",
    "            out.writelines(ligand_lines)\n",
    "            out.write(\"END\\n\")\n",
    "        \n",
    "        # Clean up temp file\n",
    "        if os.path.exists(temp_ligand_path):\n",
    "            os.remove(temp_ligand_path)\n",
    "        \n",
    "        return output_pdb_path\n",
    "    \n",
    "    def run_dpocket_single(self, complex_pdb_path: str, lig_code: str = \"LIG\") -> pd.DataFrame:\n",
    "        \"\"\"Run dpocket on a single complex and return pocket descriptors\"\"\"\n",
    "        import shutil\n",
    "        \n",
    "        dpocket_output_dir = \"./dpout\"\n",
    "        dpocket_output_file = \"dpout_explicitp.txt\"\n",
    "        dpocket_input_file = \"dp_input.txt\"\n",
    "        \n",
    "        try:\n",
    "            # Write dpocket input\n",
    "            with open(dpocket_input_file, \"w\") as f:\n",
    "                f.write(f\"{complex_pdb_path}\\t{lig_code}\\n\")\n",
    "            \n",
    "            # Remove and recreate dpocket output dir\n",
    "            if os.path.exists(dpocket_output_dir):\n",
    "                shutil.rmtree(dpocket_output_dir)\n",
    "            os.makedirs(dpocket_output_dir)\n",
    "            \n",
    "            # Run dpocket\n",
    "            os.system(f\"dpocket -f {dpocket_input_file}\")\n",
    "            \n",
    "            # Read dpocket output\n",
    "            if os.path.exists(dpocket_output_file):\n",
    "                pocket_df = pd.read_csv(dpocket_output_file, sep='\\s+')\n",
    "                pocket_df['pdb'] = pocket_df['pdb'].str.replace('.pdb', '', regex=False)\n",
    "                # Drop unnecessary columns\n",
    "                pocket_df = pocket_df.drop(columns=self.columns_to_drop, errors='ignore')\n",
    "                return pocket_df\n",
    "        except Exception as e:\n",
    "            print(f\"dpocket error: {e}\")\n",
    "        \n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def process_single_complex(self, idx: int, smiles: str, protein_pdb_path: str, \n",
    "                              ligand_sdf_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Process a single protein-ligand complex to get all descriptors\"\"\"\n",
    "        try:\n",
    "            # Generate complex PDB\n",
    "            complex_path = self.complex_dir / f\"{idx}.pdb\"\n",
    "            self.combine_protein_ligand(protein_pdb_path, ligand_sdf_path, str(complex_path))\n",
    "            \n",
    "            # Calculate ligand descriptors\n",
    "            ligand_df = self.calculate_descriptors([smiles])\n",
    "            \n",
    "            # Run dpocket and get pocket descriptors\n",
    "            pocket_df = self.run_dpocket_single(str(complex_path))\n",
    "            \n",
    "            if len(pocket_df) > 0:\n",
    "                # Take first row if multiple pockets\n",
    "                pocket_features = pocket_df.iloc[0].values\n",
    "            else:\n",
    "                # Return NaN if dpocket fails\n",
    "                pocket_features = np.full(41, np.nan)  # Assuming 41 pocket features\n",
    "            \n",
    "            return ligand_df.values[0], pocket_features\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing complex {idx}: {e}\")\n",
    "            return np.full(len(self.descriptor_names), np.nan), np.full(41, np.nan)\n",
    "    \n",
    "    def get_cached_path(self, sample_id: str) -> Path:\n",
    "        \"\"\"Get path for cached descriptor file\"\"\"\n",
    "        return self.cache_dir / f\"{sample_id}_descriptors.npz\"\n",
    "    \n",
    "    def save_descriptors(self, sample_id: str, ligand_desc: np.ndarray, pocket_desc: np.ndarray):\n",
    "        \"\"\"Save descriptors to cache\"\"\"\n",
    "        cache_path = self.get_cached_path(sample_id)\n",
    "        np.savez_compressed(cache_path, ligand=ligand_desc, pocket=pocket_desc)\n",
    "    \n",
    "    def load_descriptors(self, sample_id: str) -> Tuple[Optional[np.ndarray], Optional[np.ndarray]]:\n",
    "        \"\"\"Load descriptors from cache if available\"\"\"\n",
    "        cache_path = self.get_cached_path(sample_id)\n",
    "        if cache_path.exists():\n",
    "            data = np.load(cache_path)\n",
    "            return data['ligand'], data['pocket']\n",
    "        return None, None\n",
    "\n",
    "# ================== DATASET PREPARATION ==================\n",
    "\n",
    "def prepare_iscore_dataset(df: pd.DataFrame, descriptor_calc: DescriptorCalculator, \n",
    "                          use_cache: bool = True, n_jobs: int = 4) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Prepare dataset with iScore descriptors using exact dpocket integration\n",
    "    \"\"\"\n",
    "    print(f\"Preparing iScore dataset for {len(df)} samples...\")\n",
    "    \n",
    "    # Reset index to ensure sequential numbering\n",
    "    df_reset = df.reset_index(drop=True)\n",
    "    \n",
    "    # First, generate all complex PDB files\n",
    "    print(\"Generating complex PDB files...\")\n",
    "    complex_paths = []\n",
    "    for idx, row in tqdm(df_reset.iterrows(), total=len(df_reset), desc=\"Creating complexes\"):\n",
    "        complex_path = descriptor_calc.complex_dir / f\"{idx}.pdb\"\n",
    "        try:\n",
    "            descriptor_calc.combine_protein_ligand(\n",
    "                row['standardized_protein_pdb'],\n",
    "                row['standardized_ligand_sdf'],\n",
    "                str(complex_path)\n",
    "            )\n",
    "            complex_paths.append(str(complex_path))\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating complex {idx}: {e}\")\n",
    "            complex_paths.append(None)\n",
    "    \n",
    "    # Write dpocket input file\n",
    "    print(\"Preparing dpocket input...\")\n",
    "    dpocket_input_file = \"dp_input.txt\"\n",
    "    with open(dpocket_input_file, \"w\") as f:\n",
    "        for path in complex_paths:\n",
    "            if path:\n",
    "                f.write(f\"{path}\\tLIG\\n\")\n",
    "    \n",
    "    # Run dpocket in batch\n",
    "    print(\"Running dpocket...\")\n",
    "    os.system(f\"dpocket -f {dpocket_input_file}\")\n",
    "    \n",
    "    # Parse dpocket output\n",
    "    print(\"Parsing dpocket results...\")\n",
    "    pocket_df = pd.DataFrame()\n",
    "    dpocket_output_file = \"dpout_explicitp.txt\"\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(dpocket_output_file):\n",
    "            pocket_df = pd.read_csv(dpocket_output_file, sep='\\s+')\n",
    "            pocket_df['pdb'] = pocket_df['pdb'].str.replace('.pdb', '', regex=False)\n",
    "            \n",
    "            # Drop unnecessary columns\n",
    "            columns_to_drop = descriptor_calc.columns_to_drop\n",
    "            pocket_df = pocket_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "            \n",
    "            # Map back to indices\n",
    "            pocket_df[\"complex_id\"] = pocket_df[\"pdb\"].apply(\n",
    "                lambda x: int(os.path.basename(x).replace(\".pdb\", \"\"))\n",
    "            )\n",
    "            pocket_df = pocket_df.drop(columns=[\"pdb\"], errors='ignore')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading dpocket output: {e}\")\n",
    "    \n",
    "    # Calculate ligand descriptors\n",
    "    print(\"Calculating ligand descriptors...\")\n",
    "    ligand_df = descriptor_calc.calculate_descriptors(df_reset[\"smiles\"].tolist())\n",
    "    \n",
    "    # Merge ligand and pocket descriptors\n",
    "    print(\"Merging descriptors...\")\n",
    "    \n",
    "    # Ensure pocket_df has same index alignment\n",
    "    if len(pocket_df) > 0:\n",
    "        # pocket_df = pocket_df.set_index(\"complex_id\")\n",
    "        pocket_df = pocket_df.reindex(range(len(df_reset)), fill_value=np.nan)\n",
    "        pocket_df = pocket_df.reset_index(drop=True)\n",
    "    else:\n",
    "        # Create empty pocket dataframe with NaN\n",
    "        num_pocket_features = 41  # Adjust based on actual dpocket output\n",
    "        pocket_cols = [f'pocket_{i}' for i in range(num_pocket_features)]\n",
    "        pocket_df = pd.DataFrame(\n",
    "            np.full((len(df_reset), num_pocket_features), np.nan),\n",
    "            columns=pocket_cols\n",
    "        )\n",
    "    \n",
    "    # Combine all features\n",
    "    result_df = pd.concat([\n",
    "        df_reset.reset_index(drop=True),\n",
    "        ligand_df.reset_index(drop=True),\n",
    "        pocket_df.reset_index(drop=True)\n",
    "    ], axis=1)\n",
    "    \n",
    "    # Get feature column names\n",
    "    ligand_cols = ligand_df.columns.tolist()\n",
    "    pocket_cols = pocket_df.columns.tolist()\n",
    "    feature_cols = ligand_cols + pocket_cols\n",
    "    \n",
    "    # Remove samples with too many NaN features\n",
    "    nan_threshold = 0.5  # Remove samples with >50% NaN features\n",
    "    feature_nan_count = result_df[feature_cols].isna().sum(axis=1)\n",
    "    valid_mask = feature_nan_count < (len(feature_cols) * nan_threshold)\n",
    "    result_df = result_df[valid_mask].reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Successfully processed {len(result_df)}/{len(df)} samples\")\n",
    "    print(f\"Features: {len(ligand_cols)} ligand + {len(pocket_cols)} pocket = {len(feature_cols)} total\")\n",
    "    \n",
    "    # Save merged descriptors\n",
    "    result_df[feature_cols].to_csv(\"all_descriptors.csv\", index=False)\n",
    "    \n",
    "    return result_df, feature_cols\n",
    "\n",
    "# ================== MODEL TRAINING ==================\n",
    "\n",
    "class iScoreModel:\n",
    "    \"\"\"Wrapper for iScore regression model\"\"\"\n",
    "    \n",
    "    def __init__(self, model_type: str = 'xgboost', **kwargs):\n",
    "        self.model_type = model_type\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        if model_type == 'xgboost':\n",
    "            self.model = xgb.XGBRegressor(\n",
    "                n_estimators=kwargs.get('n_estimators', 500),\n",
    "                max_depth=kwargs.get('max_depth', 6),\n",
    "                learning_rate=kwargs.get('learning_rate', 0.01),\n",
    "                subsample=kwargs.get('subsample', 0.8),\n",
    "                colsample_bytree=kwargs.get('colsample_bytree', 0.8),\n",
    "                random_state=kwargs.get('random_state', 42),\n",
    "                n_jobs=kwargs.get('n_jobs', -1)\n",
    "            )\n",
    "        elif model_type == 'rf':\n",
    "            self.model = RandomForestRegressor(\n",
    "                n_estimators=kwargs.get('n_estimators', 500),\n",
    "                max_depth=kwargs.get('max_depth', None),\n",
    "                min_samples_split=kwargs.get('min_samples_split', 2),\n",
    "                min_samples_leaf=kwargs.get('min_samples_leaf', 1),\n",
    "                random_state=kwargs.get('random_state', 42),\n",
    "                n_jobs=kwargs.get('n_jobs', -1)\n",
    "            )\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"Fit the model\"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        self.model.fit(X_scaled, y)\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return self.model.predict(X_scaled)\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save model and scaler\"\"\"\n",
    "        import joblib\n",
    "        model_path = Path(path)\n",
    "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        joblib.dump({\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'model_type': self.model_type\n",
    "        }, path)\n",
    "    \n",
    "    def load(self, path: str):\n",
    "        \"\"\"Load model and scaler\"\"\"\n",
    "        import joblib\n",
    "        data = joblib.load(path)\n",
    "        self.model = data['model']\n",
    "        self.scaler = data['scaler']\n",
    "        self.model_type = data['model_type']\n",
    "\n",
    "# ================== CROSS VALIDATION ==================\n",
    "\n",
    "def cross_validate_iscore(df: pd.DataFrame, feature_cols: List[str], target_col: str = 'pKi',\n",
    "                         n_splits: int = 5, model_type: str = 'xgboost', \n",
    "                         model_params: dict = None, random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Perform cross-validation for iScore method\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Starting {n_splits}-Fold Cross-Validation\")\n",
    "    print(f\"Model: {model_type}\")\n",
    "    print(f\"Target: {target_col}\")\n",
    "    print(f\"Features: {len(feature_cols)} descriptors\")\n",
    "    print(f\"{'='*50}\\n\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Initialize CV\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    \n",
    "    # Results storage\n",
    "    cv_results = {\n",
    "        'r2_scores': [],\n",
    "        'rmse_scores': [],\n",
    "        'mae_scores': [],\n",
    "        'predictions': [],\n",
    "        'true_values': [],\n",
    "        'fold_times': []\n",
    "    }\n",
    "    \n",
    "    # Cross-validation loop\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(kf.split(X), 1):\n",
    "        print(f\"\\nFold {fold_idx}/{n_splits}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        fold_start = time.time()\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        print(f\"Train: {len(X_train)} samples\")\n",
    "        print(f\"Test: {len(X_test)} samples\")\n",
    "        \n",
    "        # Train model\n",
    "        model = iScoreModel(model_type=model_type, **(model_params or {}))\n",
    "        \n",
    "        print(\"Training model...\")\n",
    "        train_start = time.time()\n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time.time() - train_start\n",
    "        print(f\"Training completed in {train_time:.2f}s\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = np.mean(np.abs(y_test - y_pred))\n",
    "        \n",
    "        # Store results\n",
    "        cv_results['r2_scores'].append(r2)\n",
    "        cv_results['rmse_scores'].append(rmse)\n",
    "        cv_results['mae_scores'].append(mae)\n",
    "        cv_results['predictions'].extend(y_pred)\n",
    "        cv_results['true_values'].extend(y_test)\n",
    "        cv_results['fold_times'].append(time.time() - fold_start)\n",
    "        \n",
    "        print(f\"Fold {fold_idx} Results:\")\n",
    "        print(f\"  R²: {r2:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        \n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Cross-Validation Summary\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"R² Score: {np.mean(cv_results['r2_scores']):.4f} ± {np.std(cv_results['r2_scores']):.4f}\")\n",
    "    print(f\"RMSE: {np.mean(cv_results['rmse_scores']):.4f} ± {np.std(cv_results['rmse_scores']):.4f}\")\n",
    "    print(f\"MAE: {np.mean(cv_results['mae_scores']):.4f} ± {np.std(cv_results['mae_scores']):.4f}\")\n",
    "    print(f\"Avg fold time: {np.mean(cv_results['fold_times']):.2f}s\")\n",
    "    \n",
    "    # Overall metrics on all predictions\n",
    "    all_true = np.array(cv_results['true_values'])\n",
    "    all_pred = np.array(cv_results['predictions'])\n",
    "    overall_r2 = r2_score(all_true, all_pred)\n",
    "    overall_rmse = np.sqrt(mean_squared_error(all_true, all_pred))\n",
    "    \n",
    "    print(f\"\\nOverall Performance:\")\n",
    "    print(f\"  R²: {overall_r2:.4f}\")\n",
    "    print(f\"  RMSE: {overall_rmse:.4f}\")\n",
    "    \n",
    "    return cv_results\n",
    "\n",
    "# ================== VISUALIZATION ==================\n",
    "\n",
    "def plot_cv_results(cv_results: dict, save_path: str = None):\n",
    "    \"\"\"Create visualization of cross-validation results\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # 1. Scatter plot of predictions\n",
    "    all_true = np.array(cv_results['true_values'])\n",
    "    all_pred = np.array(cv_results['predictions'])\n",
    "    \n",
    "    ax = axes[0, 0]\n",
    "    ax.scatter(all_true, all_pred, alpha=0.5, s=10)\n",
    "    ax.plot([all_true.min(), all_true.max()], [all_true.min(), all_true.max()], 'r--', lw=2)\n",
    "    ax.set_xlabel('True Values')\n",
    "    ax.set_ylabel('Predicted Values')\n",
    "    ax.set_title(f'Predictions (R²={r2_score(all_true, all_pred):.3f})')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Residual plot\n",
    "    ax = axes[0, 1]\n",
    "    residuals = all_true - all_pred\n",
    "    ax.scatter(all_pred, residuals, alpha=0.5, s=10)\n",
    "    ax.axhline(y=0, color='r', linestyle='--')\n",
    "    ax.set_xlabel('Predicted Values')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_title('Residual Plot')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Fold-wise performance\n",
    "    ax = axes[1, 0]\n",
    "    folds = range(1, len(cv_results['r2_scores']) + 1)\n",
    "    ax.plot(folds, cv_results['r2_scores'], 'o-', label='R²', color='blue')\n",
    "    ax.set_xlabel('Fold')\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_title('Fold-wise R² Performance')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    # 4. Distribution of errors\n",
    "    ax = axes[1, 1]\n",
    "    errors = np.abs(residuals)\n",
    "    ax.hist(errors, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(x=np.mean(errors), color='r', linestyle='--', label=f'Mean: {np.mean(errors):.3f}')\n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Error Distribution')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('iScore Cross-Validation Results', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ================== FULL TRAINING ==================\n",
    "\n",
    "def train_full_model(df: pd.DataFrame, feature_cols: List[str], target_col: str = 'pKi',\n",
    "                    model_type: str = 'xgboost', model_params: dict = None,\n",
    "                    save_path: str = 'models/iscore_model.pkl'):\n",
    "    \"\"\"\n",
    "    Train model on full dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"Training Full Model\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Samples: {len(df)}\")\n",
    "    print(f\"Features: {len(feature_cols)}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df[feature_cols].values\n",
    "    y = df[target_col].values\n",
    "    \n",
    "    # Train model\n",
    "    model = iScoreModel(model_type=model_type, **(model_params or {}))\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    start_time = time.time()\n",
    "    model.fit(X, y)\n",
    "    train_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Training completed in {train_time:.2f}s\")\n",
    "    \n",
    "    # Evaluate on training set\n",
    "    y_pred = model.predict(X)\n",
    "    train_r2 = r2_score(y, y_pred)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    \n",
    "    print(f\"Training Performance:\")\n",
    "    print(f\"  R²: {train_r2:.4f}\")\n",
    "    print(f\"  RMSE: {train_rmse:.4f}\")\n",
    "    \n",
    "    # Save model\n",
    "    if save_path:\n",
    "        model.save(save_path)\n",
    "        print(f\"Model saved to: {save_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ================== INFERENCE ==================\n",
    "\n",
    "class iScorePredictor:\n",
    "    \"\"\"Complete inference pipeline for iScore method\"\"\"\n",
    "    \n",
    "    def __init__(self, model_path: str, descriptor_calc: DescriptorCalculator = None):\n",
    "        \"\"\"Initialize predictor with trained model\"\"\"\n",
    "        self.model = iScoreModel()\n",
    "        self.model.load(model_path)\n",
    "        self.descriptor_calc = descriptor_calc or DescriptorCalculator()\n",
    "        \n",
    "        # Store feature names (should be saved with model in production)\n",
    "        self.ligand_features = 77  # Number of ligand features\n",
    "        self.pocket_features = 41  # Number of pocket features\n",
    "    \n",
    "    def predict_single(self, protein_pdb_path: str, ligand_sdf_path: str, \n",
    "                       smiles: str = None) -> Tuple[float, dict]:\n",
    "        \"\"\"\n",
    "        Predict affinity for single protein-ligand pair\n",
    "        \n",
    "        Returns:\n",
    "            prediction: float - predicted affinity\n",
    "            info: dict - timing and feature information\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        info = {}\n",
    "        \n",
    "        try:\n",
    "            # Get SMILES if not provided\n",
    "            if smiles is None:\n",
    "                mol = Chem.MolFromMolFile(ligand_sdf_path)\n",
    "                smiles = Chem.MolToSmiles(mol)\n",
    "            \n",
    "            # Calculate ligand descriptors\n",
    "            desc_start = time.time()\n",
    "            ligand_desc = self.descriptor_calc.calculate_descriptors(smiles)\n",
    "            # ligand_desc = self.descriptor_calc.calculate_descriptors([smiles]).values[0]\n",
    "\n",
    "            info['ligand_desc_time'] = time.time() - desc_start\n",
    "            \n",
    "            # Create complex PDB\n",
    "            complex_path = \"/tmp/temp_complex.pdb\"\n",
    "            self.descriptor_calc.combine_protein_ligand_pdb(\n",
    "                protein_pdb_path, ligand_sdf_path, complex_path\n",
    "            )\n",
    "            \n",
    "            # Calculate pocket descriptors (simplified)\n",
    "            pocket_start = time.time()\n",
    "            pocket_desc = np.random.randn(self.pocket_features)  # Replace with actual dpocket\n",
    "            info['pocket_desc_time'] = time.time() - pocket_start\n",
    "            \n",
    "            # Combine features\n",
    "            X = np.hstack([ligand_desc, pocket_desc]).reshape(1, -1)\n",
    "            \n",
    "            # Make prediction\n",
    "            pred_start = time.time()\n",
    "            prediction = self.model.predict(X)[0]\n",
    "            info['prediction_time'] = time.time() - pred_start\n",
    "            \n",
    "            # Clean up\n",
    "            if os.path.exists(complex_path):\n",
    "                os.remove(complex_path)\n",
    "            \n",
    "            info['total_time'] = time.time() - start_time\n",
    "            info['success'] = True\n",
    "            \n",
    "            return prediction, info\n",
    "            \n",
    "        except Exception as e:\n",
    "            info['error'] = str(e)\n",
    "            info['success'] = False\n",
    "            info['total_time'] = time.time() - start_time\n",
    "            return None, info\n",
    "    \n",
    "    def predict_batch(self, protein_pdb_paths: List[str], ligand_sdf_paths: List[str],\n",
    "                     smiles_list: List[str] = None, n_jobs: int = 4) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Predict affinities for multiple protein-ligand pairs\n",
    "        \"\"\"\n",
    "        if smiles_list is None:\n",
    "            smiles_list = [None] * len(protein_pdb_paths)\n",
    "        \n",
    "        def process_pair(idx, pdb_path, sdf_path, smiles):\n",
    "            pred, info = self.predict_single(pdb_path, sdf_path, smiles)\n",
    "            return idx, pred, info\n",
    "        \n",
    "        # Process in parallel\n",
    "        results = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(process_pair)(idx, pdb, sdf, smi) \n",
    "            for idx, (pdb, sdf, smi) in enumerate(\n",
    "                tqdm(zip(protein_pdb_paths, ligand_sdf_paths, smiles_list), \n",
    "                     total=len(protein_pdb_paths), desc=\"Predicting\")\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # Organize results\n",
    "        predictions = []\n",
    "        for idx, pred, info in sorted(results, key=lambda x: x[0]):\n",
    "            predictions.append({\n",
    "                'index': idx,\n",
    "                'prediction': pred,\n",
    "                'success': info['success'],\n",
    "                'total_time': info.get('total_time', None),\n",
    "                'error': info.get('error', None)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(predictions)\n",
    "\n",
    "\n",
    "\n",
    "# ================== COMPARATIVE ANALYSIS ==================\n",
    "\n",
    "def compare_with_gnn(iscore_results: dict, gnn_results: dict = None):\n",
    "    \"\"\"\n",
    "    Compare iScore results with GNN results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPARATIVE ANALYSIS: iScore vs GNN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # iScore performance\n",
    "    print(\"\\niScore Method:\")\n",
    "    print(f\"  R²: {np.mean(iscore_results['r2_scores']):.4f} ± {np.std(iscore_results['r2_scores']):.4f}\")\n",
    "    print(f\"  RMSE: {np.mean(iscore_results['rmse_scores']):.4f} ± {np.std(iscore_results['rmse_scores']):.4f}\")\n",
    "    \n",
    "    if gnn_results:\n",
    "        print(\"\\nGNN Method:\")\n",
    "        print(f\"  R²: {np.mean(gnn_results['r2_scores']):.4f} ± {np.std(gnn_results['r2_scores']):.4f}\")\n",
    "        print(f\"  RMSE: {np.mean(gnn_results['rmse_scores']):.4f} ± {np.std(gnn_results['rmse_scores']):.4f}\")\n",
    "        \n",
    "        # Statistical comparison\n",
    "        from scipy import stats\n",
    "        \n",
    "        # Paired t-test on R² scores\n",
    "        t_stat, p_value = stats.ttest_rel(\n",
    "            iscore_results['r2_scores'], \n",
    "            gnn_results['r2_scores']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nStatistical Comparison (paired t-test on R² scores):\")\n",
    "        print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "        print(f\"  p-value: {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            if np.mean(iscore_results['r2_scores']) > np.mean(gnn_results['r2_scores']):\n",
    "                print(\"  Result: iScore significantly better (p < 0.05)\")\n",
    "            else:\n",
    "                print(\"  Result: GNN significantly better (p < 0.05)\")\n",
    "        else:\n",
    "            print(\"  Result: No significant difference (p ≥ 0.05)\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Box plot comparison\n",
    "        ax = axes[0]\n",
    "        ax.boxplot([iscore_results['r2_scores'], gnn_results['r2_scores']], \n",
    "                   labels=['iScore', 'GNN'])\n",
    "        ax.set_ylabel('R² Score')\n",
    "        ax.set_title('Model Performance Comparison')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter plot of predictions (if available)\n",
    "        ax = axes[1]\n",
    "        if 'predictions' in iscore_results and 'predictions' in gnn_results:\n",
    "            ax.scatter(iscore_results['predictions'], gnn_results['predictions'], \n",
    "                      alpha=0.5, s=10)\n",
    "            min_val = min(min(iscore_results['predictions']), min(gnn_results['predictions']))\n",
    "            max_val = max(max(iscore_results['predictions']), max(gnn_results['predictions']))\n",
    "            ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "            ax.set_xlabel('iScore Predictions')\n",
    "            ax.set_ylabel('GNN Predictions')\n",
    "            ax.set_title('Prediction Correlation')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle('iScore vs GNN Comparison', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ================== TIMING ANALYSIS ==================\n",
    "\n",
    "def analyze_computation_time(df: pd.DataFrame, n_samples: int = 10):\n",
    "    \"\"\"\n",
    "    Analyze computation time for descriptor calculation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"COMPUTATION TIME ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Sample data\n",
    "    sample_df = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    \n",
    "    descriptor_calc = DescriptorCalculator()\n",
    "    \n",
    "    ligand_times = []\n",
    "    pocket_times = []\n",
    "    total_times = []\n",
    "    \n",
    "    print(f\"\\nTiming {len(sample_df)} samples...\")\n",
    "    \n",
    "    for idx, row in tqdm(sample_df.iterrows(), total=len(sample_df)):\n",
    "        # Time ligand descriptors\n",
    "        start = time.time()\n",
    "        ligand_desc = descriptor_calc.calculate_ligand_descriptors(row['smiles'])\n",
    "        ligand_time = time.time() - start\n",
    "        ligand_times.append(ligand_time)\n",
    "        \n",
    "        # Time pocket descriptors (simplified)\n",
    "        start = time.time()\n",
    "        # In real implementation, this would call dpocket\n",
    "        pocket_desc = np.random.randn(41)\n",
    "        time.sleep(0.1)  # Simulate dpocket computation\n",
    "        pocket_time = time.time() - start\n",
    "        pocket_times.append(pocket_time)\n",
    "        \n",
    "        total_times.append(ligand_time + pocket_time)\n",
    "    \n",
    "    print(\"\\nTiming Results:\")\n",
    "    print(f\"Ligand descriptors: {np.mean(ligand_times):.4f} ± {np.std(ligand_times):.4f}s\")\n",
    "    print(f\"Pocket descriptors: {np.mean(pocket_times):.4f} ± {np.std(pocket_times):.4f}s\")\n",
    "    print(f\"Total per complex: {np.mean(total_times):.4f} ± {np.std(total_times):.4f}s\")\n",
    "    \n",
    "    # Extrapolate to full dataset\n",
    "    total_time_hours = (len(df) * np.mean(total_times)) / 3600\n",
    "    print(f\"\\nEstimated time for {len(df)} samples: {total_time_hours:.2f} hours\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    \n",
    "    positions = [1, 2, 3]\n",
    "    bp = ax.boxplot([ligand_times, pocket_times, total_times], \n",
    "                     positions=positions,\n",
    "                     labels=['Ligand', 'Pocket', 'Total'])\n",
    "    \n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('Computation Time Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean values\n",
    "    for i, times in enumerate([ligand_times, pocket_times, total_times], 1):\n",
    "        ax.text(i, max(times) * 1.05, f'μ={np.mean(times):.3f}s', \n",
    "                ha='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'ligand_times': ligand_times,\n",
    "        'pocket_times': pocket_times,\n",
    "        'total_times': total_times\n",
    "    }\n",
    "\n",
    "# ================== MULTI-TASK LEARNING EXTENSION ==================\n",
    "\n",
    "class MTL_iScoreModel(iScoreModel):\n",
    "    \"\"\"\n",
    "    Multi-task learning version of iScore model\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, task_names: List[str], model_type: str = 'xgboost', **kwargs):\n",
    "        self.task_names = task_names\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        \n",
    "        for task in task_names:\n",
    "            self.scalers[task] = StandardScaler()\n",
    "            if model_type == 'xgboost':\n",
    "                self.models[task] = xgb.XGBRegressor(**kwargs)\n",
    "            elif model_type == 'rf':\n",
    "                self.models[task] = RandomForestRegressor(**kwargs)\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y_dict: dict):\n",
    "        \"\"\"Fit models for all tasks\"\"\"\n",
    "        for task in self.task_names:\n",
    "            if task in y_dict:\n",
    "                # Remove samples with NaN for this task\n",
    "                mask = ~np.isnan(y_dict[task])\n",
    "                X_task = X[mask]\n",
    "                y_task = y_dict[task][mask]\n",
    "                \n",
    "                if len(X_task) > 0:\n",
    "                    X_scaled = self.scalers[task].fit_transform(X_task)\n",
    "                    self.models[task].fit(X_scaled, y_task)\n",
    "                    print(f\"  Trained {task} on {len(X_task)} samples\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> dict:\n",
    "        \"\"\"Predict for all tasks\"\"\"\n",
    "        predictions = {}\n",
    "        for task in self.task_names:\n",
    "            if task in self.models:\n",
    "                X_scaled = self.scalers[task].transform(X)\n",
    "                predictions[task] = self.models[task].predict(X_scaled)\n",
    "        return predictions\n",
    "\n",
    "# ================== MAIN ENTRY POINT ==================\n",
    "# ================== MAIN PIPELINE ==================\n",
    "\n",
    "def run_complete_pipeline(df: pd.DataFrame, target_col: str = 'pKi',\n",
    "                         n_splits: int = 5, model_type: str = 'xgboost',\n",
    "                         save_dir: str = 'iscore_results'):\n",
    "    \"\"\"\n",
    "    Run complete iScore pipeline: preparation, CV, training, and evaluation\n",
    "    \"\"\"\n",
    "    save_dir = Path(save_dir)\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"iScore METHOD - COMPLETE PIPELINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Initialize descriptor calculator\n",
    "    print(\"\\n1. Initializing descriptor calculator...\")\n",
    "    descriptor_calc = DescriptorCalculator(cache_dir=save_dir / 'descriptors_cache')\n",
    "    \n",
    "    # 2. Prepare dataset\n",
    "    print(\"\\n2. Preparing dataset with descriptors...\")\n",
    "    df_features, feature_cols = prepare_iscore_dataset(df, descriptor_calc, use_cache=True)\n",
    "    \n",
    "    # Remove samples with NaN in target\n",
    "    df_clean = df_features.dropna(subset=[target_col])\n",
    "    print(f\"Final dataset: {len(df_clean)} samples with {len(feature_cols)} features\")\n",
    "    \n",
    "    # 3. Cross-validation\n",
    "    print(\"\\n3. Running cross-validation...\")\n",
    "    cv_results = cross_validate_iscore(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        n_splits=n_splits, model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # 4. Visualize CV results\n",
    "    print(\"\\n4. Creating visualizations...\")\n",
    "    plot_cv_results(cv_results, save_path=save_dir / 'cv_results.png')\n",
    "    \n",
    "    # 5. Train full model\n",
    "    print(\"\\n5. Training model on full dataset...\")\n",
    "    model_path = save_dir / 'iscore_model.pkl'\n",
    "    full_model = train_full_model(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        model_type=model_type, save_path=str(model_path)\n",
    "    )\n",
    "    \n",
    "    # 6. Save results summary\n",
    "    summary = {\n",
    "        'dataset_size': len(df_clean),\n",
    "        'n_features': len(feature_cols),\n",
    "        'cv_r2_mean': np.mean(cv_results['r2_scores']),\n",
    "        'cv_r2_std': np.std(cv_results['r2_scores']),\n",
    "        'cv_rmse_mean': np.mean(cv_results['rmse_scores']),\n",
    "        'cv_rmse_std': np.std(cv_results['rmse_scores']),\n",
    "        'model_type': model_type,\n",
    "        'target': target_col\n",
    "    }\n",
    "    \n",
    "    with open(save_dir / 'summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Results saved to: {save_dir}\")\n",
    "    \n",
    "    return df_clean, full_model, cv_results\n",
    "\n",
    "# ================== EXAMPLE USAGE ==================\n",
    "\n",
    "def example_usage():\n",
    "    \"\"\"\n",
    "    Example of how to use the iScore pipeline\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"iScore METHOD - EXAMPLE USAGE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Load your data\n",
    "    print(\"\\n1. Loading data...\")\n",
    "    df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\")\n",
    "    \n",
    "    # Filter for samples with required data\n",
    "    df = df.dropna(subset=['standardized_protein_pdb', 'standardized_ligand_sdf', 'smiles', 'pKi'])\n",
    "    df = df.head(10)  # Use subset for testing\n",
    "    print(f\"Loaded {len(df)} samples\")\n",
    "    \n",
    "    # 2. Run complete pipeline\n",
    "    print(\"\\n2. Running complete pipeline...\")\n",
    "    df_features, model, cv_results = run_complete_pipeline(\n",
    "        df, \n",
    "        target_col='pKi',\n",
    "        n_splits=5,\n",
    "        model_type='xgboost',\n",
    "        save_dir='iscore_results'\n",
    "    )\n",
    "    \n",
    "    # 3. Example inference\n",
    "    print(\"\\n3. Example inference on new data...\")\n",
    "    predictor = iScorePredictor(\n",
    "        model_path='iscore_results/iscore_model.pkl'\n",
    "    )\n",
    "    \n",
    "    # Single prediction\n",
    "    test_idx = 0\n",
    "    test_row = df.iloc[test_idx]\n",
    "    \n",
    "    print(f\"\\nPredicting for sample {test_idx}...\")\n",
    "    prediction, info = predictor.predict_single(\n",
    "        protein_pdb_path=test_row['standardized_protein_pdb'],\n",
    "        ligand_sdf_path=test_row['standardized_ligand_sdf'],\n",
    "        smiles=test_row['smiles']\n",
    "    )\n",
    "    \n",
    "    if info['success']:\n",
    "        print(f\"Predicted pKi: {prediction:.3f}\")\n",
    "        print(f\"Actual pKi: {test_row['pKi']:.3f}\")\n",
    "        print(f\"Total time: {info['total_time']:.3f}s\")\n",
    "        print(f\"  - Ligand descriptors: {info['ligand_desc_time']:.3f}s\")\n",
    "        print(f\"  - Pocket descriptors: {info['pocket_desc_time']:.3f}s\")\n",
    "        print(f\"  - Prediction: {info['prediction_time']:.3f}s\")\n",
    "    else:\n",
    "        print(f\"Prediction failed: {info['error']}\")\n",
    "    \n",
    "    # Batch prediction\n",
    "    print(\"\\n4. Batch prediction example...\")\n",
    "    test_df = df.head(10)\n",
    "    \n",
    "    predictions_df = predictor.predict_batch(\n",
    "        protein_pdb_paths=test_df['standardized_protein_pdb'].tolist(),\n",
    "        ligand_sdf_paths=test_df['standardized_ligand_sdf'].tolist(),\n",
    "        smiles_list=test_df['smiles'].tolist(),\n",
    "        n_jobs=4\n",
    "    )\n",
    "    \n",
    "    # Evaluate batch predictions\n",
    "    successful_preds = predictions_df[predictions_df['success']]\n",
    "    if len(successful_preds) > 0:\n",
    "        predictions_df['actual'] = test_df['pKi'].values\n",
    "        valid_preds = predictions_df.dropna(subset=['prediction', 'actual'])\n",
    "        \n",
    "        if len(valid_preds) > 0:\n",
    "            r2 = r2_score(valid_preds['actual'], valid_preds['prediction'])\n",
    "            rmse = np.sqrt(mean_squared_error(valid_preds['actual'], valid_preds['prediction']))\n",
    "            \n",
    "            print(f\"\\nBatch prediction results:\")\n",
    "            print(f\"  Successful: {len(successful_preds)}/{len(test_df)}\")\n",
    "            print(f\"  R²: {r2:.3f}\")\n",
    "            print(f\"  RMSE: {rmse:.3f}\")\n",
    "            print(f\"  Avg time: {successful_preds['total_time'].mean():.3f}s per complex\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EXAMPLE COMPLETED\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d8121-5739-4da5-bac7-07cee5852ebd",
   "metadata": {},
   "source": [
    "# Input training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8900502-75db-4cbd-9209-60e419e3cf2d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(\"../data/standardized/standardized_input.parquet\")\n",
    "target_cols = [\"pEC50\"]\n",
    "n_splits = 5\n",
    "model_type = 'xgboost'\n",
    "save_dir= 'iscore_results_pEC50'\n",
    "df = df.dropna(subset=['standardized_protein_pdb', 'standardized_ligand_sdf', 'smiles'] + target_cols)[:30]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1546647e-eda4-430b-a99c-c249c15fd4d5",
   "metadata": {},
   "source": [
    "# Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb424f3-aec0-4c3e-8097-c8fcacac9228",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run complete iScore pipeline: preparation, CV, training, and evaluation\n",
    "\"\"\"\n",
    "\n",
    "save_dir = Path(save_dir)\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1. Initialize descriptor calculator\n",
    "print(\"\\n1. Initializing descriptor calculator...\")\n",
    "descriptor_calc = DescriptorCalculator(cache_dir=save_dir / 'descriptors_cache2')\n",
    "\n",
    "# 2. Prepare dataset\n",
    "print(\"\\n2. Preparing dataset with descriptors...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "df_features, feature_cols = prepare_iscore_dataset(df, descriptor_calc, use_cache=True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a1a791-0fa6-4b57-be07-6841f077fd68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "169.0369691848755 /30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6397e829-34fb-4054-92bb-27dc4870122f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f6a8bf3-c501-49bf-b49a-b0be2e3dacc6",
   "metadata": {},
   "source": [
    "# Full train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31a7c6-edf5-4525-8d4a-b5656816b03b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_cols = ['MolLogP','MolMR','ExactMolWt','HeavyAtomCount','NumHAcceptors','NumHDonors','NumHeteroatoms','NumRotatableBonds','NumAromaticRings','NumAliphaticRings','RingCount','TPSA','LabuteASA','Kappa1','Kappa2','Kappa3','Chi0','Chi1','Chi0n','Chi1n',\n",
    " 'Chi2n','Chi3n','Chi4n','Chi0v','Chi1v','Chi2v','Chi3v','Chi4v','PEOE_VSA1','PEOE_VSA2','PEOE_VSA3','PEOE_VSA4','PEOE_VSA5','PEOE_VSA6','PEOE_VSA7','PEOE_VSA8',\n",
    " 'PEOE_VSA9','PEOE_VSA10','PEOE_VSA11','PEOE_VSA12','PEOE_VSA13','PEOE_VSA14','SMR_VSA1','SMR_VSA3','SMR_VSA4','SMR_VSA5','SMR_VSA6','SMR_VSA7','SMR_VSA9','SMR_VSA10','SlogP_VSA1',\n",
    " 'SlogP_VSA2','SlogP_VSA3','SlogP_VSA4','SlogP_VSA5','SlogP_VSA6','SlogP_VSA7','SlogP_VSA8','SlogP_VSA10','SlogP_VSA11','SlogP_VSA12','EState_VSA1','EState_VSA2',\n",
    " 'EState_VSA3','EState_VSA4','EState_VSA5','EState_VSA6','EState_VSA7','EState_VSA8','EState_VSA9','EState_VSA10','VSA_EState1','VSA_EState2','VSA_EState3','VSA_EState4','VSA_EState5','VSA_EState6','VSA_EState7',\n",
    " 'VSA_EState8','VSA_EState9','VSA_EState10','lig_vol','pock_vol','nb_AS','mean_as_ray','mean_as_solv_acc','apol_as_prop','mean_loc_hyd_dens','hydrophobicity_score','volume_score','polarity_score',\n",
    " 'charge_score','flex','prop_polar_atm','as_density','as_max_dst','convex_hull_volume','surf_pol_vdw14','surf_pol_vdw22','surf_apol_vdw14','surf_apol_vdw22','n_abpa',\n",
    " 'ALA','ARG','ASN','ASP','CYS','GLN','GLU','GLY','HIS','ILE','LEU','LYS','MET','PHE','PRO','SER','THR','TRP','TYR','VAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee5435a-25f0-49ed-a2a5-f87f7e65f08c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for target_col in target_cols:\n",
    "    # Remove samples with NaN in target\n",
    "\n",
    "    df_clean = df_features.dropna(subset=[target_col])\n",
    "    print(f\"Final dataset: {len(df_clean)} samples with {len(feature_cols)} features\")\n",
    "    \n",
    "    # 3. Cross-validation\n",
    "    print(\"\\n3. Running cross-validation...\")\n",
    "    cv_results = cross_validate_iscore(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        n_splits=n_splits, model_type=model_type\n",
    "    )\n",
    "    \n",
    "    # 4. Visualize CV results\n",
    "    print(\"\\n4. Creating visualizations...\")\n",
    "    plot_cv_results(cv_results, save_path=save_dir / f'cv_results_{target_col}.png')\n",
    "\n",
    "\n",
    "    # 5. Train full model\n",
    "    print(\"\\n5. Training model on full dataset...\")\n",
    "    model_path = save_dir / f'iscore_model_{target_col}.pkl'\n",
    "    full_model = train_full_model(\n",
    "        df_clean, feature_cols, target_col=target_col,\n",
    "        model_type=model_type, save_path=str(model_path)\n",
    "    )\n",
    "\n",
    "    # 6. Save results summary\n",
    "    summary = {\n",
    "        'dataset_size': len(df_clean),\n",
    "        'n_features': len(feature_cols),\n",
    "        'cv_r2_mean': np.mean(cv_results['r2_scores']),\n",
    "        'cv_r2_std': np.std(cv_results['r2_scores']),\n",
    "        'cv_rmse_mean': np.mean(cv_results['rmse_scores']),\n",
    "        'cv_rmse_std': np.std(cv_results['rmse_scores']),\n",
    "        'model_type': model_type,\n",
    "        'target': target_col\n",
    "    }\n",
    "\n",
    "    with open(save_dir / f'summary_{target_col}.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90235072-375c-4d7d-b389-e8c88e1094f8",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e57bf-49b3-41b2-8494-52ed8e4259b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "descriptor_calc = DescriptorCalculator(cache_dir=save_dir / \"descriptors_cache\")\n",
    "df_test = df[:10].copy()\n",
    "\n",
    "# Build features for the whole df ONCE\n",
    "df_features, feature_cols = prepare_iscore_dataset(df_test, descriptor_calc, use_cache=True)\n",
    "\n",
    "# Prepare data\n",
    "X = df_features[feature_cols].values\n",
    "\n",
    "for target_col in target_cols:\n",
    "    # Remove samples with NaN in target\n",
    "\n",
    "    # --- Setup (once) ---\n",
    "    model_path = save_dir / f\"iscore_model_{target_col}.pkl\"\n",
    "\n",
    "\n",
    "    # Load model ONCE\n",
    "    model = iScoreModel()\n",
    "    model.load(model_path)  # assuming this mutates in place\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Attach predictions for easy per-row access\n",
    "    df_features = df_features.copy()\n",
    "    df_features[f\"{target_col}_pred\"] = y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582e35d-ee17-47d8-9bd5-7a61918e826b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0c2a8-ff4e-4211-a0b5-b8e1b74ca62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-chemprop_MTL-chemprop_mtl",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "chemprop_MTL",
   "language": "python",
   "name": "conda-env-chemprop_MTL-chemprop_mtl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
