{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f6f3e3-5a1f-4ecb-bafe-1950d18fd8a4",
   "metadata": {},
   "source": [
    "# 1: Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9914ae96-c08f-4452-9e57-8cfe1aba0ab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu118\n",
      "CUDA available: True\n",
      "Number of GPUs: 16\n",
      "  GPU 0: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 1: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 2: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 3: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 4: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 5: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 6: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 7: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 8: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 9: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 10: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 11: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 12: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 13: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 14: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "  GPU 15: NVIDIA A100-SXM4-40GB\n",
      "    Memory: 42.3 GB\n",
      "\n",
      "✓ Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# CELL 1: IMPORTS AND SETUP\n",
    "# ==================================================================================\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "# This allows importing the gnn_dta_mtl package\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "# Set device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Import your package - use absolute import instead of relative\n",
    "from gnn_dta_mtl import (\n",
    "    MTL_DTAModel, DTAModel,\n",
    "    MTL_DTA, DTA,\n",
    "    CrossValidator, MTLTrainer,\n",
    "    StructureStandardizer, StructureProcessor, StructureChunkLoader,\n",
    "    ESMEmbedder,\n",
    "    add_molecular_properties_parallel,\n",
    "    compute_ligand_efficiency,\n",
    "    compute_mean_ligand_efficiency,\n",
    "    filter_by_properties,\n",
    "    prepare_mtl_experiment,\n",
    "    build_mtl_dataset, build_mtl_dataset_optimized,\n",
    "    evaluate_model,\n",
    "    plot_results, plot_predictions, create_summary_report,\n",
    "    ExperimentLogger,\n",
    "    save_model, save_results, create_output_dir\n",
    ")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(''))))\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Import your package\n",
    "from gnn_dta_mtl import (\n",
    "    MTL_DTAModel,\n",
    "    StructureChunkLoader,\n",
    "    prepare_mtl_experiment,\n",
    "    build_mtl_dataset_optimized,\n",
    "    save_model, save_results, create_output_dir\n",
    ")\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"\\n✓ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe88965-ecb2-4431-9d6a-f0257ae969f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Experiment: ddp_training_20250926_074933\n",
      "✓ Config saved to: ../output/experiments/ddp_training_20250926_074933/config.json\n",
      "✓ Using 16 GPUs\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 2: CONFIGURATION\n",
    "# ==================================================================================\n",
    "from datetime import datetime\n",
    "\n",
    "# Main configuration\n",
    "CONFIG = {\n",
    "    # Data paths - POINT TO YOUR DATA HERE\n",
    "    'data_path': './deduplicated_complexes_parallel.csv',  # YOUR DEDUPLICATED DATA\n",
    "    'structure_chunks_dir': '../input/chunk/',  # YOUR STRUCTURE CHUNKS\n",
    "    \n",
    "    # Task configuration\n",
    "    'task_cols': ['pKi', 'pEC50', 'pKd (Wang, FEP)', 'pKd', 'pIC50', 'potency'],\n",
    "    \n",
    "    # Model configuration\n",
    "    'model_config': {\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_edge_in_dim': [16, 1],\n",
    "        'drug_edge_h_dims': [32, 1],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    },\n",
    "    \n",
    "    # Training configuration for multi-GPU\n",
    "    'training_config': {\n",
    "        'batch_size_per_gpu': 512,  # 512 * 16 GPUs = 8192 total batch size\n",
    "        'n_epochs': 200,\n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 30,\n",
    "        'n_folds': 5\n",
    "    },\n",
    "    \n",
    "    # Other settings\n",
    "    'seed': SEED,\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'use_mixed_precision': True  # For A100 tensor cores\n",
    "}\n",
    "\n",
    "# Create experiment directory\n",
    "experiment_name = f'ddp_training_{datetime.now():%Y%m%d_%H%M%S}'\n",
    "experiment_dir = Path(f'../output/experiments/{experiment_name}')\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG['experiment_dir'] = str(experiment_dir)\n",
    "CONFIG['checkpoint_dir'] = str(experiment_dir / 'checkpoints')\n",
    "CONFIG['log_dir'] = str(experiment_dir / 'logs')\n",
    "\n",
    "# Create subdirectories\n",
    "(experiment_dir / 'checkpoints').mkdir(exist_ok=True)\n",
    "(experiment_dir / 'logs').mkdir(exist_ok=True)\n",
    "(experiment_dir / 'results').mkdir(exist_ok=True)\n",
    "(experiment_dir / 'figures').mkdir(exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "config_path = experiment_dir / 'config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"✓ Experiment: {experiment_name}\")\n",
    "print(f\"✓ Config saved to: {config_path}\")\n",
    "print(f\"✓ Using {torch.cuda.device_count()} GPUs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ddfb988-de6f-45cd-8d50-9a63341408ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data...\n",
      "✓ Loaded 1,000 samples\n",
      "\n",
      "Data columns: ['protein_pdb_path', 'ligand_sdf_path', 'smiles', 'pKi', 'source_file', 'is_experimental', 'resolution', 'pEC50', 'pKd (Wang, FEP)', 'pKd', 'pIC50', 'SMILES', 'potency', 'assay', 'standardized_protein_pdb', 'standardized_ligand_sdf', 'std_smiles', 'protein_id', 'InChIKey', 'MolWt', 'HeavyAtomCount', 'QED', 'NumHDonors', 'NumHAcceptors', 'NumRotatableBonds', 'TPSA', 'LogP', 'LE_pKi', 'LEnorm_pKi', 'LE_pEC50', 'LEnorm_pEC50', 'LE_pKd (Wang, FEP)', 'LEnorm_pKd (Wang, FEP)', 'LE_pKd', 'LEnorm_pKd', 'LE_pIC50', 'LEnorm_pIC50', 'LE_potency', 'LEnorm_potency', 'LE', 'LE_norm', 'carbon_count', 'sequence_id', 'processed_inchikey', 'inchikey_hash', 'has_valid_inchikey', 'num_merged', 'merged_from_sources']\n",
      "\n",
      "Data shape: (1000, 48)\n",
      "\n",
      "Task coverage:\n",
      "  pKi: 218 samples (21.8%)\n",
      "  pEC50: 56 samples (5.6%)\n",
      "  pKd (Wang, FEP): 1 samples (0.1%)\n",
      "  pKd: 77 samples (7.7%)\n",
      "  pIC50: 652 samples (65.2%)\n",
      "  potency: 0 samples (0.0%)\n",
      "Task ranges for weighting:\n",
      "  pKi: range=7.28, weight=0.0523\n",
      "  pEC50: range=5.74, weight=0.0662\n",
      "  pKd (Wang, FEP): range=0.00, weight=0.3804\n",
      "  pKd: range=5.52, weight=0.0690\n",
      "  pIC50: range=7.36, weight=0.0517\n",
      "  potency: range=1.00, weight=0.3804\n",
      "\n",
      "Initializing structure chunk loader...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n",
      "IOStream.flush timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 406088 structures from 10 chunks\n",
      "Optimization: mmap=True, pickle=False, preload=False\n",
      "✓ Chunk loader ready\n",
      "\n",
      "✓ Data preparation complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================================================================================\n",
    "# CELL 3: LOAD AND PREPARE DATA\n",
    "# ==================================================================================\n",
    "print(\"\\nLoading data...\")\n",
    "\n",
    "# Load your deduplicated data\n",
    "df = pd.read_csv(CONFIG['data_path'])[:1000]\n",
    "print(f\"✓ Loaded {len(df):,} samples\")\n",
    "\n",
    "# Display data info\n",
    "print(\"\\nData columns:\", df.columns.tolist())\n",
    "print(\"\\nData shape:\", df.shape)\n",
    "\n",
    "# Check task coverage\n",
    "print(\"\\nTask coverage:\")\n",
    "for task in CONFIG['task_cols']:\n",
    "    if task in df.columns:\n",
    "        coverage = df[task].notna().sum()\n",
    "        print(f\"  {task}: {coverage:,} samples ({coverage/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Calculate task ranges for weighting\n",
    "task_ranges = prepare_mtl_experiment(df, CONFIG['task_cols'])\n",
    "\n",
    "# Create chunk loader for structure loading\n",
    "print(\"\\nInitializing structure chunk loader...\")\n",
    "chunk_loader = StructureChunkLoader(\n",
    "    chunk_dir=CONFIG['structure_chunks_dir'],\n",
    "    cache_size=10\n",
    ")\n",
    "print(\"✓ Chunk loader ready\")\n",
    "\n",
    "# Save task ranges to config\n",
    "CONFIG['task_ranges'] = task_ranges\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(\"\\n✓ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0739e79-cea5-4aa1-9eb9-c2558824a680",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LAUNCHING FULL TRAINING ON 16 GPUs\n",
      "======================================================================\n",
      "Total batch size: 8192\n",
      "Learning rate: 0.001\n",
      "Epochs: 200\n",
      "Mixed precision: True\n",
      "======================================================================\n",
      "Launching train with 16 GPUs...\n",
      "Command: torchrun --nproc_per_node 16 --master_port 12355 ../training/launch_training.py --config ../output/experiments/ddp_training_20250926_074933/config.json --mode train --n_gpus 16\n",
      "W0926 08:08:42.672000 56446 site-packages/torch/distributed/run.py:766]\n",
      "W0926 08:08:42.672000 56446 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0926 08:08:42.672000 56446 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "W0926 08:08:42.672000 56446 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "\n",
      "train completed!\n",
      "\n",
      "✓ Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 4: LAUNCH FULL TRAINING (MULTI-GPU)\n",
    "# ==================================================================================\n",
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def launch_ddp_training(config_path, mode='train', n_gpus=16):\n",
    "    \"\"\"\n",
    "    Launch DDP training using torchrun.\n",
    "    \"\"\"\n",
    "    # Save data path for the launch script\n",
    "    config_with_data = CONFIG.copy()\n",
    "    config_with_data['data_path'] = os.path.abspath(CONFIG['data_path'])\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(config_with_data, f, indent=2)\n",
    "    \n",
    "    cmd = [\n",
    "        'torchrun',\n",
    "        '--nproc_per_node', str(n_gpus),\n",
    "        '--master_port', '12355',\n",
    "        '../training/launch_training.py',\n",
    "        '--config', str(config_path),\n",
    "        '--mode', mode,\n",
    "        '--n_gpus', str(n_gpus)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Launching {mode} with {n_gpus} GPUs...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Launch process\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    # Print output in real-time\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        if line:\n",
    "            print(line.rstrip())\n",
    "    \n",
    "    process.wait()\n",
    "    print(f\"\\n{mode} completed!\")\n",
    "    \n",
    "    return process.returncode\n",
    "\n",
    "# Launch full training\n",
    "print(\"=\"*70)\n",
    "print(\"LAUNCHING FULL TRAINING ON 16 GPUs\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total batch size: {CONFIG['training_config']['batch_size_per_gpu'] * 16}\")\n",
    "print(f\"Learning rate: {CONFIG['training_config']['learning_rate']}\")\n",
    "print(f\"Epochs: {CONFIG['training_config']['n_epochs']}\")\n",
    "print(f\"Mixed precision: {CONFIG['use_mixed_precision']}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "return_code = launch_ddp_training(config_path, mode='train', n_gpus=16)\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"\\n✓ Training completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Training finished with return code {return_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "476f73cb-26c5-400c-8e2b-a02f67b7274e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking outputs...\n",
      "Log directory exists: True\n",
      "Checkpoint directory exists: True\n",
      "Log files: []\n",
      "Checkpoint files: []\n",
      "✓ Created simplified training script: ../simple_ddp_train.py\n",
      "======================================================================\n",
      "RUNNING SIMPLIFIED DDP TRAINING\n",
      "======================================================================\n",
      "GPUs: 16 x A100\n",
      "Samples: 1000 (for testing)\n",
      "Epochs: 3\n",
      "======================================================================\n",
      "Running command: torchrun --nproc_per_node 16 --master_port 12356 ../simple_ddp_train.py --config ../output/experiments/ddp_training_20250926_074933/config.json --n_gpus 16\n",
      "W0926 08:17:01.782000 59034 site-packages/torch/distributed/run.py:766]\n",
      "W0926 08:17:01.782000 59034 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0926 08:17:01.782000 59034 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "W0926 08:17:01.782000 59034 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../simple_ddp_train.py\", line 21, in <module>\n",
      "    from gnn_dta_mtl import (\n",
      "    from gnn_dta_mtl import (    ImportError\n",
      ": from gnn_dta_mtl import (cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "    from gnn_dta_mtl import (    from gnn_dta_mtl import (\n",
      "ImportError\n",
      ": cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "    from gnn_dta_mtl import (    from gnn_dta_mtl import (    from gnn_dta_mtl import (\n",
      "ImportError:\n",
      "cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "\n",
      "    ImportErrorfrom gnn_dta_mtl import (: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)ImportError\n",
      ": cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "from gnn_dta_mtl import (\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "\n",
      "    from gnn_dta_mtl import (ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "\n",
      "    from gnn_dta_mtl import (from gnn_dta_mtl import (ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "    from gnn_dta_mtl import (\n",
      "\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "    from gnn_dta_mtl import (\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "    from gnn_dta_mtl import (\n",
      "ImportError: cannot import name 'MTL_DTAModel' from 'gnn_dta_mtl' (unknown location)\n",
      "W0926 08:17:06.860000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59109 closing signal SIGTERM\n",
      "W0926 08:17:06.861000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59110 closing signal SIGTERM\n",
      "W0926 08:17:06.861000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59111 closing signal SIGTERM\n",
      "W0926 08:17:06.861000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59112 closing signal SIGTERM\n",
      "W0926 08:17:06.861000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59113 closing signal SIGTERM\n",
      "W0926 08:17:06.861000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59114 closing signal SIGTERM\n",
      "W0926 08:17:06.862000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59116 closing signal SIGTERM\n",
      "W0926 08:17:06.862000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59117 closing signal SIGTERM\n",
      "W0926 08:17:06.862000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59119 closing signal SIGTERM\n",
      "W0926 08:17:06.862000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59120 closing signal SIGTERM\n",
      "W0926 08:17:06.862000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59121 closing signal SIGTERM\n",
      "W0926 08:17:06.862000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59122 closing signal SIGTERM\n",
      "W0926 08:17:06.863000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59123 closing signal SIGTERM\n",
      "W0926 08:17:06.863000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 59124 closing signal SIGTERM\n",
      "E0926 08:17:06.927000 59034 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 6 (pid: 59115) of binary: /opt/conda/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n",
      "============================================================\n",
      "../simple_ddp_train.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-09-26_08:17:06\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 9 (local_rank: 9)\n",
      "  exitcode  : 1 (pid: 59118)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-09-26_08:17:06\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 1 (pid: 59115)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "\n",
      "⚠ Training failed with return code 1\n"
     ]
    }
   ],
   "source": [
    "# ==================================================================================\n",
    "# CELL 1: CHECK WHAT HAPPENED\n",
    "# ==================================================================================\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if log files were created\n",
    "log_dir = Path(CONFIG['log_dir'])\n",
    "checkpoint_dir = Path(CONFIG['checkpoint_dir'])\n",
    "\n",
    "print(\"Checking outputs...\")\n",
    "print(f\"Log directory exists: {log_dir.exists()}\")\n",
    "print(f\"Checkpoint directory exists: {checkpoint_dir.exists()}\")\n",
    "\n",
    "if log_dir.exists():\n",
    "    log_files = list(log_dir.glob('*'))\n",
    "    print(f\"Log files: {log_files}\")\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoint_files = list(checkpoint_dir.glob('*'))\n",
    "    print(f\"Checkpoint files: {checkpoint_files}\")\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 2: CREATE A SIMPLER SINGLE-FILE TRAINING SCRIPT\n",
    "# ==================================================================================\n",
    "\n",
    "# Let's create a simpler, single-file solution that's easier to debug\n",
    "training_script = '''\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path(__file__).parent.parent))\n",
    "\n",
    "from gnn_dta_mtl import (\n",
    "    MTL_DTAModel, \n",
    "    build_mtl_dataset_optimized, \n",
    "    StructureChunkLoader,\n",
    "    prepare_mtl_experiment,\n",
    "    create_data_splits\n",
    ")\n",
    "\n",
    "class SimpleDDPTrainer:\n",
    "    def __init__(self, rank, world_size, config):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.config = config\n",
    "        \n",
    "        # Setup process group\n",
    "        os.environ['MASTER_ADDR'] = 'localhost'\n",
    "        os.environ['MASTER_PORT'] = '12355'\n",
    "        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "        \n",
    "        # Set device\n",
    "        self.device = torch.device(f'cuda:{rank}')\n",
    "        torch.cuda.set_device(rank)\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Main training function\"\"\"\n",
    "        print(f\"Process {self.rank} starting...\")\n",
    "        \n",
    "        # Load data on rank 0 only\n",
    "        if self.rank == 0:\n",
    "            print(\"Loading data...\")\n",
    "            df = pd.read_csv(self.config['data_path'])\n",
    "            \n",
    "            # Limit data size for testing\n",
    "            if 'sample_size' in self.config:\n",
    "                df = df.head(self.config['sample_size'])\n",
    "                print(f\"Using {len(df)} samples\")\n",
    "            \n",
    "            # Calculate task ranges\n",
    "            from gnn_dta_mtl import prepare_mtl_experiment\n",
    "            task_ranges = prepare_mtl_experiment(df, self.config['task_cols'])\n",
    "            \n",
    "            # Create chunk loader\n",
    "            chunk_loader = StructureChunkLoader(\n",
    "                chunk_dir=self.config['structure_chunks_dir'],\n",
    "                cache_size=10\n",
    "            )\n",
    "            \n",
    "            # Create splits\n",
    "            from gnn_dta_mtl.datasets import create_data_splits\n",
    "            splits = create_data_splits(df, split_method='random', split_frac=[0.8, 0.1, 0.1], seed=42)\n",
    "            df_train = splits['train']\n",
    "            df_valid = splits['valid']\n",
    "            \n",
    "            print(f\"Train: {len(df_train)}, Valid: {len(df_valid)}\")\n",
    "            \n",
    "            # Build datasets\n",
    "            print(\"Building datasets...\")\n",
    "            train_dataset = build_mtl_dataset_optimized(df_train, chunk_loader, self.config['task_cols'])\n",
    "            valid_dataset = build_mtl_dataset_optimized(df_valid, chunk_loader, self.config['task_cols'])\n",
    "            \n",
    "            data_ready = True\n",
    "        else:\n",
    "            train_dataset = None\n",
    "            valid_dataset = None\n",
    "            task_ranges = None\n",
    "            data_ready = None\n",
    "        \n",
    "        # Broadcast data ready signal\n",
    "        data_ready = [data_ready]\n",
    "        dist.broadcast_object_list(data_ready, src=0)\n",
    "        \n",
    "        if not data_ready[0]:\n",
    "            print(f\"Process {self.rank}: Data preparation failed\")\n",
    "            return\n",
    "        \n",
    "        # Broadcast datasets\n",
    "        if self.rank == 0:\n",
    "            datasets_and_ranges = [train_dataset, valid_dataset, task_ranges]\n",
    "        else:\n",
    "            datasets_and_ranges = [None, None, None]\n",
    "        \n",
    "        dist.broadcast_object_list(datasets_and_ranges, src=0)\n",
    "        train_dataset, valid_dataset, task_ranges = datasets_and_ranges\n",
    "        \n",
    "        # Create distributed samplers\n",
    "        train_sampler = DistributedSampler(\n",
    "            train_dataset,\n",
    "            num_replicas=self.world_size,\n",
    "            rank=self.rank,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        valid_sampler = DistributedSampler(\n",
    "            valid_dataset,\n",
    "            num_replicas=self.world_size,\n",
    "            rank=self.rank,\n",
    "            shuffle=False\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        import torch_geometric\n",
    "        batch_size = self.config['training_config'].get('batch_size_per_gpu', 32)\n",
    "        \n",
    "        train_loader = torch_geometric.loader.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        valid_loader = torch_geometric.loader.DataLoader(\n",
    "            valid_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=valid_sampler,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Create model\n",
    "        model = MTL_DTAModel(\n",
    "            task_names=self.config['task_cols'],\n",
    "            **self.config['model_config']\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Wrap in DDP\n",
    "        model = DDP(model, device_ids=[self.rank])\n",
    "        \n",
    "        # Create optimizer and loss\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=self.config['training_config']['learning_rate'])\n",
    "        \n",
    "        # Simple MSE loss\n",
    "        criterion = nn.MSELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        n_epochs = self.config['training_config'].get('n_epochs', 5)\n",
    "        \n",
    "        if self.rank == 0:\n",
    "            print(f\"Starting training for {n_epochs} epochs...\")\n",
    "            checkpoint_dir = Path(self.config['checkpoint_dir'])\n",
    "            checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Set epoch for sampler\n",
    "            train_sampler.set_epoch(epoch)\n",
    "            \n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            if self.rank == 0:\n",
    "                pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\")\n",
    "            else:\n",
    "                pbar = train_loader\n",
    "            \n",
    "            for batch in pbar:\n",
    "                xd = batch['drug'].to(self.device)\n",
    "                xp = batch['protein'].to(self.device)\n",
    "                y = batch['y'].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                pred = model(xd, xp)\n",
    "                \n",
    "                # Handle NaN values\n",
    "                mask = ~torch.isnan(y)\n",
    "                if mask.sum() > 0:\n",
    "                    loss = criterion(pred[mask], y[mask])\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    train_loss += loss.item()\n",
    "                    n_batches += 1\n",
    "            \n",
    "            # Print progress\n",
    "            if self.rank == 0 and n_batches > 0:\n",
    "                avg_train_loss = train_loss / n_batches\n",
    "                print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}\")\n",
    "                \n",
    "                # Save checkpoint\n",
    "                if epoch % 5 == 0 or epoch == n_epochs - 1:\n",
    "                    checkpoint = {\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.module.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': avg_train_loss\n",
    "                    }\n",
    "                    torch.save(checkpoint, checkpoint_dir / f'checkpoint_epoch_{epoch}.pt')\n",
    "                    print(f\"Saved checkpoint at epoch {epoch+1}\")\n",
    "        \n",
    "        if self.rank == 0:\n",
    "            print(\"Training completed!\")\n",
    "            # Save final model\n",
    "            torch.save(model.module.state_dict(), checkpoint_dir / 'final_model.pt')\n",
    "        \n",
    "        # Clean up\n",
    "        dist.destroy_process_group()\n",
    "\n",
    "def run_process(rank, world_size, config):\n",
    "    \"\"\"Function to run in each process\"\"\"\n",
    "    trainer = SimpleDDPTrainer(rank, world_size, config)\n",
    "    trainer.train()\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--config', type=str, required=True)\n",
    "    parser.add_argument('--n_gpus', type=int, default=16)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load config\n",
    "    with open(args.config, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Spawn processes\n",
    "    mp.spawn(run_process, args=(args.n_gpus, config), nprocs=args.n_gpus, join=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the training script\n",
    "script_path = Path('../simple_ddp_train.py')\n",
    "with open(script_path, 'w') as f:\n",
    "    f.write(training_script)\n",
    "\n",
    "print(f\"✓ Created simplified training script: {script_path}\")\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 3: RUN THE SIMPLIFIED TRAINING\n",
    "# ==================================================================================\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def run_simple_training(config_path, n_gpus=16):\n",
    "    \"\"\"Run the simplified training script\"\"\"\n",
    "    \n",
    "    # Update config for quick test\n",
    "    CONFIG['sample_size'] = 1000  # Use only 1000 samples\n",
    "    CONFIG['training_config']['n_epochs'] = 3  # Just 3 epochs for testing\n",
    "    CONFIG['training_config']['batch_size_per_gpu'] = 16  # Smaller batch\n",
    "    \n",
    "    # Save updated config\n",
    "    with open(config_path, 'w') as f:\n",
    "        json.dump(CONFIG, f, indent=2)\n",
    "    \n",
    "    cmd = [\n",
    "        'torchrun',\n",
    "        '--nproc_per_node', str(n_gpus),\n",
    "        '--master_port', '12356',  # Different port to avoid conflicts\n",
    "        '../simple_ddp_train.py',\n",
    "        '--config', str(config_path),\n",
    "        '--n_gpus', str(n_gpus)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Running command: {' '.join(cmd)}\")\n",
    "    \n",
    "    # Run the training\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True\n",
    "    )\n",
    "    \n",
    "    # Print output\n",
    "    for line in iter(process.stdout.readline, ''):\n",
    "        if line:\n",
    "            print(line.rstrip())\n",
    "    \n",
    "    process.wait()\n",
    "    return process.returncode\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RUNNING SIMPLIFIED DDP TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(f\"GPUs: 16 x A100\")\n",
    "print(f\"Samples: 1000 (for testing)\")\n",
    "print(f\"Epochs: 3\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "return_code = run_simple_training(config_path, n_gpus=16)\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"\\n✓ Training completed successfully!\")\n",
    "    \n",
    "    # Check outputs\n",
    "    checkpoint_dir = Path(CONFIG['checkpoint_dir'])\n",
    "    if checkpoint_dir.exists():\n",
    "        checkpoints = list(checkpoint_dir.glob('*.pt'))\n",
    "        print(f\"\\nSaved checkpoints:\")\n",
    "        for ckpt in checkpoints:\n",
    "            print(f\"  - {ckpt.name}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Training failed with return code {return_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ce951-f79d-4b05-b9e9-ec22cfa96c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2b4ba-2b38-4afd-a569-1f90e403f412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45da21-261d-4e65-8698-16fd6eb00782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481edb28-bd08-483a-a920-10372aa9ca87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd48ac3-c700-45ab-b970-5f7b258cf127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6c6222-7e63-4600-a550-3fc815d27ae9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Progress:\n",
      "--------------------------------------------------\n",
      "No metrics files found yet...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==================================================================================\n",
    "# CELL 5: MONITOR TRAINING PROGRESS\n",
    "# ==================================================================================\n",
    "import json\n",
    "import glob\n",
    "\n",
    "def check_training_progress(log_dir):\n",
    "    \"\"\"Check training progress from log files.\"\"\"\n",
    "    log_dir = Path(log_dir)\n",
    "    \n",
    "    # Find metrics file\n",
    "    metrics_files = list(log_dir.glob('metrics_*.json'))\n",
    "    if not metrics_files:\n",
    "        print(\"No metrics files found yet...\")\n",
    "        return None\n",
    "    \n",
    "    # Load latest metrics\n",
    "    latest_file = max(metrics_files, key=os.path.getmtime)\n",
    "    with open(latest_file, 'r') as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    if metrics:\n",
    "        latest = metrics[-1]\n",
    "        print(f\"Latest epoch: {latest.get('epoch', 'N/A')}\")\n",
    "        print(f\"Train loss: {latest.get('train_loss', 0):.4f}\")\n",
    "        print(f\"Valid loss: {latest.get('val_loss', 0):.4f}\")\n",
    "        \n",
    "        if 'task_metrics' in latest:\n",
    "            print(\"\\nTask metrics:\")\n",
    "            for task, task_metrics in latest['task_metrics'].items():\n",
    "                print(f\"  {task}: RMSE={task_metrics['rmse']:.3f}, R²={task_metrics['r2']:.3f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Check progress\n",
    "print(\"Training Progress:\")\n",
    "print(\"-\" * 50)\n",
    "metrics = check_training_progress(CONFIG['log_dir'])\n",
    "\n",
    "# Plot training curves if available\n",
    "if metrics and len(metrics) > 1:\n",
    "    epochs = [m['epoch'] for m in metrics]\n",
    "    train_losses = [m.get('train_loss', 0) for m in metrics]\n",
    "    val_losses = [m.get('val_loss', 0) for m in metrics]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Valid Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Progress')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d72db928-be46-401c-b7a0-26288a2c7879",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "LAUNCHING CROSS-VALIDATION ON 16 GPUs\n",
      "======================================================================\n",
      "Number of folds: 5\n",
      "Total batch size: 8192\n",
      "======================================================================\n",
      "Launching cv with 16 GPUs...\n",
      "Command: torchrun --nproc_per_node 16 --master_port 12355 ../gnn_dta_mtl/training/launch_training.py --config ../output/experiments/ddp_training_20250926_074933/config.json --mode cv --n_gpus 16\n",
      "W0926 08:06:24.358000 56009 site-packages/torch/distributed/run.py:766]\n",
      "W0926 08:06:24.358000 56009 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "W0926 08:06:24.358000 56009 site-packages/torch/distributed/run.py:766] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.\n",
      "W0926 08:06:24.358000 56009 site-packages/torch/distributed/run.py:766] *****************************************\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "/opt/conda/bin/python3.10: can't open file '/home/HX46_FR5/github/APEX2/gnn_dta_mtl/notebooks/../gnn_dta_mtl/training/launch_training.py': [Errno 2] No such file or directory\n",
      "E0926 08:06:24.523000 56009 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 2) local_rank: 0 (pid: 56080) of binary: /opt/conda/bin/python3.10\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 892, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 883, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 139, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 270, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError:\n",
      "============================================================\n",
      "../gnn_dta_mtl/training/launch_training.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 2 (pid: 56081)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 2 (pid: 56082)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[3]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 2 (pid: 56083)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[4]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 4 (local_rank: 4)\n",
      "  exitcode  : 2 (pid: 56084)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[5]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 5 (local_rank: 5)\n",
      "  exitcode  : 2 (pid: 56085)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[6]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 2 (pid: 56086)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[7]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 7 (local_rank: 7)\n",
      "  exitcode  : 2 (pid: 56087)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[8]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 8 (local_rank: 8)\n",
      "  exitcode  : 2 (pid: 56088)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[9]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 9 (local_rank: 9)\n",
      "  exitcode  : 2 (pid: 56089)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[10]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 10 (local_rank: 10)\n",
      "  exitcode  : 2 (pid: 56090)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[11]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 11 (local_rank: 11)\n",
      "  exitcode  : 2 (pid: 56091)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[12]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 12 (local_rank: 12)\n",
      "  exitcode  : 2 (pid: 56092)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[13]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 13 (local_rank: 13)\n",
      "  exitcode  : 2 (pid: 56093)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[14]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 14 (local_rank: 14)\n",
      "  exitcode  : 2 (pid: 56094)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[15]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 15 (local_rank: 15)\n",
      "  exitcode  : 2 (pid: 56095)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-09-26_08:06:24\n",
      "  host      : hx46-fr5-workbench-prod.europe-west4-a.c.computing-axe-prod-ac02.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 2 (pid: 56080)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "\n",
      "cv completed!\n",
      "\n",
      "⚠ Cross-validation finished with return code 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 6: LAUNCH CROSS-VALIDATION (MULTI-GPU)\n",
    "# ==================================================================================\n",
    "print(\"=\"*70)\n",
    "print(\"LAUNCHING CROSS-VALIDATION ON 16 GPUs\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Number of folds: {CONFIG['training_config']['n_folds']}\")\n",
    "print(f\"Total batch size: {CONFIG['training_config']['batch_size_per_gpu'] * 16}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "return_code = launch_ddp_training(config_path, mode='cv', n_gpus=16)\n",
    "\n",
    "if return_code == 0:\n",
    "    print(\"\\n✓ Cross-validation completed successfully!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Cross-validation finished with return code {return_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d548f3f5-c4d0-4e76-b7e6-2b6c7b2074a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Results:\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 7: CHECK RESULTS\n",
    "# ==================================================================================\n",
    "def check_training_results(checkpoint_dir):\n",
    "    \"\"\"Check final training results.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    \n",
    "    # Check for best model\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        print(\"✓ Best Model Found!\")\n",
    "        print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"  Validation Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
    "        \n",
    "        if 'task_metrics' in checkpoint:\n",
    "            print(\"\\n  Task Metrics:\")\n",
    "            for task, metrics in checkpoint['task_metrics'].items():\n",
    "                print(f\"    {task}: RMSE={metrics['rmse']:.3f}, R²={metrics['r2']:.3f}\")\n",
    "    \n",
    "    # Check for CV results\n",
    "    cv_results_path = checkpoint_dir / 'cv_results.json'\n",
    "    if cv_results_path.exists():\n",
    "        with open(cv_results_path, 'r') as f:\n",
    "            cv_results = json.load(f)\n",
    "        \n",
    "        print(\"\\n✓ Cross-Validation Results:\")\n",
    "        for task, metrics in cv_results.items():\n",
    "            print(f\"  {task}:\")\n",
    "            print(f\"    R²: {metrics['r2_mean']:.3f} ± {metrics['r2_std']:.3f}\")\n",
    "            print(f\"    RMSE: {metrics['rmse_mean']:.3f} ± {metrics['rmse_std']:.3f}\")\n",
    "\n",
    "print(\"Final Results:\")\n",
    "print(\"=\"*70)\n",
    "check_training_results(CONFIG['checkpoint_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d576d6d-26f9-415e-98c3-7af4dd814ad6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ No best model found!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 8: LOAD BEST MODEL FOR INFERENCE\n",
    "# ==================================================================================\n",
    "def load_best_model(checkpoint_dir, model_config, task_cols):\n",
    "    \"\"\"Load the best trained model.\"\"\"\n",
    "    # Create model\n",
    "    model = MTL_DTAModel(\n",
    "        task_names=task_cols,\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = Path(checkpoint_dir) / 'best_model.pt'\n",
    "    if checkpoint_path.exists():\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✓ Loaded model from epoch {checkpoint['epoch']}\")\n",
    "        return model\n",
    "    else:\n",
    "        print(\"⚠ No best model found!\")\n",
    "        return None\n",
    "\n",
    "# Load the best model\n",
    "best_model = load_best_model(\n",
    "    CONFIG['checkpoint_dir'],\n",
    "    CONFIG['model_config'],\n",
    "    CONFIG['task_cols']\n",
    ")\n",
    "\n",
    "if best_model:\n",
    "    # Move to GPU for inference\n",
    "    best_model = best_model.cuda()\n",
    "    best_model.eval()\n",
    "    print(\"✓ Model ready for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163d55a-f9ed-446a-8b98-07ce8748ce04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dacf503f-01c9-4ab6-aa87-824ef1825815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ All done!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ==================================================================================\n",
    "# CELL 9: INFERENCE EXAMPLE\n",
    "# ==================================================================================\n",
    "if best_model:\n",
    "    # Example: Make predictions on a sample\n",
    "    print(\"\\nPreparing sample for inference...\")\n",
    "    \n",
    "    # Take a small sample from your data\n",
    "    sample_df = df.sample(n=10, random_state=42)\n",
    "    \n",
    "    # Build dataset\n",
    "    sample_dataset = build_mtl_dataset_optimized(\n",
    "        sample_df, \n",
    "        chunk_loader, \n",
    "        CONFIG['task_cols']\n",
    "    )\n",
    "    \n",
    "    # Create loader\n",
    "    sample_loader = torch_geometric.loader.DataLoader(\n",
    "        sample_dataset,\n",
    "        batch_size=10,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    best_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in sample_loader:\n",
    "            xd = batch['drug'].cuda()\n",
    "            xp = batch['protein'].cuda()\n",
    "            y_true = batch['y'].cuda()\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = best_model(xd, xp)\n",
    "            \n",
    "            # Display results\n",
    "            print(\"\\nSample predictions:\")\n",
    "            for i, task in enumerate(CONFIG['task_cols']):\n",
    "                if not torch.isnan(y_true[0, i]):\n",
    "                    print(f\"{task}:\")\n",
    "                    print(f\"  True: {y_true[0, i].item():.3f}\")\n",
    "                    print(f\"  Pred: {y_pred[0, i].item():.3f}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n✓ All done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa043a6-6cea-4451-88ae-c381c8009915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee8bd93-8be0-4e7b-85f9-22cf0d9b739d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c5bb3-8ab0-4639-af68-f787b3dbc7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe089554-b242-42a0-8ede-6be0d4581dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b648b963-b487-420b-ae10-a163733208dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee94c56e-222a-4c25-a633-01e7e0cc9e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d7040-3f93-425d-864b-e0d44fe16f81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5c76f1-e130-442b-bb5d-8656893e4d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c343da18-f8c8-4b4b-82de-afdb1f9c0e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05315fe8-430d-4b57-9ea3-64aaeada0275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd620e1-ead1-4037-987b-59f1c3dafdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e869467a-d50b-4775-aba6-d540f3d2076d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b26b94a3-6c25-435c-89d1-980b3ca67b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "406088"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c35eb-8335-4e1e-a50d-b518fbd11569",
   "metadata": {},
   "source": [
    "# Drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2df7b5e-d5f3-4950-988b-2371404097de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import PDB\n",
    "from Bio.PDB import PDBParser\n",
    "from collections import Counter, defaultdict\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from functools import partial\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL PDB PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def extract_sequence_from_pdb_single(pdb_path):\n",
    "    \"\"\"\n",
    "    Extract sequences from a single PDB file for each chain.\n",
    "    This function is designed to be run in parallel.\n",
    "    \n",
    "    Returns: (pdb_path, list of sequences)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        parser = PDBParser(QUIET=True)\n",
    "        structure = parser.get_structure('protein', pdb_path)\n",
    "        \n",
    "        sequences = []\n",
    "        for model in structure:\n",
    "            for chain in model:\n",
    "                seq = []\n",
    "                for residue in chain:\n",
    "                    if PDB.is_aa(residue):\n",
    "                        res_name = residue.get_resname()\n",
    "                        # Convert 3-letter code to 1-letter code\n",
    "                        three_to_one = {\n",
    "                            'ALA': 'A', 'CYS': 'C', 'ASP': 'D', 'GLU': 'E',\n",
    "                            'PHE': 'F', 'GLY': 'G', 'HIS': 'H', 'ILE': 'I',\n",
    "                            'LYS': 'K', 'LEU': 'L', 'MET': 'M', 'ASN': 'N',\n",
    "                            'PRO': 'P', 'GLN': 'Q', 'ARG': 'R', 'SER': 'S',\n",
    "                            'THR': 'T', 'VAL': 'V', 'TRP': 'W', 'TYR': 'Y'\n",
    "                        }\n",
    "                        if res_name in three_to_one:\n",
    "                            seq.append(three_to_one[res_name])\n",
    "                \n",
    "                if seq:\n",
    "                    sequences.append(''.join(seq))\n",
    "        \n",
    "        return (pdb_path, sequences)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pdb_path}: {e}\")\n",
    "        return (pdb_path, [])\n",
    "\n",
    "def parallel_extract_pdb_sequences(pdb_paths, n_workers=None):\n",
    "    \"\"\"\n",
    "    Extract sequences from multiple PDB files in parallel.\n",
    "    \n",
    "    Args:\n",
    "        pdb_paths: List of PDB file paths\n",
    "        n_workers: Number of parallel workers (None = use all CPUs)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping pdb_path to sequences\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = cpu_count()\n",
    "    \n",
    "    print(f\"Processing {len(pdb_paths)} PDB files with {n_workers} workers...\")\n",
    "    \n",
    "    with Pool(n_workers) as pool:\n",
    "        results = list(tqdm(\n",
    "            pool.imap(extract_sequence_from_pdb_single, pdb_paths),\n",
    "            total=len(pdb_paths),\n",
    "            desc=\"Extracting PDB sequences\"\n",
    "        ))\n",
    "    \n",
    "    # Convert results to dictionary\n",
    "    pdb_to_sequences = {pdb_path: sequences for pdb_path, sequences in results}\n",
    "    return pdb_to_sequences\n",
    "\n",
    "def create_global_sequence_mapping(pdb_to_sequences):\n",
    "    \"\"\"\n",
    "    Create a global mapping of sequences to IDs across all PDBs.\n",
    "    \n",
    "    Args:\n",
    "        pdb_to_sequences: Dictionary mapping PDB paths to their sequences\n",
    "    \n",
    "    Returns:\n",
    "        seq_to_id: Dictionary mapping sequences to IDs\n",
    "        id_to_seq: Dictionary mapping IDs to sequences\n",
    "    \"\"\"\n",
    "    seq_to_id = {}\n",
    "    id_to_seq = {}\n",
    "    \n",
    "    # Collect all unique sequences\n",
    "    all_sequences = set()\n",
    "    for sequences in pdb_to_sequences.values():\n",
    "        all_sequences.update(sequences)\n",
    "    \n",
    "    # Assign IDs to unique sequences\n",
    "    for i, seq in enumerate(sorted(all_sequences)):\n",
    "        if i < 26:\n",
    "            seq_id = chr(65 + i)  # A, B, C, ...\n",
    "        else:\n",
    "            # Use two letters for more than 26 sequences\n",
    "            seq_id = f\"{chr(65 + (i // 26 - 1))}{chr(65 + (i % 26))}\"\n",
    "        \n",
    "        seq_to_id[seq] = seq_id\n",
    "        id_to_seq[seq_id] = seq\n",
    "    \n",
    "    return seq_to_id, id_to_seq\n",
    "\n",
    "def create_sequence_id_for_pdb(sequences, seq_to_id):\n",
    "    \"\"\"\n",
    "    Create a sequence ID string for a PDB based on its sequences.\n",
    "    E.g., \"2A4B\" means 2 of sequence A and 4 of sequence B\n",
    "    \"\"\"\n",
    "    if not sequences:\n",
    "        return \"EMPTY\"\n",
    "    \n",
    "    # Count occurrences of each sequence\n",
    "    seq_counter = Counter(sequences)\n",
    "    \n",
    "    # Map sequences to IDs and count them\n",
    "    id_counts = defaultdict(int)\n",
    "    for seq, count in seq_counter.items():\n",
    "        if seq in seq_to_id:\n",
    "            id_counts[seq_to_id[seq]] += count\n",
    "    \n",
    "    # Create the ID string (alphabetically sorted)\n",
    "    id_string = ''\n",
    "    for seq_id in sorted(id_counts.keys()):\n",
    "        count = id_counts[seq_id]\n",
    "        if count == 1:\n",
    "            id_string += seq_id\n",
    "        else:\n",
    "            id_string += f\"{count}{seq_id}\"\n",
    "    \n",
    "    return id_string\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL INCHIKEY PROCESSING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def process_inchikey_batch(batch_data):\n",
    "    \"\"\"\n",
    "    Process a batch of InChIKeys in parallel.\n",
    "    This function can be extended to perform additional InChIKey processing.\n",
    "    \n",
    "    Args:\n",
    "        batch_data: Tuple of (indices, inchikeys, smiles)\n",
    "    \n",
    "    Returns:\n",
    "        List of tuples (index, processed_inchikey, additional_data)\n",
    "    \"\"\"\n",
    "    indices, inchikeys, smiles = batch_data\n",
    "    results = []\n",
    "    \n",
    "    for idx, inchikey, smile in zip(indices, inchikeys, smiles):\n",
    "        # Here you can add more complex InChIKey processing\n",
    "        # For now, we just validate and potentially standardize\n",
    "        processed_inchikey = inchikey if pd.notna(inchikey) else f\"UNKNOWN_{idx}\"\n",
    "        \n",
    "        # You can add molecular property calculations here\n",
    "        additional_data = {\n",
    "            'inchikey_hash': hashlib.md5(str(inchikey).encode()).hexdigest()[:8],\n",
    "            'has_valid_inchikey': pd.notna(inchikey)\n",
    "        }\n",
    "        \n",
    "        results.append((idx, processed_inchikey, additional_data))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def parallel_process_inchikeys(df, n_workers=None, batch_size=10000):\n",
    "    \"\"\"\n",
    "    Process InChIKeys in parallel batches.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing InChIKey column\n",
    "        n_workers: Number of parallel workers\n",
    "        batch_size: Size of each processing batch\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with processed InChIKey information\n",
    "    \"\"\"\n",
    "    if n_workers is None:\n",
    "        n_workers = cpu_count()\n",
    "    \n",
    "    print(f\"Processing {len(df)} InChIKeys with {n_workers} workers...\")\n",
    "    \n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(df), batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size]\n",
    "        batches.append((\n",
    "            batch_df.index.tolist(),\n",
    "            batch_df['InChIKey'].tolist(),\n",
    "            batch_df['std_smiles'].tolist() if 'std_smiles' in df.columns else [None] * len(batch_df)\n",
    "        ))\n",
    "    \n",
    "    # Process batches in parallel\n",
    "    with Pool(n_workers) as pool:\n",
    "        batch_results = list(tqdm(\n",
    "            pool.imap(process_inchikey_batch, batches),\n",
    "            total=len(batches),\n",
    "            desc=\"Processing InChIKeys\"\n",
    "        ))\n",
    "    \n",
    "    # Flatten results and create mapping\n",
    "    all_results = []\n",
    "    for batch_result in batch_results:\n",
    "        all_results.extend(batch_result)\n",
    "    \n",
    "    # Sort by index to maintain order\n",
    "    all_results.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    for idx, processed_inchikey, additional_data in all_results:\n",
    "        df.loc[idx, 'processed_inchikey'] = processed_inchikey\n",
    "        for key, value in additional_data.items():\n",
    "            df.loc[idx, key] = value\n",
    "    \n",
    "    return df\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL DEDUPLICATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def process_duplicate_group(group_data, response_cols, priority_func):\n",
    "    \"\"\"\n",
    "    Process a single group of duplicate complexes.\n",
    "    \n",
    "    Args:\n",
    "        group_data: Tuple of (complex_id, group_dataframe)\n",
    "        response_cols: List of response column names\n",
    "        priority_func: Function to calculate priority\n",
    "    \n",
    "    Returns:\n",
    "        Processed row as dictionary\n",
    "    \"\"\"\n",
    "    complex_id, group = group_data\n",
    "    \n",
    "    if len(group) == 1:\n",
    "        # No duplicates, keep as is\n",
    "        row_dict = group.iloc[0].to_dict()\n",
    "        row_dict['num_merged'] = 1\n",
    "        return row_dict\n",
    "    else:\n",
    "        # Multiple rows for same complex\n",
    "        group = group.copy()\n",
    "        group['priority'] = group['source_file'].apply(priority_func)\n",
    "        group = group.sort_values('priority')\n",
    "        \n",
    "        # Take the row with highest priority as base\n",
    "        best_row = group.iloc[0].to_dict()\n",
    "        \n",
    "        # Merge response values from all rows in the group\n",
    "        for col in response_cols:\n",
    "            values = group[col].dropna().values\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                best_row[col] = np.nan\n",
    "            elif len(values) == 1:\n",
    "                best_row[col] = values[0]\n",
    "            elif len(values) == 2:\n",
    "                best_row[col] = min(values)\n",
    "            else:  # 3 or more values\n",
    "                best_row[col] = np.median(values)\n",
    "        \n",
    "        # Add merge information\n",
    "        best_row['merged_from_sources'] = ','.join(group['source_file'].unique())\n",
    "        best_row['num_merged'] = len(group)\n",
    "        \n",
    "        return best_row\n",
    "\n",
    "def parallel_remove_duplicates(df, n_workers=None):\n",
    "    \"\"\"\n",
    "    Remove duplicates iteratively based on complex (protein+ligand).\n",
    "    Creates and returns a new dataframe without modifying the original.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with sequence_id and InChIKey columns\n",
    "    \n",
    "    Returns:\n",
    "        result_df: New deduplicated DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Define response value columns\n",
    "    response_cols = ['pKd (Wang, FEP)', 'pKd', 'pIC50', 'potency', 'pEC50', 'pKi']\n",
    "    \n",
    "    # Create complex identifier\n",
    "    df_copy['complex_id'] = df_copy['InChIKey'] + '_' + df_copy['sequence_id']\n",
    "    \n",
    "    # Group by complex_id\n",
    "    grouped = df_copy.groupby('complex_id')\n",
    "    \n",
    "    print(f\"Processing {len(grouped)} unique complexes...\")\n",
    "    \n",
    "    # Define priority function\n",
    "    def get_priority(source):\n",
    "        priority_map = {\n",
    "            'PDBbind2020': 1,\n",
    "            'FEP_Wang_2015': 2,\n",
    "            'FEP_Zariquiey_extended_Wang_2015': 3,\n",
    "            'HiQBind': 4,\n",
    "            'BioLip2': 5,\n",
    "            'processed_data': 6,\n",
    "            'BindingNetv2': 7,\n",
    "            'BindingNetv1': 8\n",
    "        }\n",
    "        return priority_map.get(source, 999)\n",
    "    \n",
    "    # Function to merge response values\n",
    "    def merge_response_values(group, response_cols):\n",
    "        \"\"\"\n",
    "        Merge response values from a group of duplicate rows.\n",
    "        For each response type, select median if >=3 datapoints, else minimum.\n",
    "        \"\"\"\n",
    "        merged = {}\n",
    "        \n",
    "        for col in response_cols:\n",
    "            # Collect non-NaN values for this response type\n",
    "            values = group[col].dropna().values\n",
    "            \n",
    "            if len(values) == 0:\n",
    "                merged[col] = np.nan\n",
    "            elif len(values) == 1:\n",
    "                merged[col] = values[0]\n",
    "            elif len(values) == 2:\n",
    "                merged[col] = min(values)\n",
    "            else:  # 3 or more values\n",
    "                merged[col] = np.median(values)\n",
    "        \n",
    "        return merged\n",
    "    \n",
    "    # List to store processed rows\n",
    "    result_rows = []\n",
    "    \n",
    "    # Iterate through each group\n",
    "    for complex_id, group in tqdm(grouped, desc=\"Deduplicating complexes\"):\n",
    "        if len(group) == 1:\n",
    "            # No duplicates, keep as is\n",
    "            row_dict = group.iloc[0].to_dict()\n",
    "            row_dict['num_merged'] = 1\n",
    "            row_dict.pop('complex_id', None)  # Remove complex_id from result\n",
    "            result_rows.append(row_dict)\n",
    "        else:\n",
    "            # Multiple rows for same complex\n",
    "            # Create a copy of the group to avoid warnings\n",
    "            group = group.copy()\n",
    "            \n",
    "            # Add priority column\n",
    "            group['priority'] = group['source_file'].apply(get_priority)\n",
    "            \n",
    "            # Sort by priority (lowest number = highest priority)\n",
    "            group = group.sort_values('priority')\n",
    "            \n",
    "            # Take the row with highest priority as base\n",
    "            best_row = group.iloc[0].to_dict()\n",
    "            \n",
    "            # Merge response values from all rows in the group\n",
    "            merged_values = merge_response_values(group, response_cols)\n",
    "            \n",
    "            # Update the best row with merged values\n",
    "            for col, value in merged_values.items():\n",
    "                best_row[col] = value\n",
    "            \n",
    "            # Add merge information\n",
    "            best_row['merged_from_sources'] = ','.join(group['source_file'].unique())\n",
    "            best_row['num_merged'] = len(group)\n",
    "            \n",
    "            # Remove temporary columns\n",
    "            best_row.pop('priority', None)\n",
    "            best_row.pop('complex_id', None)\n",
    "            \n",
    "            result_rows.append(best_row)\n",
    "    \n",
    "    # Create new dataframe from results\n",
    "    result_df = pd.DataFrame(result_rows)\n",
    "    \n",
    "    # Ensure column order matches original (plus new columns)\n",
    "    original_cols = [col for col in df.columns if col in result_df.columns]\n",
    "    new_cols = [col for col in result_df.columns if col not in df.columns]\n",
    "    result_df = result_df[original_cols + new_cols]\n",
    "    \n",
    "    print(f\"Deduplication complete: {len(df)} -> {len(result_df)} rows\")\n",
    "    print(f\"Removed {len(df) - len(result_df)} duplicate rows ({(1 - len(result_df)/len(df))*100:.2f}% reduction)\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN PIPELINE FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def print_summary_statistics(original_df, final_df):\n",
    "    \"\"\"\n",
    "    Print detailed summary statistics about the deduplication process.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nOriginal dataset:\")\n",
    "    print(f\"  - Total rows: {len(original_df):,}\")\n",
    "    print(f\"  - Unique proteins: {original_df['standardized_protein_pdb'].nunique():,}\")\n",
    "    print(f\"  - Unique ligands: {original_df['InChIKey'].nunique():,}\")\n",
    "    \n",
    "    print(f\"\\nProcessed dataset:\")\n",
    "    print(f\"  - Total rows: {len(final_df):,}\")\n",
    "    print(f\"  - Rows removed: {len(original_df) - len(final_df):,}\")\n",
    "    print(f\"  - Reduction: {(1 - len(final_df)/len(original_df))*100:.2f}%\")\n",
    "    \n",
    "    if 'num_merged' in final_df.columns:\n",
    "        merged_stats = final_df['num_merged'].value_counts().sort_index()\n",
    "        print(f\"\\nMerging statistics:\")\n",
    "        for num, count in merged_stats.items():\n",
    "            print(f\"  - {count:,} complexes {'kept as-is' if num == 1 else f'merged from {num} sources'}\")\n",
    "    \n",
    "    print(\"\\nResponse value coverage:\")\n",
    "    response_cols = ['pKd (Wang, FEP)', 'pKd', 'pIC50', 'potency', 'pEC50', 'pKi']\n",
    "    for col in response_cols:\n",
    "        if col in final_df.columns:\n",
    "            coverage = (final_df[col].notna().sum() / len(final_df)) * 100\n",
    "            print(f\"  - {col}: {coverage:.2f}% coverage ({final_df[col].notna().sum():,} values)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc24a1-c059-4db5-8623-e83233b7d0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### print(\"=\"*60)\n",
    "print(\"STARTING PARALLEL PROCESSING PIPELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Initial dataframe shape: {df.shape}\")\n",
    "# Step 1: Parallel PDB sequence extraction\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: PARALLEL PDB SEQUENCE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "pdb_paths = df['standardized_protein_pdb'].unique().tolist()\n",
    "pdb_to_sequences = parallel_extract_pdb_sequences(pdb_paths, cpu_count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838b728b-f637-4e07-9923-bd8f1d46e135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create global sequence mapping\n",
    "seq_to_id, id_to_seq = create_global_sequence_mapping(pdb_to_sequences)\n",
    "print(f\"Found {len(seq_to_id)} unique protein sequences\")\n",
    "# Apply sequence IDs to dataframe\n",
    "sequence_ids = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    pdb_path = row['standardized_protein_pdb']\n",
    "    sequences = pdb_to_sequences.get(pdb_path, [])\n",
    "    seq_id = create_sequence_id_for_pdb(sequences, seq_to_id)\n",
    "    sequence_ids.append(seq_id)\n",
    "df['sequence_id'] = sequence_ids\n",
    "# Create sequence mapping dataframe\n",
    "seq_mapping = pd.DataFrame([\n",
    "    {'sequence_id': sid, 'sequence': seq} \n",
    "    for sid, seq in id_to_seq.items()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6162b2a-1af1-4896-949e-5f0fb852b352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 2: Parallel InChIKey processing\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: PARALLEL INCHIKEY PROCESSING\")\n",
    "print(\"=\"*60)\n",
    "df = parallel_process_inchikeys(df, cpu_count())\n",
    "# Step 3: Parallel deduplication\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: PARALLEL DEDUPLICATION\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb89a94-5b54-4f5a-bfe0-5e7d213a52df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_pdb_path</th>\n",
       "      <th>ligand_sdf_path</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pKi</th>\n",
       "      <th>source_file</th>\n",
       "      <th>is_experimental</th>\n",
       "      <th>resolution</th>\n",
       "      <th>pEC50</th>\n",
       "      <th>pKd (Wang, FEP)</th>\n",
       "      <th>pKd</th>\n",
       "      <th>...</th>\n",
       "      <th>LE_potency</th>\n",
       "      <th>LEnorm_potency</th>\n",
       "      <th>LE</th>\n",
       "      <th>LE_norm</th>\n",
       "      <th>carbon_count</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>processed_inchikey</th>\n",
       "      <th>inchikey_hash</th>\n",
       "      <th>has_valid_inchikey</th>\n",
       "      <th>complex_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>CCCCCCSCC(NC(=O)CCC(N)C(=O)O)C(=O)NCCC(=O)O</td>\n",
       "      <td>3.259637</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.120727</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>17</td>\n",
       "      <td>2ƘC</td>\n",
       "      <td>RILVFYFKIDXJNY-UHFFFAOYSA-M</td>\n",
       "      <td>2f8150df</td>\n",
       "      <td>True</td>\n",
       "      <td>RILVFYFKIDXJNY-UHFFFAOYSA-M_2ƘC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>NC(CCC(=O)NC(CSCc1ccccc1)C(=O)NC(C(=O)O)c1cccc...</td>\n",
       "      <td>6.376751</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.193235</td>\n",
       "      <td>0.000409</td>\n",
       "      <td>23</td>\n",
       "      <td>2ƘC</td>\n",
       "      <td>ZPSKWMFLCHMEOY-UHFFFAOYSA-M</td>\n",
       "      <td>71a014c7</td>\n",
       "      <td>True</td>\n",
       "      <td>ZPSKWMFLCHMEOY-UHFFFAOYSA-M_2ƘC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>Cc1ccc(CSCC(NC(=O)CCC(N)C(=O)O)C(=O)NCCC(=O)O)cc1</td>\n",
       "      <td>4.397940</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.151653</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>19</td>\n",
       "      <td>2ƉY</td>\n",
       "      <td>MBXWAPNNAOGFPH-UHFFFAOYSA-M</td>\n",
       "      <td>1cba16d5</td>\n",
       "      <td>True</td>\n",
       "      <td>MBXWAPNNAOGFPH-UHFFFAOYSA-M_2ƉY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>NC(CCC(=O)NC(CSCc1ccc(Cl)cc1)C(=O)NC(C(=O)O)c1...</td>\n",
       "      <td>6.920819</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.203553</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>24</td>\n",
       "      <td>2ƘC</td>\n",
       "      <td>BXJSPWKYSSRFEB-UHFFFAOYSA-M</td>\n",
       "      <td>a823c50d</td>\n",
       "      <td>True</td>\n",
       "      <td>BXJSPWKYSSRFEB-UHFFFAOYSA-M_2ƘC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL390...</td>\n",
       "      <td>NC(CCC(=O)NC(CSCc1ccccc1)C(=O)NCCC(=O)O)C(=O)O</td>\n",
       "      <td>3.148742</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.112455</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>18</td>\n",
       "      <td>2ŘN</td>\n",
       "      <td>QLVGMERIDWMEBM-UHFFFAOYSA-M</td>\n",
       "      <td>46313ec9</td>\n",
       "      <td>True</td>\n",
       "      <td>QLVGMERIDWMEBM-UHFFFAOYSA-M_2ŘN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406083</th>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>CC1CCCC(N2NC(C(C)(C)C)C[C@@H]2N[C@@H](O)NC2CCC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BioLip2</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.130768</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.187652</td>\n",
       "      <td>0.000351</td>\n",
       "      <td>29</td>\n",
       "      <td>¦Q</td>\n",
       "      <td>NACQWABIDVXOMV-VFCRXLDWSA-N</td>\n",
       "      <td>b325f1a8</td>\n",
       "      <td>True</td>\n",
       "      <td>NACQWABIDVXOMV-VFCRXLDWSA-N_¦Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406084</th>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>OC[C@H]1O[C@H](O[PH](O)(O)O[PH](O)(O)OC[C@H]2O...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BioLip2</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.124939</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.086804</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>15</td>\n",
       "      <td>ĔV</td>\n",
       "      <td>PEEYOHTXAULSGT-KPLOLNPKSA-N</td>\n",
       "      <td>ba696fe5</td>\n",
       "      <td>True</td>\n",
       "      <td>PEEYOHTXAULSGT-KPLOLNPKSA-N_ĔV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406085</th>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>C[C@H]1S[C@H]2NC(N)N[C@@H](O)[C@@H]2C1SC1CCC(C...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BioLip2</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.698970</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.248354</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>19</td>\n",
       "      <td>ȚS</td>\n",
       "      <td>VYTCQXDFOQVDLB-WZBLSFTPSA-N</td>\n",
       "      <td>3c3e05eb</td>\n",
       "      <td>True</td>\n",
       "      <td>VYTCQXDFOQVDLB-WZBLSFTPSA-N_ȚS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406086</th>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>CCCCCCC1CCC(C(O)NNC(S)NC)O1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BioLip2</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.096910</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.215627</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>13</td>\n",
       "      <td>ǋO</td>\n",
       "      <td>QDYXIFNPTRANPQ-UHFFFAOYSA-N</td>\n",
       "      <td>217020ae</td>\n",
       "      <td>True</td>\n",
       "      <td>QDYXIFNPTRANPQ-UHFFFAOYSA-N_ǋO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406087</th>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>../data/raw/BioLip2/biolip_downloads/biolip_re...</td>\n",
       "      <td>CCOC(O)C1NNN2C3CCCCC3C(S[C@H](C(C)O)C(O)OCC)NC12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BioLip2</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.920819</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.140029</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>18</td>\n",
       "      <td>ŞK</td>\n",
       "      <td>NZHNUIAGNWEXIG-BICOIMHRSA-N</td>\n",
       "      <td>9d8f902a</td>\n",
       "      <td>True</td>\n",
       "      <td>NZHNUIAGNWEXIG-BICOIMHRSA-N_ŞK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>406088 rows × 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_pdb_path  \\\n",
       "0       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "1       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "3       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "4       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "...                                                   ...   \n",
       "406083  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406084  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406085  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406086  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406087  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "\n",
       "                                          ligand_sdf_path  \\\n",
       "0       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "1       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "3       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "4       ../data/raw/BindingNetv2/high/target_CHEMBL390...   \n",
       "...                                                   ...   \n",
       "406083  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406084  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406085  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406086  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "406087  ../data/raw/BioLip2/biolip_downloads/biolip_re...   \n",
       "\n",
       "                                                   smiles       pKi  \\\n",
       "0             CCCCCCSCC(NC(=O)CCC(N)C(=O)O)C(=O)NCCC(=O)O  3.259637   \n",
       "1       NC(CCC(=O)NC(CSCc1ccccc1)C(=O)NC(C(=O)O)c1cccc...  6.376751   \n",
       "2       Cc1ccc(CSCC(NC(=O)CCC(N)C(=O)O)C(=O)NCCC(=O)O)cc1  4.397940   \n",
       "3       NC(CCC(=O)NC(CSCc1ccc(Cl)cc1)C(=O)NC(C(=O)O)c1...  6.920819   \n",
       "4          NC(CCC(=O)NC(CSCc1ccccc1)C(=O)NCCC(=O)O)C(=O)O  3.148742   \n",
       "...                                                   ...       ...   \n",
       "406083  CC1CCCC(N2NC(C(C)(C)C)C[C@@H]2N[C@@H](O)NC2CCC...       NaN   \n",
       "406084  OC[C@H]1O[C@H](O[PH](O)(O)O[PH](O)(O)OC[C@H]2O...       NaN   \n",
       "406085  C[C@H]1S[C@H]2NC(N)N[C@@H](O)[C@@H]2C1SC1CCC(C...       NaN   \n",
       "406086                        CCCCCCC1CCC(C(O)NNC(S)NC)O1       NaN   \n",
       "406087   CCOC(O)C1NNN2C3CCCCC3C(S[C@H](C(C)O)C(O)OCC)NC12       NaN   \n",
       "\n",
       "         source_file  is_experimental  resolution  pEC50  pKd (Wang, FEP)  \\\n",
       "0       BindingNetv2            False         NaN    NaN              NaN   \n",
       "1       BindingNetv2            False         NaN    NaN              NaN   \n",
       "2       BindingNetv2            False         NaN    NaN              NaN   \n",
       "3       BindingNetv2            False         NaN    NaN              NaN   \n",
       "4       BindingNetv2            False         NaN    NaN              NaN   \n",
       "...              ...              ...         ...    ...              ...   \n",
       "406083       BioLip2             True         NaN    NaN              NaN   \n",
       "406084       BioLip2             True         NaN    NaN              NaN   \n",
       "406085       BioLip2             True         NaN    NaN              NaN   \n",
       "406086       BioLip2             True         NaN    NaN              NaN   \n",
       "406087       BioLip2             True         NaN    NaN              NaN   \n",
       "\n",
       "             pKd  ...  LE_potency LEnorm_potency        LE   LE_norm  \\\n",
       "0            NaN  ...        None           None  0.120727  0.000298   \n",
       "1            NaN  ...        None           None  0.193235  0.000409   \n",
       "2            NaN  ...        None           None  0.151653  0.000357   \n",
       "3            NaN  ...        None           None  0.203553  0.000401   \n",
       "4            NaN  ...        None           None  0.112455  0.000274   \n",
       "...          ...  ...         ...            ...       ...       ...   \n",
       "406083  7.130768  ...        None           None  0.187652  0.000351   \n",
       "406084  3.124939  ...        None           None  0.086804  0.000151   \n",
       "406085  7.698970  ...        None           None  0.248354  0.000515   \n",
       "406086  4.096910  ...        None           None  0.215627  0.000740   \n",
       "406087  3.920819  ...        None           None  0.140029  0.000333   \n",
       "\n",
       "       carbon_count sequence_id           processed_inchikey inchikey_hash  \\\n",
       "0                17         2ƘC  RILVFYFKIDXJNY-UHFFFAOYSA-M      2f8150df   \n",
       "1                23         2ƘC  ZPSKWMFLCHMEOY-UHFFFAOYSA-M      71a014c7   \n",
       "2                19         2ƉY  MBXWAPNNAOGFPH-UHFFFAOYSA-M      1cba16d5   \n",
       "3                24         2ƘC  BXJSPWKYSSRFEB-UHFFFAOYSA-M      a823c50d   \n",
       "4                18         2ŘN  QLVGMERIDWMEBM-UHFFFAOYSA-M      46313ec9   \n",
       "...             ...         ...                          ...           ...   \n",
       "406083           29          ¦Q  NACQWABIDVXOMV-VFCRXLDWSA-N      b325f1a8   \n",
       "406084           15          ĔV  PEEYOHTXAULSGT-KPLOLNPKSA-N      ba696fe5   \n",
       "406085           19          ȚS  VYTCQXDFOQVDLB-WZBLSFTPSA-N      3c3e05eb   \n",
       "406086           13          ǋO  QDYXIFNPTRANPQ-UHFFFAOYSA-N      217020ae   \n",
       "406087           18          ŞK  NZHNUIAGNWEXIG-BICOIMHRSA-N      9d8f902a   \n",
       "\n",
       "       has_valid_inchikey                       complex_id  \n",
       "0                    True  RILVFYFKIDXJNY-UHFFFAOYSA-M_2ƘC  \n",
       "1                    True  ZPSKWMFLCHMEOY-UHFFFAOYSA-M_2ƘC  \n",
       "2                    True  MBXWAPNNAOGFPH-UHFFFAOYSA-M_2ƉY  \n",
       "3                    True  BXJSPWKYSSRFEB-UHFFFAOYSA-M_2ƘC  \n",
       "4                    True  QLVGMERIDWMEBM-UHFFFAOYSA-M_2ŘN  \n",
       "...                   ...                              ...  \n",
       "406083               True   NACQWABIDVXOMV-VFCRXLDWSA-N_¦Q  \n",
       "406084               True   PEEYOHTXAULSGT-KPLOLNPKSA-N_ĔV  \n",
       "406085               True   VYTCQXDFOQVDLB-WZBLSFTPSA-N_ȚS  \n",
       "406086               True   QDYXIFNPTRANPQ-UHFFFAOYSA-N_ǋO  \n",
       "406087               True   NZHNUIAGNWEXIG-BICOIMHRSA-N_ŞK  \n",
       "\n",
       "[406088 rows x 47 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9badf66-a0a7-442f-882f-e69114df3cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 395836 unique complexes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating complexes: 100%|██████████| 395836/395836 [01:30<00:00, 4389.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete: 406088 -> 395836 rows\n",
      "Removed 10252 duplicate rows (2.52% reduction)\n",
      "\n",
      "============================================================\n",
      "PROCESSING COMPLETE\n",
      "============================================================\n",
      "Final dataframe shape: (395836, 48)\n",
      "Removed 10252 duplicate rows\n",
      "Reduction: 2.52%\n"
     ]
    }
   ],
   "source": [
    "final_df = parallel_remove_duplicates(df, cpu_count())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final dataframe shape: {final_df.shape}\")\n",
    "print(f\"Removed {len(df) - len(final_df)} duplicate rows\")\n",
    "print(f\"Reduction: {(1 - len(final_df)/len(df))*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b61c3a4-c6c6-468e-9989-ac5117f7859d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "\n",
      "Original dataset:\n",
      "  - Total rows: 406,088\n",
      "  - Unique proteins: 406,088\n",
      "  - Unique ligands: 306,191\n",
      "\n",
      "Processed dataset:\n",
      "  - Total rows: 395,836\n",
      "  - Rows removed: 10,252\n",
      "  - Reduction: 2.52%\n",
      "\n",
      "Merging statistics:\n",
      "  - 388,888 complexes kept as-is\n",
      "  - 5,986 complexes merged from 2 sources\n",
      "  - 558 complexes merged from 3 sources\n",
      "  - 185 complexes merged from 4 sources\n",
      "  - 40 complexes merged from 5 sources\n",
      "  - 40 complexes merged from 6 sources\n",
      "  - 18 complexes merged from 7 sources\n",
      "  - 24 complexes merged from 8 sources\n",
      "  - 15 complexes merged from 9 sources\n",
      "  - 6 complexes merged from 10 sources\n",
      "  - 2 complexes merged from 11 sources\n",
      "  - 12 complexes merged from 12 sources\n",
      "  - 9 complexes merged from 13 sources\n",
      "  - 7 complexes merged from 14 sources\n",
      "  - 2 complexes merged from 15 sources\n",
      "  - 2 complexes merged from 16 sources\n",
      "  - 5 complexes merged from 17 sources\n",
      "  - 2 complexes merged from 18 sources\n",
      "  - 2 complexes merged from 19 sources\n",
      "  - 1 complexes merged from 20 sources\n",
      "  - 3 complexes merged from 21 sources\n",
      "  - 1 complexes merged from 22 sources\n",
      "  - 3 complexes merged from 23 sources\n",
      "  - 4 complexes merged from 24 sources\n",
      "  - 2 complexes merged from 27 sources\n",
      "  - 2 complexes merged from 29 sources\n",
      "  - 1 complexes merged from 31 sources\n",
      "  - 1 complexes merged from 32 sources\n",
      "  - 1 complexes merged from 33 sources\n",
      "  - 2 complexes merged from 34 sources\n",
      "  - 1 complexes merged from 35 sources\n",
      "  - 1 complexes merged from 36 sources\n",
      "  - 1 complexes merged from 38 sources\n",
      "  - 1 complexes merged from 40 sources\n",
      "  - 1 complexes merged from 44 sources\n",
      "  - 1 complexes merged from 47 sources\n",
      "  - 1 complexes merged from 50 sources\n",
      "  - 1 complexes merged from 70 sources\n",
      "  - 1 complexes merged from 74 sources\n",
      "  - 1 complexes merged from 79 sources\n",
      "  - 1 complexes merged from 99 sources\n",
      "  - 1 complexes merged from 101 sources\n",
      "\n",
      "Response value coverage:\n",
      "  - pKd (Wang, FEP): 0.07% coverage (262 values)\n",
      "  - pKd: 4.07% coverage (16,114 values)\n",
      "  - pIC50: 67.48% coverage (267,096 values)\n",
      "  - potency: 0.00% coverage (0 values)\n",
      "  - pEC50: 4.61% coverage (18,266 values)\n",
      "  - pKi: 24.11% coverage (95,436 values)\n",
      "\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seq_mapping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m print_summary_statistics(df, final_df)\n\u001b[1;32m      3\u001b[0m final_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeduplicated_complexes_parallel.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mseq_mapping\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence_mapping.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use: final_df, seq_mapping = process_dataframe_parallel(df)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq_mapping' is not defined"
     ]
    }
   ],
   "source": [
    "print_summary_statistics(df, final_df)\n",
    "\n",
    "final_df.to_csv('deduplicated_complexes_parallel.csv', index=False)\n",
    "seq_mapping.to_csv('sequence_mapping.csv', index=False)\n",
    "    \n",
    "print(\"Pipeline loaded successfully!\")\n",
    "print(\"To use: final_df, seq_mapping = process_dataframe_parallel(df)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3dfd00d-6199-4932-9839-7fef17134676",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_pdb_path</th>\n",
       "      <th>ligand_sdf_path</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pKi</th>\n",
       "      <th>source_file</th>\n",
       "      <th>is_experimental</th>\n",
       "      <th>resolution</th>\n",
       "      <th>pEC50</th>\n",
       "      <th>pKd (Wang, FEP)</th>\n",
       "      <th>pKd</th>\n",
       "      <th>...</th>\n",
       "      <th>LEnorm_potency</th>\n",
       "      <th>LE</th>\n",
       "      <th>LE_norm</th>\n",
       "      <th>carbon_count</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>processed_inchikey</th>\n",
       "      <th>inchikey_hash</th>\n",
       "      <th>has_valid_inchikey</th>\n",
       "      <th>num_merged</th>\n",
       "      <th>merged_from_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.167298</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>33</td>\n",
       "      <td>QS</td>\n",
       "      <td>AAAAZQPHATYWOK-JXMROGBWSA-O</td>\n",
       "      <td>4f129304</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.156415</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>33</td>\n",
       "      <td>ãK</td>\n",
       "      <td>AAAAZQPHATYWOK-JXMROGBWSA-O</td>\n",
       "      <td>4f129304</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL258...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL258...</td>\n",
       "      <td>Cc1ccc(O)cc1Nc1cc(N2CCOCC2)nc(-n2cnc3ccccc32)n1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.228776</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>22</td>\n",
       "      <td>ȭG</td>\n",
       "      <td>AAABTPAECTZDET-UHFFFAOYSA-N</td>\n",
       "      <td>62568322</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL297...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL297...</td>\n",
       "      <td>COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.264950</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>21</td>\n",
       "      <td>ǱN</td>\n",
       "      <td>AAACGYYPWMUUFL-LJQANCHMSA-N</td>\n",
       "      <td>b8ce4f00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL323...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL323...</td>\n",
       "      <td>COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.236761</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>21</td>\n",
       "      <td>ȍV</td>\n",
       "      <td>AAACGYYPWMUUFL-LJQANCHMSA-N</td>\n",
       "      <td>b8ce4f00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395831</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.070581</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.191911</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>6</td>\n",
       "      <td>ÆIăS</td>\n",
       "      <td></td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395832</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NC(=O)C1:N:C(C2:C([H]):N:N(C([H])([H]...</td>\n",
       "      <td>6.873234</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.242082</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>16</td>\n",
       "      <td>ôX</td>\n",
       "      <td></td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>PDBbind2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395833</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NC(=O)C1:N:C(Cl):C(N2C([H])([H])C([H]...</td>\n",
       "      <td>5.867740</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.279416</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>13</td>\n",
       "      <td>ôY</td>\n",
       "      <td></td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>PDBbind2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395834</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NC(=O)C1:N:C(Cl):C(N([H])[H]):N:C:1N(...</td>\n",
       "      <td>5.275724</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>2.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.351715</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>7</td>\n",
       "      <td>õB</td>\n",
       "      <td></td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395835</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.889410</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>0.305588</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>6</td>\n",
       "      <td>ƲW</td>\n",
       "      <td></td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395836 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_pdb_path  \\\n",
       "0       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "1       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2       ../data/raw/BindingNetv2/high/target_CHEMBL258...   \n",
       "3       ../data/raw/BindingNetv2/high/target_CHEMBL297...   \n",
       "4       ../data/raw/BindingNetv2/high/target_CHEMBL323...   \n",
       "...                                                   ...   \n",
       "395831  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395832  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395833  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395834  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395835  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "\n",
       "                                          ligand_sdf_path  \\\n",
       "0       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "1       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2       ../data/raw/BindingNetv2/high/target_CHEMBL258...   \n",
       "3       ../data/raw/BindingNetv2/high/target_CHEMBL297...   \n",
       "4       ../data/raw/BindingNetv2/high/target_CHEMBL323...   \n",
       "...                                                   ...   \n",
       "395831  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395832  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395833  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395834  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395835  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "\n",
       "                                                   smiles       pKi  \\\n",
       "0       CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...       NaN   \n",
       "1       CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...       NaN   \n",
       "2         Cc1ccc(O)cc1Nc1cc(N2CCOCC2)nc(-n2cnc3ccccc32)n1       NaN   \n",
       "3       COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...       NaN   \n",
       "4       COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...       NaN   \n",
       "...                                                   ...       ...   \n",
       "395831  [H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...       NaN   \n",
       "395832  [H]/N=C(\\NC(=O)C1:N:C(C2:C([H]):N:N(C([H])([H]...  6.873234   \n",
       "395833  [H]/N=C(\\NC(=O)C1:N:C(Cl):C(N2C([H])([H])C([H]...  5.867740   \n",
       "395834  [H]/N=C(\\NC(=O)C1:N:C(Cl):C(N([H])[H]):N:C:1N(...  5.275724   \n",
       "395835  [H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...       NaN   \n",
       "\n",
       "         source_file  is_experimental  resolution  pEC50  pKd (Wang, FEP)  \\\n",
       "0       BindingNetv2            False         NaN    NaN              NaN   \n",
       "1       BindingNetv2            False         NaN    NaN              NaN   \n",
       "2       BindingNetv2            False         NaN    NaN              NaN   \n",
       "3       BindingNetv2            False         NaN    NaN              NaN   \n",
       "4       BindingNetv2            False         NaN    NaN              NaN   \n",
       "...              ...              ...         ...    ...              ...   \n",
       "395831   PDBbind2020             True        1.85    NaN              NaN   \n",
       "395832   PDBbind2020             True        1.90    NaN              NaN   \n",
       "395833   PDBbind2020             True        1.70    NaN              NaN   \n",
       "395834   PDBbind2020             True        2.10    NaN              NaN   \n",
       "395835   PDBbind2020             True        1.60    NaN              NaN   \n",
       "\n",
       "             pKd  ...  LEnorm_potency        LE   LE_norm carbon_count  \\\n",
       "0            NaN  ...            None  0.167298  0.000272           33   \n",
       "1            NaN  ...            None  0.156415  0.000255           33   \n",
       "2            NaN  ...            None  0.228776  0.000568           22   \n",
       "3            NaN  ...            None  0.264950  0.000729           21   \n",
       "4            NaN  ...            None  0.236761  0.000651           21   \n",
       "...          ...  ...             ...       ...       ...          ...   \n",
       "395831  3.070581  ...            None  0.191911  0.000755            6   \n",
       "395832       NaN  ...            None  0.242082  0.000679           16   \n",
       "395833       NaN  ...            None  0.279416  0.000899           13   \n",
       "395834       NaN  ...            None  0.351715  0.001538            7   \n",
       "395835  4.889410  ...            None  0.305588  0.001202            6   \n",
       "\n",
       "       sequence_id           processed_inchikey inchikey_hash  \\\n",
       "0               QS  AAAAZQPHATYWOK-JXMROGBWSA-O      4f129304   \n",
       "1               ãK  AAAAZQPHATYWOK-JXMROGBWSA-O      4f129304   \n",
       "2               ȭG  AAABTPAECTZDET-UHFFFAOYSA-N      62568322   \n",
       "3               ǱN  AAACGYYPWMUUFL-LJQANCHMSA-N      b8ce4f00   \n",
       "4               ȍV  AAACGYYPWMUUFL-LJQANCHMSA-N      b8ce4f00   \n",
       "...            ...                          ...           ...   \n",
       "395831        ÆIăS                                   d41d8cd9   \n",
       "395832          ôX                                   d41d8cd9   \n",
       "395833          ôY                                   d41d8cd9   \n",
       "395834          õB                                   d41d8cd9   \n",
       "395835          ƲW                                   d41d8cd9   \n",
       "\n",
       "       has_valid_inchikey num_merged  merged_from_sources  \n",
       "0                    True          1                  NaN  \n",
       "1                    True          1                  NaN  \n",
       "2                    True          1                  NaN  \n",
       "3                    True          1                  NaN  \n",
       "4                    True          1                  NaN  \n",
       "...                   ...        ...                  ...  \n",
       "395831               True          1                  NaN  \n",
       "395832               True          6          PDBbind2020  \n",
       "395833               True          2          PDBbind2020  \n",
       "395834               True          1                  NaN  \n",
       "395835               True          1                  NaN  \n",
       "\n",
       "[395836 rows x 48 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419e2f92-8f3d-4a02-bd49-1f0f1da3e846",
   "metadata": {},
   "source": [
    "# Task range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8334fa7-4eec-47bd-a539-3a4daef8ba7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"deduplicated_complexes_parallel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d080baa-0acd-43a6-b4fe-0ea2bc31a386",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>protein_pdb_path</th>\n",
       "      <th>ligand_sdf_path</th>\n",
       "      <th>smiles</th>\n",
       "      <th>pKi</th>\n",
       "      <th>source_file</th>\n",
       "      <th>is_experimental</th>\n",
       "      <th>resolution</th>\n",
       "      <th>pEC50</th>\n",
       "      <th>pKd (Wang, FEP)</th>\n",
       "      <th>pKd</th>\n",
       "      <th>...</th>\n",
       "      <th>LEnorm_potency</th>\n",
       "      <th>LE</th>\n",
       "      <th>LE_norm</th>\n",
       "      <th>carbon_count</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>processed_inchikey</th>\n",
       "      <th>inchikey_hash</th>\n",
       "      <th>has_valid_inchikey</th>\n",
       "      <th>num_merged</th>\n",
       "      <th>merged_from_sources</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.167298</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>33</td>\n",
       "      <td>QS</td>\n",
       "      <td>AAAAZQPHATYWOK-JXMROGBWSA-O</td>\n",
       "      <td>4f129304</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>../data/raw/BindingNetv2/moderate/target_CHEMB...</td>\n",
       "      <td>CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.156415</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>33</td>\n",
       "      <td>ãK</td>\n",
       "      <td>AAAAZQPHATYWOK-JXMROGBWSA-O</td>\n",
       "      <td>4f129304</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL258...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL258...</td>\n",
       "      <td>Cc1ccc(O)cc1Nc1cc(N2CCOCC2)nc(-n2cnc3ccccc32)n1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.228776</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>22</td>\n",
       "      <td>ȭG</td>\n",
       "      <td>AAABTPAECTZDET-UHFFFAOYSA-N</td>\n",
       "      <td>62568322</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL297...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL297...</td>\n",
       "      <td>COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.264950</td>\n",
       "      <td>0.000729</td>\n",
       "      <td>21</td>\n",
       "      <td>ǱN</td>\n",
       "      <td>AAACGYYPWMUUFL-LJQANCHMSA-N</td>\n",
       "      <td>b8ce4f00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL323...</td>\n",
       "      <td>../data/raw/BindingNetv2/high/target_CHEMBL323...</td>\n",
       "      <td>COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BindingNetv2</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.236761</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>21</td>\n",
       "      <td>ȍV</td>\n",
       "      <td>AAACGYYPWMUUFL-LJQANCHMSA-N</td>\n",
       "      <td>b8ce4f00</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395831</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.070581</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.191911</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>6</td>\n",
       "      <td>ÆIăS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395832</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NC(=O)C1:N:C(C2:C([H]):N:N(C([H])([H]...</td>\n",
       "      <td>6.873234</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.242082</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>16</td>\n",
       "      <td>ôX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "      <td>PDBbind2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395833</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NC(=O)C1:N:C(Cl):C(N2C([H])([H])C([H]...</td>\n",
       "      <td>5.867740</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.279416</td>\n",
       "      <td>0.000899</td>\n",
       "      <td>13</td>\n",
       "      <td>ôY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>PDBbind2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395834</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NC(=O)C1:N:C(Cl):C(N([H])[H]):N:C:1N(...</td>\n",
       "      <td>5.275724</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>2.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.351715</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>7</td>\n",
       "      <td>õB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395835</th>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>../data/raw/PDBbind2020/PDBbind2020/main/refin...</td>\n",
       "      <td>[H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>PDBbind2020</td>\n",
       "      <td>True</td>\n",
       "      <td>1.60</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.889410</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.305588</td>\n",
       "      <td>0.001202</td>\n",
       "      <td>6</td>\n",
       "      <td>ƲW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d41d8cd9</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>395836 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         protein_pdb_path  \\\n",
       "0       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "1       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2       ../data/raw/BindingNetv2/high/target_CHEMBL258...   \n",
       "3       ../data/raw/BindingNetv2/high/target_CHEMBL297...   \n",
       "4       ../data/raw/BindingNetv2/high/target_CHEMBL323...   \n",
       "...                                                   ...   \n",
       "395831  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395832  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395833  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395834  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395835  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "\n",
       "                                          ligand_sdf_path  \\\n",
       "0       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "1       ../data/raw/BindingNetv2/moderate/target_CHEMB...   \n",
       "2       ../data/raw/BindingNetv2/high/target_CHEMBL258...   \n",
       "3       ../data/raw/BindingNetv2/high/target_CHEMBL297...   \n",
       "4       ../data/raw/BindingNetv2/high/target_CHEMBL323...   \n",
       "...                                                   ...   \n",
       "395831  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395832  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395833  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395834  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "395835  ../data/raw/PDBbind2020/PDBbind2020/main/refin...   \n",
       "\n",
       "                                                   smiles       pKi  \\\n",
       "0       CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...       NaN   \n",
       "1       CCOc1cc2ncc(C#N)c(Nc3ccc(OCc4nc5ccccc5s4)c(Cl)...       NaN   \n",
       "2         Cc1ccc(O)cc1Nc1cc(N2CCOCC2)nc(-n2cnc3ccccc32)n1       NaN   \n",
       "3       COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...       NaN   \n",
       "4       COC(=O)C[C@@H](NC(=O)c1ccc(-c2cn[nH]c2)c(C)c1)...       NaN   \n",
       "...                                                   ...       ...   \n",
       "395831  [H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...       NaN   \n",
       "395832  [H]/N=C(\\NC(=O)C1:N:C(C2:C([H]):N:N(C([H])([H]...  6.873234   \n",
       "395833  [H]/N=C(\\NC(=O)C1:N:C(Cl):C(N2C([H])([H])C([H]...  5.867740   \n",
       "395834  [H]/N=C(\\NC(=O)C1:N:C(Cl):C(N([H])[H]):N:C:1N(...  5.275724   \n",
       "395835  [H]/N=C(\\NP(=O)(O[H])O[H])N([H])C([H])([H])C([...       NaN   \n",
       "\n",
       "         source_file  is_experimental  resolution  pEC50  pKd (Wang, FEP)  \\\n",
       "0       BindingNetv2            False         NaN    NaN              NaN   \n",
       "1       BindingNetv2            False         NaN    NaN              NaN   \n",
       "2       BindingNetv2            False         NaN    NaN              NaN   \n",
       "3       BindingNetv2            False         NaN    NaN              NaN   \n",
       "4       BindingNetv2            False         NaN    NaN              NaN   \n",
       "...              ...              ...         ...    ...              ...   \n",
       "395831   PDBbind2020             True        1.85    NaN              NaN   \n",
       "395832   PDBbind2020             True        1.90    NaN              NaN   \n",
       "395833   PDBbind2020             True        1.70    NaN              NaN   \n",
       "395834   PDBbind2020             True        2.10    NaN              NaN   \n",
       "395835   PDBbind2020             True        1.60    NaN              NaN   \n",
       "\n",
       "             pKd  ...  LEnorm_potency        LE   LE_norm  carbon_count  \\\n",
       "0            NaN  ...             NaN  0.167298  0.000272            33   \n",
       "1            NaN  ...             NaN  0.156415  0.000255            33   \n",
       "2            NaN  ...             NaN  0.228776  0.000568            22   \n",
       "3            NaN  ...             NaN  0.264950  0.000729            21   \n",
       "4            NaN  ...             NaN  0.236761  0.000651            21   \n",
       "...          ...  ...             ...       ...       ...           ...   \n",
       "395831  3.070581  ...             NaN  0.191911  0.000755             6   \n",
       "395832       NaN  ...             NaN  0.242082  0.000679            16   \n",
       "395833       NaN  ...             NaN  0.279416  0.000899            13   \n",
       "395834       NaN  ...             NaN  0.351715  0.001538             7   \n",
       "395835  4.889410  ...             NaN  0.305588  0.001202             6   \n",
       "\n",
       "       sequence_id           processed_inchikey inchikey_hash  \\\n",
       "0               QS  AAAAZQPHATYWOK-JXMROGBWSA-O      4f129304   \n",
       "1               ãK  AAAAZQPHATYWOK-JXMROGBWSA-O      4f129304   \n",
       "2               ȭG  AAABTPAECTZDET-UHFFFAOYSA-N      62568322   \n",
       "3               ǱN  AAACGYYPWMUUFL-LJQANCHMSA-N      b8ce4f00   \n",
       "4               ȍV  AAACGYYPWMUUFL-LJQANCHMSA-N      b8ce4f00   \n",
       "...            ...                          ...           ...   \n",
       "395831        ÆIăS                          NaN      d41d8cd9   \n",
       "395832          ôX                          NaN      d41d8cd9   \n",
       "395833          ôY                          NaN      d41d8cd9   \n",
       "395834          õB                          NaN      d41d8cd9   \n",
       "395835          ƲW                          NaN      d41d8cd9   \n",
       "\n",
       "        has_valid_inchikey num_merged  merged_from_sources  \n",
       "0                     True          1                  NaN  \n",
       "1                     True          1                  NaN  \n",
       "2                     True          1                  NaN  \n",
       "3                     True          1                  NaN  \n",
       "4                     True          1                  NaN  \n",
       "...                    ...        ...                  ...  \n",
       "395831                True          1                  NaN  \n",
       "395832                True          6          PDBbind2020  \n",
       "395833                True          2          PDBbind2020  \n",
       "395834                True          1                  NaN  \n",
       "395835                True          1                  NaN  \n",
       "\n",
       "[395836 rows x 48 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea7aa09-6da5-4337-a68e-2d3e978504c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ranges for weighting:\n",
      "  pKi: range=10.00, weight=0.0627\n",
      "  pEC50: range=9.95, weight=0.0629\n",
      "  pKd (Wang, FEP): range=4.90, weight=0.1278\n",
      "  pKd: range=10.92, weight=0.0574\n",
      "  pIC50: range=10.00, weight=0.0627\n",
      "  potency: range=1.00, weight=0.6266\n",
      "\n",
      "Task Statistics:\n",
      "pKi:\n",
      "  Count: 95436\n",
      "  Mean: 7.02\n",
      "  Std: 1.43\n",
      "  Range: [3.00, 13.00]\n",
      "pEC50:\n",
      "  Count: 18266\n",
      "  Mean: 6.76\n",
      "  Std: 1.30\n",
      "  Range: [3.05, 13.00]\n",
      "pKd (Wang, FEP):\n",
      "  Count: 262\n",
      "  Mean: 6.88\n",
      "  Std: 1.00\n",
      "  Range: [4.24, 9.14]\n",
      "pKd:\n",
      "  Count: 16114\n",
      "  Mean: 6.52\n",
      "  Std: 1.53\n",
      "  Range: [3.00, 13.92]\n",
      "pIC50:\n",
      "  Count: 267096\n",
      "  Mean: 6.81\n",
      "  Std: 1.31\n",
      "  Range: [3.00, 13.00]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate task ranges for weighting\n",
    "task_ranges = prepare_mtl_experiment(df, CONFIG['task_cols'])\n",
    "\n",
    "# Create task statistics\n",
    "print(\"\\nTask Statistics:\")\n",
    "for task in CONFIG['task_cols']:\n",
    "    if task in df.columns:\n",
    "        valid_values = df[task].dropna()\n",
    "        if len(valid_values) > 0:\n",
    "            print(f\"{task}:\")\n",
    "            print(f\"  Count: {len(valid_values)}\")\n",
    "            print(f\"  Mean: {valid_values.mean():.2f}\")\n",
    "            print(f\"  Std: {valid_values.std():.2f}\")\n",
    "            print(f\"  Range: [{valid_values.min():.2f}, {valid_values.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c173037-958c-44b0-8ac5-9b5b63878c01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../input/chunk/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG['structure_chunks_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e217515f-9345-4b00-96a7-e566d7d27a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import RDLogger\n",
    "\n",
    "# Disable RDKit warnings\n",
    "RDLogger.DisableLog('rdApp.*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a69a2447-4ebf-43f1-a9d6-b8bc41d61d21",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pKi', 'pEC50', 'pKd (Wang, FEP)', 'pKd', 'pIC50', 'potency']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG['task_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e001a15e-e15e-4db1-bd06-090da92f947b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOStream.flush timed out\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create chunk loader\n",
    "chunk_loader = StructureChunkLoader(\n",
    "    chunk_dir=CONFIG['structure_chunks_dir'],\n",
    "    cache_size=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4eb8f2-503b-4753-a457-829fc0adcf25",
   "metadata": {},
   "source": [
    "# 4 : Cross-Validation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3a40a55-84f4-4e1e-a353-6bd2bc471f22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 9: Cross-Validation Training\n",
    "# Initialize cross-validator\n",
    "cv = CrossValidator(\n",
    "    model_config=CONFIG['model_config'],\n",
    "    task_cols=CONFIG['task_cols'],\n",
    "    task_ranges=task_ranges,\n",
    "    n_folds=CONFIG['training_config']['n_folds'],\n",
    "    batch_size=CONFIG['training_config']['batch_size'],\n",
    "    n_epochs=CONFIG['training_config']['n_epochs'],\n",
    "    learning_rate=CONFIG['training_config']['learning_rate'],\n",
    "    patience=CONFIG['training_config']['patience'],\n",
    "    device=device,\n",
    "    seed=SEED\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba8c037-a360-466f-84f5-ec502f9248c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Run cross-validation\n",
    "print(\"\\nStarting cross-validation...\")\n",
    "cv_results = cv.run(df, chunk_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab16aa3-d5fc-4c7a-b9ad-6624c4b90910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to clear memory between each fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73369947-19e7-4721-b781-46b6bd0892bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100-200 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184011d6-6c33-4aec-bfec-44af316fa99d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print summary\n",
    "cv.print_summary()\n",
    "\n",
    "# Save CV results\n",
    "save_results(\n",
    "    cv_results, \n",
    "    os.path.join(CONFIG['experiment_dir'], 'results', 'cv_results.pkl'),\n",
    "    format='pickle'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e83cf3-2ca5-498d-bac1-ecae09e61289",
   "metadata": {},
   "source": [
    "# 4 : Full Training on All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32aea5c-a3d8-4fc1-85ce-b91951db3558",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create train/valid/test splits\n",
    "from gnn_dta_mtl.datasets import create_data_splits\n",
    "\n",
    "splits = create_data_splits(\n",
    "    df,\n",
    "    split_method='random',  # or 'scaffold', 'protein', 'drug'\n",
    "    split_frac=[0.7, 0.1, 0.2],\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "df_train = splits['train']\n",
    "df_valid = splits['valid']\n",
    "df_test = splits['test']\n",
    "\n",
    "print(f\"Train: {len(df_train)}, Valid: {len(df_valid)}, Test: {len(df_test)}\")\n",
    "\n",
    "# Build datasets\n",
    "train_dataset = build_mtl_dataset_optimized(df_train, chunk_loader, CONFIG['task_cols'])\n",
    "valid_dataset = build_mtl_dataset_optimized(df_valid, chunk_loader, CONFIG['task_cols'])\n",
    "test_dataset = build_mtl_dataset_optimized(df_test, chunk_loader, CONFIG['task_cols'])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch_geometric.loader.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['training_config']['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "valid_loader = torch_geometric.loader.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CONFIG['training_config']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch_geometric.loader.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG['training_config']['batch_size'],\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf773c48-f518-4184-a303-ecfb3b263efa",
   "metadata": {
    "tags": []
   },
   "source": [
    "172 Go pour la totalitédu set\n",
    "\n",
    "(monter à 1000 Go ?)... il faut aussi test les GPUs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cba45f-3022-46c1-a746-d79601a50636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f541c2-ec63-4854-b1ce-f4b90f0f7802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da694662-0722-4c66-9336-27b24c52eb67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c14f4a-f41c-4af1-8b5c-abba0725c3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fa45a-0a17-43c2-814b-1d0107c6a5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b4aee7-e441-4e02-af23-4cefe4b36c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380eff80-085c-46bd-83fa-9a41f53c9ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb2970b-6a00-44b1-a7c2-69dbea017bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "51a93b99-a0ca-4361-9c9b-3eee31d37af1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create model\n",
    "model = MTL_DTAModel(\n",
    "    task_names=CONFIG['task_cols'],\n",
    "    **CONFIG['model_config']\n",
    ").to(device)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = MTLTrainer(\n",
    "    model=model,\n",
    "    task_cols=CONFIG['task_cols'],\n",
    "    task_ranges=task_ranges,\n",
    "    device=device,\n",
    "    learning_rate=CONFIG['training_config']['learning_rate']\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"\\nTraining model...\")\n",
    "history = trainer.train(\n",
    "    train_loader,\n",
    "    valid_loader,\n",
    "    n_epochs=CONFIG['training_config']['n_epochs'],\n",
    "    patience=CONFIG['training_config']['patience'],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save model\n",
    "save_model(\n",
    "    model,\n",
    "    os.path.join(CONFIG['experiment_dir'], 'models', 'final_model.pt'),\n",
    "    metrics=history\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5d867c82-02ce-4722-afd8-81d91de5d7a8",
   "metadata": {},
   "source": [
    "Epoch 42/250\n",
    "  Train Loss: 0.0798\n",
    "  Valid Loss: 0.0684\n",
    "  pKi: RMSE=1.169, R²=0.628\n",
    "  pEC50: RMSE=1.080, R²=0.620\n",
    "  pKd (Wang, FEP): RMSE=0.411, R²=0.814\n",
    "  pKd: RMSE=1.120, R²=0.614\n",
    "  pIC50: RMSE=1.061, R²=0.396\n",
    "Training:  39%|███▉      | 25/64 [00:08<00:12,  3.01it/s]\n",
    "\n",
    "Epoch 56/250\n",
    "  Train Loss: 0.0704\n",
    "  Valid Loss: 0.0739\n",
    "  pKi: RMSE=1.182, R²=0.619\n",
    "  pEC50: RMSE=1.086, R²=0.615\n",
    "  pKd (Wang, FEP): RMSE=0.380, R²=0.841\n",
    "  pKd: RMSE=1.147, R²=0.595\n",
    "  pIC50: RMSE=1.136, R²=0.308\n",
    "Training:  34%|███▍      | 22/64 [00:07<00:14,  2.90it/s]\n",
    "\n",
    "\n",
    "Epoch 66/250\n",
    "  Train Loss: 0.0613\n",
    "  Valid Loss: 0.0662\n",
    "  pKi: RMSE=1.147, R²=0.642\n",
    "  pEC50: RMSE=0.925, R²=0.721\n",
    "  pKd (Wang, FEP): RMSE=0.412, R²=0.813\n",
    "  pKd: RMSE=1.104, R²=0.624\n",
    "  pIC50: RMSE=0.980, R²=0.485\n",
    "\n",
    "    \n",
    "Epoch 75/250\n",
    "  Train Loss: 0.0561\n",
    "  Valid Loss: 0.0682\n",
    "  pKi: RMSE=1.206, R²=0.604\n",
    "  pEC50: RMSE=0.940, R²=0.712\n",
    "  pKd (Wang, FEP): RMSE=0.424, R²=0.802\n",
    "  pKd: RMSE=1.127, R²=0.609\n",
    "  pIC50: RMSE=0.949, R²=0.516\n",
    "    \n",
    "    \n",
    "Epoch 132/250\n",
    "  Train Loss: 0.0288\n",
    "  Valid Loss: 0.0593\n",
    "  pKi: RMSE=1.078, R²=0.683\n",
    "  pEC50: RMSE=1.014, R²=0.665\n",
    "  pKd (Wang, FEP): RMSE=0.310, R²=0.894\n",
    "  pKd: RMSE=1.062, R²=0.652\n",
    "  pIC50: RMSE=0.914, R²=0.551\n",
    "\n",
    "Epoch 157/250\n",
    "  Train Loss: 0.0224\n",
    "  Valid Loss: 0.0596\n",
    "  pKi: RMSE=1.066, R²=0.690\n",
    "  pEC50: RMSE=0.950, R²=0.706\n",
    "  pKd (Wang, FEP): RMSE=0.229, R²=0.942\n",
    "  pKd: RMSE=1.105, R²=0.624\n",
    "  pIC50: RMSE=0.906, R²=0.560"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05895f85-3fef-451a-8876-848818d70d39",
   "metadata": {},
   "source": [
    "Epoch 14/200\n",
    "  Train Loss: 0.1062\n",
    "  Valid Loss: 0.0827\n",
    "  pKi: RMSE=1.275, R²=0.557\n",
    "  pEC50: RMSE=1.432, R²=0.332\n",
    "  pKd (Wang, FEP): RMSE=0.417, R²=0.808\n",
    "  pKd: RMSE=1.162, R²=0.584\n",
    "  pIC50: RMSE=1.144, R²=0.298\n",
    "\n",
    " Epoch 52/200\n",
    "  Train Loss: 0.0387\n",
    "  Valid Loss: 0.0623\n",
    "  pKi: RMSE=1.071, R²=0.687\n",
    "  pEC50: RMSE=0.956, R²=0.702\n",
    "  pKd (Wang, FEP): RMSE=0.338, R²=0.875\n",
    "  pKd: RMSE=1.088, R²=0.635\n",
    "  pIC50: RMSE=0.973, R²=0.492\n",
    "\n",
    "\n",
    "Epoch 55/200\n",
    "  Train Loss: 0.0366\n",
    "  Valid Loss: 0.0644\n",
    "  pKi: RMSE=1.043, R²=0.704\n",
    "  pEC50: RMSE=1.081, R²=0.619\n",
    "  pKd (Wang, FEP): RMSE=0.297, R²=0.903\n",
    "  pKd: RMSE=1.154, R²=0.589\n",
    "  pIC50: RMSE=1.011, R²=0.451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7605e1a9-516e-4f46-9aa6-d15d77b8702d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.path.join(CONFIG['experiment_dir'], 'models', 'final_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ef37f-a5e7-4269-96b7-d540bb5ddaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2060d8-26fb-4a5a-8a0a-602b0cf9fd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf5389a-fd86-4bbd-880e-987f6a54a213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a501e17-0b5c-4290-9119-e9beb1d42964",
   "metadata": {},
   "source": [
    "# multi gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0aedb997-49d7-4071-8705-04f77b2cfbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment: ddp_training_20250926_073940\n",
      "Config saved to: ../output/experiments/ddp_training_20250926_073940/config.json\n",
      "Using 16 GPUs\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Configuration\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import threading\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Your existing configuration\n",
    "CONFIG = {\n",
    "    'data_path': '../input/combined/deduplicated_complexes_parallel.csv',\n",
    "    'structure_chunks_dir': '../input/chunk/',\n",
    "    'task_cols': ['pKi', 'pEC50', 'pKd (Wang, FEP)', 'pKd', 'pIC50', 'potency'],\n",
    "    \n",
    "    'model_config': {\n",
    "        'prot_emb_dim': 1280,\n",
    "        'prot_gcn_dims': [128, 256, 256],\n",
    "        'prot_fc_dims': [1024, 128],\n",
    "        'drug_node_in_dim': [66, 1],\n",
    "        'drug_node_h_dims': [128, 64],\n",
    "        'drug_fc_dims': [1024, 128],\n",
    "        'mlp_dims': [1024, 512],\n",
    "        'mlp_dropout': 0.25\n",
    "    },\n",
    "    \n",
    "    'training_config': {\n",
    "        'batch_size_per_gpu': 512,  # With 16 GPUs = 8192 total batch size\n",
    "        'n_epochs': 200,\n",
    "        'learning_rate': 0.001,\n",
    "        'patience': 30,\n",
    "        'n_folds': 5\n",
    "    },\n",
    "    \n",
    "    'seed': 42,\n",
    "    'gradient_accumulation_steps': 1\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "experiment_name = f'ddp_training_{datetime.now():%Y%m%d_%H%M%S}'\n",
    "experiment_dir = Path(f'../output/experiments/{experiment_name}')\n",
    "experiment_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG['checkpoint_dir'] = str(experiment_dir / 'checkpoints')\n",
    "CONFIG['log_dir'] = str(experiment_dir / 'logs')\n",
    "\n",
    "# Save config\n",
    "config_path = experiment_dir / 'config.json'\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "print(f\"Experiment: {experiment_name}\")\n",
    "print(f\"Config saved to: {config_path}\")\n",
    "print(f\"Using {torch.cuda.device_count()} GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35c32a41-6199-4ba6-8e88-8a94f56db1a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching train with command:\n",
      "torchrun --nproc_per_node 16 --master_port 12355 ../training/launch_training.py --config ../output/experiments/ddp_training_20250926_073940/config.json --mode train --n_gpus 16\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Launch DDP Training\n",
    "def launch_ddp_training(config_path, mode='train', n_gpus=16):\n",
    "    \"\"\"\n",
    "    Launch DDP training using torchrun.\n",
    "    \"\"\"\n",
    "    cmd = [\n",
    "        'torchrun',\n",
    "        '--nproc_per_node', str(n_gpus),\n",
    "        '--master_port', '12355',\n",
    "        '../training/launch_training.py',\n",
    "        '--config', str(config_path),\n",
    "        '--mode', mode,\n",
    "        '--n_gpus', str(n_gpus)\n",
    "    ]\n",
    "    \n",
    "    print(f\"Launching {mode} with command:\")\n",
    "    print(' '.join(cmd))\n",
    "    \n",
    "    # Launch process\n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    return process\n",
    "\n",
    "# Launch training\n",
    "training_process = launch_ddp_training(config_path, mode='train', n_gpus=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80f282de-dab6-4706-8a58-ed8b80741a4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Monitor Training Progress\n",
    "class TrainingMonitor:\n",
    "    \"\"\"Monitor DDP training progress from logs.\"\"\"\n",
    "    \n",
    "    def __init__(self, log_dir, update_interval=5):\n",
    "        self.log_dir = Path(log_dir)\n",
    "        self.update_interval = update_interval\n",
    "        self.metrics_file = self.log_dir / 'metrics_*.json'\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        \n",
    "    def start(self):\n",
    "        \"\"\"Start monitoring.\"\"\"\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor_loop)\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        \"\"\"Stop monitoring.\"\"\"\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join()\n",
    "    \n",
    "    def _monitor_loop(self):\n",
    "        \"\"\"Main monitoring loop.\"\"\"\n",
    "        while self.running:\n",
    "            try:\n",
    "                # Find latest metrics file\n",
    "                metrics_files = list(self.log_dir.glob('metrics_*.json'))\n",
    "                if metrics_files:\n",
    "                    latest_file = max(metrics_files, key=os.path.getmtime)\n",
    "                    \n",
    "                    # Load metrics\n",
    "                    with open(latest_file, 'r') as f:\n",
    "                        metrics = json.load(f)\n",
    "                    \n",
    "                    if metrics:\n",
    "                        self._display_progress(metrics)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading metrics: {e}\")\n",
    "            \n",
    "            time.sleep(self.update_interval)\n",
    "    \n",
    "    def _display_progress(self, metrics):\n",
    "        \"\"\"Display training progress.\"\"\"\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        latest = metrics[-1] if metrics else {}\n",
    "        \n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"TRAINING PROGRESS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        if 'epoch' in latest:\n",
    "            print(f\"Epoch: {latest['epoch']}\")\n",
    "            print(f\"Train Loss: {latest.get('train_loss', 0):.4f}\")\n",
    "            print(f\"Valid Loss: {latest.get('val_loss', 0):.4f}\")\n",
    "            print(f\"Batch Time: {latest.get('batch_time', 0):.2f}s\")\n",
    "            print(f\"GPU Memory: {latest.get('gpu_memory_gb', 0):.1f}GB\")\n",
    "            \n",
    "            if 'task_metrics' in latest:\n",
    "                print(\"\\nTask Metrics:\")\n",
    "                for task, task_metrics in latest['task_metrics'].items():\n",
    "                    print(f\"  {task}: RMSE={task_metrics['rmse']:.3f}, R²={task_metrics['r2']:.3f}\")\n",
    "        \n",
    "        # Plot loss curves\n",
    "        if len(metrics) > 1:\n",
    "            epochs = [m['epoch'] for m in metrics]\n",
    "            train_losses = [m.get('train_loss', 0) for m in metrics]\n",
    "            val_losses = [m.get('val_loss', 0) for m in metrics]\n",
    "            \n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "            \n",
    "            ax1.plot(epochs, train_losses, label='Train Loss')\n",
    "            ax1.plot(epochs, val_losses, label='Valid Loss')\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.set_title('Training Progress')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Plot R2 scores\n",
    "            if 'task_metrics' in metrics[-1]:\n",
    "                tasks = list(metrics[-1]['task_metrics'].keys())\n",
    "                r2_scores = {task: [] for task in tasks}\n",
    "                \n",
    "                for m in metrics:\n",
    "                    if 'task_metrics' in m:\n",
    "                        for task in tasks:\n",
    "                            if task in m['task_metrics']:\n",
    "                                r2_scores[task].append(m['task_metrics'][task]['r2'])\n",
    "                \n",
    "                for task, scores in r2_scores.items():\n",
    "                    if scores:\n",
    "                        ax2.plot(epochs[:len(scores)], scores, label=task)\n",
    "                \n",
    "                ax2.set_xlabel('Epoch')\n",
    "                ax2.set_ylabel('R² Score')\n",
    "                ax2.set_title('Task Performance')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Start monitoring\n",
    "monitor = TrainingMonitor(CONFIG['log_dir'])\n",
    "monitor.start()\n",
    "\n",
    "# To stop monitoring:\n",
    "# monitor.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44e7510-a67b-4c7a-91d7-24be43847577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a9ba69-1060-4cac-92de-be4711ba8091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74496b3b-4a4b-4479-814f-34e21e6c2e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "787c5422-a885-4ff7-a812-6b8f20eaedbb",
   "metadata": {},
   "source": [
    "# CV multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39a013c-bf47-4d24-9b72-83c91976be7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Launch Cross-Validation\n",
    "# Stop previous monitoring if running\n",
    "if 'monitor' in locals():\n",
    "    monitor.stop()\n",
    "\n",
    "# Launch CV\n",
    "cv_process = launch_ddp_training(config_path, mode='cv', n_gpus=16)\n",
    "\n",
    "# Start monitoring for CV\n",
    "cv_monitor = TrainingMonitor(CONFIG['log_dir'])\n",
    "cv_monitor.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70018f8b-f22f-4d37-8010-a24d799c9c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Check Training Status and Results\n",
    "def check_training_status(checkpoint_dir):\n",
    "    \"\"\"Check training status from checkpoints.\"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    \n",
    "    # Check for best model\n",
    "    best_model_path = checkpoint_dir / 'best_model.pt'\n",
    "    if best_model_path.exists():\n",
    "        checkpoint = torch.load(best_model_path, map_location='cpu')\n",
    "        print(\"Best Model Found!\")\n",
    "        print(f\"  Epoch: {checkpoint['epoch']}\")\n",
    "        print(f\"  Validation Loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n",
    "        \n",
    "        if 'task_metrics' in checkpoint:\n",
    "            print(\"\\n  Task Metrics:\")\n",
    "            for task, metrics in checkpoint['task_metrics'].items():\n",
    "                print(f\"    {task}: RMSE={metrics['rmse']:.3f}, R²={metrics['r2']:.3f}\")\n",
    "    \n",
    "    # Check for CV results\n",
    "    cv_results_path = checkpoint_dir / 'cv_results.json'\n",
    "    if cv_results_path.exists():\n",
    "        with open(cv_results_path, 'r') as f:\n",
    "            cv_results = json.load(f)\n",
    "        \n",
    "        print(\"\\nCross-Validation Results:\")\n",
    "        for task, metrics in cv_results.items():\n",
    "            print(f\"  {task}:\")\n",
    "            print(f\"    R²: {metrics['r2_mean']:.3f} ± {metrics['r2_std']:.3f}\")\n",
    "            print(f\"    RMSE: {metrics['rmse_mean']:.3f} ± {metrics['rmse_std']:.3f}\")\n",
    "\n",
    "check_training_status(CONFIG['checkpoint_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56c9be-6c5d-456e-bc43-9eb17ebb1b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Load Best Model for Inference\n",
    "def load_best_model(checkpoint_dir, model_config, task_cols):\n",
    "    \"\"\"Load the best trained model.\"\"\"\n",
    "    from gnn_dta_mtl import MTL_DTAModel\n",
    "    \n",
    "    # Create model\n",
    "    model = MTL_DTAModel(\n",
    "        task_names=task_cols,\n",
    "        **model_config\n",
    "    )\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint_path = Path(checkpoint_dir) / 'best_model.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    return model\n",
    "\n",
    "# Load model\n",
    "best_model = load_best_model(\n",
    "    CONFIG['checkpoint_dir'],\n",
    "    CONFIG['model_config'],\n",
    "    CONFIG['task_cols']\n",
    ")\n",
    "\n",
    "# Move to GPU for inference\n",
    "best_model = best_model.cuda()\n",
    "best_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7fdf92-8e57-4067-8aba-0988419d739c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76bf0d-9bee-45a6-91e8-0a021d04361f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a577732-0b21-4e6c-8e6e-e1a94fc4a243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b11b895-87e8-4eaf-bcd8-f876254e8468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c522b9b-1322-40a0-91d2-1986401560f3",
   "metadata": {},
   "source": [
    "# Cell 4: Launch Cross-Validation\n",
    "# Stop previous monitoring if running\n",
    "if 'monitor' in locals():\n",
    "    monitor.stop()\n",
    "\n",
    "# Launch CV\n",
    "cv_process = launch_ddp_training(config_path, mode='cv', n_gpus=16)\n",
    "\n",
    "# Start monitoring for CV\n",
    "cv_monitor = TrainingMonitor(CONFIG['log_dir'])\n",
    "cv_monitor.start()# 6 : Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d1753f-3642-4b26-ad43-ad02cb87f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set (for full training)\n",
    "if 'test_loader' in locals():\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_results = evaluate_model(model, test_loader, CONFIG['task_cols'], device)\n",
    "    \n",
    "    # Print test results\n",
    "    print(\"\\nTest Results:\")\n",
    "    for task, metrics in test_results.items():\n",
    "        print(f\"\\n{task}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {metric_name}: {value:.4f}\")\n",
    "    \n",
    "    # Save test results\n",
    "    save_results(\n",
    "        test_results,\n",
    "        os.path.join(CONFIG['experiment_dir'], 'results', 'test_results.json'),\n",
    "        format='json'\n",
    "    )\n",
    "\n",
    "# For cross-validation results\n",
    "else:\n",
    "    print(\"\\nCross-Validation Summary:\")\n",
    "    summary_df = create_summary_report(\n",
    "        cv_results,\n",
    "        CONFIG['task_cols'],\n",
    "        os.path.join(CONFIG['experiment_dir'], 'results', 'cv_summary.csv')\n",
    "    )\n",
    "    print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2f97ab-74c0-4ec7-8de8-1cb5fe0ebad8",
   "metadata": {},
   "source": [
    "# 7 : Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764735f-2d6f-49fa-83d1-625c5be5f0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Plot CV results\n",
    "if 'cv_results' in locals():\n",
    "    fig = plot_results(\n",
    "        cv_results,\n",
    "        CONFIG['task_cols'],\n",
    "        save_path=os.path.join(CONFIG['experiment_dir'], 'figures', 'cv_results.png')\n",
    "    )\n",
    "\n",
    "# Plot training history\n",
    "if 'trainer' in locals() and hasattr(trainer, 'train_losses'):\n",
    "    from gnn_dta_mtl.evaluation.visualization import plot_training_history\n",
    "    plot_training_history(\n",
    "        trainer.train_losses[:-22] + trainer.train_losses[-21:],\n",
    "        trainer.val_losses[:-22] + trainer.val_losses[-21:],\n",
    "        save_path=os.path.join(CONFIG['experiment_dir'], 'figures', 'training_history.png')\n",
    "    )\n",
    "\n",
    "# Plot metrics distribution across folds\n",
    "if 'cv_results' in locals():\n",
    "    from gnn_dta_mtl.evaluation.visualization import plot_metrics_distribution\n",
    "    plot_metrics_distribution(\n",
    "        cv_results,\n",
    "        CONFIG['task_cols'],\n",
    "        metric='r2',\n",
    "        save_path=os.path.join(CONFIG['experiment_dir'], 'figures', 'r2_distribution.png')\n",
    "    )\n",
    "    \n",
    "    plot_metrics_distribution(\n",
    "        cv_results,\n",
    "        CONFIG['task_cols'],\n",
    "        metric='rmse',\n",
    "        save_path=os.path.join(CONFIG['experiment_dir'], 'figures', 'rmse_distribution.png')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd492dd-9623-4d08-abc0-dac226e1b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors\n",
    "if 'cv_results' in locals():\n",
    "    for task in CONFIG['task_cols']:\n",
    "        if len(cv_results[task]['all_targets']) > 0:\n",
    "            targets = np.array(cv_results[task]['all_targets'])\n",
    "            preds = np.array(cv_results[task]['all_predictions'])\n",
    "            \n",
    "            # Calculate residuals\n",
    "            residuals = targets - preds\n",
    "            \n",
    "            # Create figure with subplots\n",
    "            fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "            \n",
    "            # 1. Predictions vs Targets\n",
    "            ax = axes[0, 0]\n",
    "            from gnn_dta_mtl.evaluation.visualization import plot_predictions\n",
    "            plot_predictions(targets, preds, task, ax)\n",
    "            \n",
    "            # 2. Residuals plot\n",
    "            ax = axes[0, 1]\n",
    "            from gnn_dta_mtl.evaluation.visualization import plot_residuals\n",
    "            plot_residuals(targets, preds, task, ax)\n",
    "            \n",
    "            # 3. Residual distribution\n",
    "            ax = axes[1, 0]\n",
    "            ax.hist(residuals, bins=50, edgecolor='black', alpha=0.7)\n",
    "            ax.axvline(x=0, color='red', linestyle='--', alpha=0.7)\n",
    "            ax.set_xlabel('Residuals')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_title(f'Residual Distribution - {task}')\n",
    "            \n",
    "            # 4. Q-Q plot\n",
    "            ax = axes[1, 1]\n",
    "            from scipy import stats\n",
    "            stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "            ax.set_title(f'Q-Q Plot - {task}')\n",
    "            \n",
    "            plt.suptitle(f'Detailed Analysis - {task}', fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(\n",
    "                os.path.join(CONFIG['experiment_dir'], 'figures', f'analysis_{task}.png'),\n",
    "                dpi=300, bbox_inches='tight'\n",
    "            )\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128be980-3a05-44db-887e-f22be13dd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Bootstrap Analysis\n",
    "from gnn_dta_mtl.evaluation.metrics import bootstrap_metrics\n",
    "\n",
    "if 'cv_results' in locals():\n",
    "    print(\"\\nBootstrap Confidence Intervals (95%):\")\n",
    "    \n",
    "    for task in CONFIG['task_cols']:\n",
    "        if len(cv_results[task]['all_targets']) > 0:\n",
    "            targets = np.array(cv_results[task]['all_targets'])\n",
    "            preds = np.array(cv_results[task]['all_predictions'])\n",
    "            \n",
    "            # Calculate bootstrap CIs\n",
    "            boot_results = bootstrap_metrics(\n",
    "                targets, preds,\n",
    "                n_bootstrap=1000,\n",
    "                confidence=0.95,\n",
    "                seed=SEED\n",
    "            )\n",
    "            \n",
    "            print(f\"\\n{task}:\")\n",
    "            for metric, (mean, lower, upper) in boot_results.items():\n",
    "                print(f\"  {metric}: {mean:.3f} [{lower:.3f}, {upper:.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7920e06-1788-41c5-87dd-fcd55dfedfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Analyze Model Features\n",
    "if 'model' in locals():\n",
    "    from gnn_dta_mtl.utils.model_utils import (\n",
    "        count_parameters, \n",
    "        get_model_size,\n",
    "        get_activation_stats\n",
    "    )\n",
    "    \n",
    "    # Model statistics\n",
    "    n_params = count_parameters(model)\n",
    "    model_size = get_model_size(model)\n",
    "    \n",
    "    print(\"Model Statistics:\")\n",
    "    print(f\"  Total parameters: {n_params:,}\")\n",
    "    print(f\"  Model size: {model_size['total_size_mb']:.2f} MB\")\n",
    "    \n",
    "    # Get activation statistics\n",
    "    if 'test_loader' in locals():\n",
    "        act_stats = get_activation_stats(model, test_loader, device)\n",
    "        \n",
    "        # Visualize activation statistics\n",
    "        layers = list(act_stats.keys())[-10:]  # Last 10 layers\n",
    "        means = [act_stats[l]['mean'] for l in layers]\n",
    "        stds = [act_stats[l]['std'] for l in layers]\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        x = range(len(layers))\n",
    "        ax1.bar(x, means, alpha=0.7)\n",
    "        ax1.set_xticks(x)\n",
    "        ax1.set_xticklabels(layers, rotation=45, ha='right')\n",
    "        ax1.set_ylabel('Mean Activation')\n",
    "        ax1.set_title('Layer Activation Statistics')\n",
    "        \n",
    "        ax2.bar(x, stds, alpha=0.7, color='orange')\n",
    "        ax2.set_xticks(x)\n",
    "        ax2.set_xticklabels(layers, rotation=45, ha='right')\n",
    "        ax2.set_ylabel('Std Activation')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\n",
    "            os.path.join(CONFIG['experiment_dir'], 'figures', 'activation_stats.png'),\n",
    "            dpi=300, bbox_inches='tight'\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a90c59-3760-463d-bda6-b1c2b0c47970",
   "metadata": {},
   "source": [
    "# Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23caa6cc-c28f-49dc-9c68-2790ac410885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Export Final Results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Compile all results\n",
    "final_results = {\n",
    "    'experiment_name': experiment_name,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'config': CONFIG,\n",
    "    'data_stats': {\n",
    "        'total_samples': len(df),\n",
    "        'train_samples': len(df_train) if 'df_train' in locals() else None,\n",
    "        'valid_samples': len(df_valid) if 'df_valid' in locals() else None,\n",
    "        'test_samples': len(df_test) if 'df_test' in locals() else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add CV results summary\n",
    "if 'cv' in locals() and hasattr(cv, 'summary'):\n",
    "    final_results['cv_summary'] = cv.summary\n",
    "\n",
    "# Add test results\n",
    "if 'test_results' in locals():\n",
    "    final_results['test_results'] = test_results\n",
    "\n",
    "# Save comprehensive report\n",
    "report_path = os.path.join(CONFIG['experiment_dir'], 'final_report.json')\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, default=str)\n",
    "\n",
    "print(f\"\\nFinal report saved to: {report_path}\")\n",
    "print(f\"All results saved in: {CONFIG['experiment_dir']}\")\n",
    "\n",
    "# Create LaTeX table for publication\n",
    "if 'cv' in locals() and hasattr(cv, 'summary'):\n",
    "    print(\"\\nLaTeX Table for Publication:\")\n",
    "    print(\"\\\\begin{table}[h]\")\n",
    "    print(\"\\\\centering\")\n",
    "    print(\"\\\\begin{tabular}{lcccc}\")\n",
    "    print(\"\\\\hline\")\n",
    "    print(\"Task & R² & RMSE & MAE & N \\\\\\\\\")\n",
    "    print(\"\\\\hline\")\n",
    "    \n",
    "    for task, metrics in cv.summary.items():\n",
    "        print(f\"{task} & \"\n",
    "              f\"{metrics['r2_mean']:.3f}$\\\\pm${metrics['r2_std']:.3f} & \"\n",
    "              f\"{metrics['rmse_mean']:.3f}$\\\\pm${metrics['rmse_std']:.3f} & \"\n",
    "              f\"- & \"\n",
    "              f\"{metrics['n_samples']} \\\\\\\\\")\n",
    "    \n",
    "    print(\"\\\\hline\")\n",
    "    print(\"\\\\end{tabular}\")\n",
    "    print(\"\\\\caption{Cross-validation results for multi-task drug-target affinity prediction}\")\n",
    "    print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed3ebe-a837-40cb-bcb7-792ea6901b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Interactive Analysis Functions\n",
    "def analyze_predictions_by_property(task, property_col='MolWt', n_bins=5):\n",
    "    \"\"\"Analyze predictions by molecular property\"\"\"\n",
    "    if task not in cv_results or len(cv_results[task]['all_targets']) == 0:\n",
    "        print(f\"No results for {task}\")\n",
    "        return\n",
    "    \n",
    "    # Get predictions and targets\n",
    "    targets = np.array(cv_results[task]['all_targets'])\n",
    "    preds = np.array(cv_results[task]['all_predictions'])\n",
    "    \n",
    "    # Get property values (need to match with original df)\n",
    "    # This assumes df is still aligned with cv_results\n",
    "    property_values = df[property_col].values[:len(targets)]\n",
    "    \n",
    "    # Create bins\n",
    "    bins = pd.qcut(property_values, n_bins, labels=False, duplicates='drop')\n",
    "    \n",
    "    # Calculate metrics per bin\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    \n",
    "    results = []\n",
    "    for bin_idx in range(n_bins):\n",
    "        mask = bins == bin_idx\n",
    "        if mask.sum() > 0:\n",
    "            r2 = r2_score(targets[mask], preds[mask])\n",
    "            rmse = np.sqrt(mean_squared_error(targets[mask], preds[mask]))\n",
    "            results.append({\n",
    "                'bin': bin_idx,\n",
    "                'n_samples': mask.sum(),\n",
    "                'r2': r2,\n",
    "                'rmse': rmse,\n",
    "                f'{property_col}_mean': property_values[mask].mean()\n",
    "            })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.bar(results_df['bin'], results_df['r2'], alpha=0.7)\n",
    "    ax1.set_xlabel(f'{property_col} Bin')\n",
    "    ax1.set_ylabel('R²')\n",
    "    ax1.set_title(f'R² by {property_col} - {task}')\n",
    "    \n",
    "    ax2.bar(results_df['bin'], results_df['rmse'], alpha=0.7, color='orange')\n",
    "    ax2.set_xlabel(f'{property_col} Bin')\n",
    "    ax2.set_ylabel('RMSE')\n",
    "    ax2.set_title(f'RMSE by {property_col} - {task}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Example usage\n",
    "if 'cv_results' in locals():\n",
    "    for task in CONFIG['task_cols'][:1]:  # Analyze first task\n",
    "        results_by_mw = analyze_predictions_by_property(task, 'MolWt')\n",
    "        results_by_logp = analyze_predictions_by_property(task, 'LogP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b8ad6-bf62-4958-a88a-7ca8761d0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Save Session State\n",
    "import pickle\n",
    "\n",
    "# Save important objects\n",
    "session_state = {\n",
    "    'config': CONFIG,\n",
    "    'task_ranges': task_ranges,\n",
    "    'cv_results': cv_results if 'cv_results' in locals() else None,\n",
    "    'test_results': test_results if 'test_results' in locals() else None,\n",
    "    'df_stats': {\n",
    "        'shape': df.shape,\n",
    "        'columns': df.columns.tolist(),\n",
    "        'task_coverage': {task: df[task].notna().sum() for task in CONFIG['task_cols']}\n",
    "    }\n",
    "}\n",
    "\n",
    "session_path = os.path.join(CONFIG['experiment_dir'], 'session_state.pkl')\n",
    "with open(session_path, 'wb') as f:\n",
    "    pickle.dump(session_state, f)\n",
    "\n",
    "print(f\"Session state saved to: {session_path}\")\n",
    "print(\"\\nTo restore session in a new notebook:\")\n",
    "print(f\"with open('{session_path}', 'rb') as f:\")\n",
    "print(\"    session_state = pickle.load(f)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f0451-7ceb-4558-9c0f-cf9ac8068a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dce3bc-57c4-4bfb-b1a0-edbad99f49d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6179874-15b6-43ef-9dd6-52eecf32a384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2495a5d-9c94-45bd-b669-7108b25a8ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "bioml",
   "name": "workbench-notebooks.m130",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m130"
  },
  "kernelspec": {
   "display_name": "BioML (Python 3.10)",
   "language": "python",
   "name": "bioml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
